<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Scott Viteri</title>
    <link>http://scottviteri.github.io/post/</link>
      <atom:link href="http://scottviteri.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2023</copyright><lastBuildDate>Mon, 27 Feb 2023 10:31:53 -0700</lastBuildDate>
    <image>
      <url>http://scottviteri.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>http://scottviteri.github.io/post/</link>
    </image>
    
    <item>
      <title>Conversationism</title>
      <link>http://scottviteri.github.io/post/conversationism/</link>
      <pubDate>Mon, 27 Feb 2023 10:31:53 -0700</pubDate>
      <guid>http://scottviteri.github.io/post/conversationism/</guid>
      <description>&lt;p&gt;Crossposted at 
&lt;a href=&#34;https://www.lesswrong.com/posts/HpHyERTmsmhDiRHtY/conversationism&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.lesswrong.com/posts/HpHyERTmsmhDiRHtY/conversationism&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;There seems to be a stark contrast between my alignment research on 
&lt;a href=&#34;https://www.lesswrong.com/posts/C5PZNi5fueH2RC6aF/repl-s-and-elk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ontology maps&lt;/a&gt; and on 
&lt;a href=&#34;https://www.lesswrong.com/posts/FnfAnsAH6dva3kCHS/research-direction-be-the-agi-you-want-to-see-in-the-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cyborgism via Neuralink&lt;/a&gt;. I claim that the path has consisted of a series of forced moves, and that my approach to the second follows from my conclusions about the first. This post is an attempt to document those forced moves, in the hope that others do not have to duplicate my work.&lt;/p&gt;
&lt;p&gt;Ontology maps are about finding shape semantic-preserving functions between the internal states of agents. Each commutative diagram denotes a particular way of training neural networks to act as conceptual bridges between a human and AI. I originally started playing with these diagrams as a way of addressing the 
&lt;a href=&#34;https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eliciting Latent Knowledge&lt;/a&gt; problem. Erik Jenner has been thinking about similar objects 
&lt;a href=&#34;https://www.lesswrong.com/posts/nLhHY2c8MWFcuWRLx/good-ontologies-induce-commutative-diagrams&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and 
&lt;a href=&#34;https://www.lesswrong.com/posts/L8LHBTMvhLDpxDaqv/research-agenda-formalizing-abstractions-of-computations-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Cyborgism via Neuralink is about using a brain-computer interface to read neural activation and write those predictions back to the brain, in the hope that neuroplasticity will allow you to harness the extra compute. The goal is to train an artificial neural network with a particularly human inductive bias. Given severe bandwidth constraints (currently three thousand read/write electrodes), the main reason to think this might work is that the brain and the transistor-based brain predictor are being jointly trained &amp;ndash; the brain can learn to send inputs to the predictor such that the predictions are useful for some downstream task. In other words, both the brain and the neural prosthesis are trained online. Think of predicting the activations of a fixed neural network given time series data (hard) versus the same task, but you pass gradients with respect to the base neural network&amp;rsquo;s final loss through the bandwidth bottleneck into the predictor (less hard).&lt;/p&gt;
&lt;p&gt;In the section &amp;ldquo;An Alternative Form of Feedback&amp;rdquo;, I introduce a vision of alignment that takes communication bandwidth as fundamental, and I sketch out how one might amplify human feedback on AI via conversation. I might recommend reading that section more than the &amp;ldquo;TLDR &amp;ndash; My Conceptual Path&amp;rdquo; section.&lt;/p&gt;
&lt;h2 id=&#34;tldr----my-conceptual-path&#34;&gt;TLDR &amp;ndash; My Conceptual Path&lt;/h2&gt;
&lt;p&gt;I started thinking about ontology maps in the context of ELK, because ontology maps can translate the AI&amp;rsquo;s beliefs about the diamond&amp;rsquo;s location into the corresponding human beliefs. Over time I stopped thinking about aligning an external agent that is super-intelligent and has contrary goals to your own, and instead I thought more about adding compute capacity to yourself/merging with the AI. The former seems somewhat doomed, and in the context of ELK I have come to feel like the human simulator problem is basically unsolvable if you take the aligning external agent point of view. In this document I will go through a series of ontology map training procedures and briefly note on their properties and problems. I will end up at a training procedure that basically looks like the shape of learning to me, and describes how to write the software for Neuralink to leads to a human-AI symbiosis. In bullets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Various training procedures for human/AI symbiosis correspond to differently shaped commutative diagrams.&lt;/li&gt;
&lt;li&gt;These commutative diagrams have importantly different alignment properties.&lt;/li&gt;
&lt;li&gt;The diagram that makes the most sense is about agents trying to predict each other.&lt;/li&gt;
&lt;li&gt;These various diagrams don&amp;rsquo;t address ELK-style human simulator issues.&lt;/li&gt;
&lt;li&gt;Instead of building and/or aligning a separate superintelligence, we should focus on augmenting a human into superintelligence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ontology-maps&#34;&gt;Ontology Maps&lt;/h2&gt;
&lt;p&gt;The broad intuition is that given two state machines representing agents I would like to find a map between them that preserves as much structure as possible. Intuitively I am trying to find a fuzzy homomorphism between graphs. Suppose an agent is defined as a set of states and a transition function on those states. Namely, suppose we have a human agent with state type SH and transition function TH:SH→SH and an AI denoted SMand TM:SM→SM, where M stands for machine. We could train a functionF:SM→SH and G:SH→SM that minimize the distance between TM and F;TH;G, where &amp;ldquo;;&amp;rdquo; represents forward composition. This is depicted by the following commutative diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/jylykd2tx7syzzvsiqcc.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;However, this set-up as an alignment strategy would expose us to the risk of steganography. The loss on F and G could be minimized by encoding extraneous information into the belief state SH of the human (such as some random bits about the location of a moon of Jupiter) which TH would not modify. Then G could reconstruct the original SM by interpreting those extra bits, and then use TM to achieve zero loss. I thought at the time that this steganography was being caused by the extra degree of freedom in learning both F and G. So then I thought about the following diagram:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677543189/mirroredImages/HpHyERTmsmhDiRHtY/kvcwdjxz7zcr3zvw8xlk.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can call this picture the abstraction diagram. For this intuition think of TM as quantum mechanics and TH as Newtonian mechanics. F abstracts the state of the worlds into coarse statements about mass and energy and such, so that propagating the system forward with quantum and Newtonian transitions commute.&lt;/p&gt;
&lt;p&gt;So now we are only learning one function F:SH→SH, which may prevent steganography. And it has a certain intuitive appeal: it could be interpreted as saying two belief states are equal if they have the same relationship to other beliefs. However, I have not yet added any additional structural assumptions to the state set SH, so there is not a natural differentiable distance metric in SH (to my knowledge). So instead we can consider probability distributions ΔSH, use Kullback-Liebler divergence, and this model is now PyTorch ready:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/tyooe58mx7szkxtpaxdj.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;But then I can achieve zero loss by learning an F that maps a state in SM to the uniform probability distribution over SH. So we could add a term in the loss to penalize the average distance between the images of two random states in SH under F. But this is ad-hoc, and what I am really trying to do is preserve the information in agent M when mapping through agent H, which sounds like our first diagram. So why don&amp;rsquo;t we try a different method to prevent steganography &amp;ndash; reversing the directions of F and G (and keeping the probabilistic modification):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/twqjqpjfovjyayvmqmow.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use an example to illustrate what we want from this diagram. I want an SM,TM that fully captures the partial information from SH,TH. Let SH have 5 elements and let TH cycle between the first three elements and a cycle between the last two elements. We can write TH as the permutation (012)(34). Let SM have six elements and let TH be a 6-cycle (012345). If you haven&amp;rsquo;t seen this syntax for permutations before, you can just look at the following diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/psexizfv13lr5t0h07yu.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can run this training algorithm with the following encodings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TM is the off-diagonal 6 by 6 matrix&lt;/li&gt;
&lt;li&gt;TH is the 5 by 5 matrix consisting of the off-diagonal 3 by 3 and 2 by 2 matrices&lt;/li&gt;
&lt;li&gt;F is a neural network with 5 inputs and 6 outputs&lt;/li&gt;
&lt;li&gt;G is a neural network with 6 inputs and 5 outputs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Intuitively, I wanted F to become the matrix:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/bko2p6ldbykgpzxvpq0o.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In words, if the state was 0 I would like it to map to uniform distribution of values that are 0 modulo 3. If the state is 3 then I would like it to map to a uniform distribution over values that are 0 modulo 2. This is because I want to be able to use F;TM;G to reconstruct TH as well as possible. But what happens if F and G are nonlinear functions (which is necessary if we want to scale this procedure to non-trivial agents)? Answer: horrible, horrible steganography. If you think of floating points numbers as being real numbers, then F can encode arbitrary information about SH in the particular probabilities that it picks, even if SM only consisted of two states. We could try sampling after taking F, and training with Gumbel-softmax or the reparameterization trick or with REINFORCE, or something. But this feels kind of ad-hoc and gross again.&lt;/p&gt;
&lt;p&gt;So the only kind of information bottleneck guarantees that we are going to get when routing through TM are based on the capacities/vc-dimension/inductive biases of F, TM, and G. At this point, we might as well only consider continuous spaces and let SH and SM always be Rn and Rm for some n and m. Large language models seems to be getting mileage out of tokenizing and embedding everything anyway. So Erik Jenner was right all along, and we can write these diagrams first, and think about what category our diagrams are in later, as long as that category has a distance metric (which R does of course). So let&amp;rsquo;s strip off the probability symbols and write my favorite diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/hf9alyf46wqy1g54tbdu.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;To be clear, at this point we have basically given up on the steganography story. But I think we have arrived at something very nice. I think this diagram is essentially the shape of the learning. Namely, F and G are the glue between you and external computation processes that predict you. For instance, F is typing into the calculator, TM is the calculator running, G is reading the output. Crucially, you must find that it produced the same number that you would have produced yourself.&lt;/p&gt;
&lt;p&gt;This is a general process that can be used to parallelize a serial computation. Suppose I have some sequence of instructions in a programming language, such as python.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/ctpvaxtcowdmbsizse7e.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I want to think of this computation as happening continuously in lockstep. The variables &amp;ldquo;a&amp;rdquo; and &amp;ldquo;b&amp;rdquo; are really more like electrical wires, which always have a voltage, which sometimes changes. The variables &amp;ldquo;c&amp;rdquo;, &amp;ldquo;d&amp;rdquo;, and &amp;ldquo;e&amp;rdquo; are each getting updated each clock cycle with the values from their most recent inputs. An arithmetic circuit with open input wires will also work for this story.&lt;/p&gt;
&lt;p&gt;Suppose I also have some extra computers lying around that I want to use to speed up this computation. Among the most naive things I could do is point an extra computer at &amp;ldquo;e&amp;rdquo;, and predict the next step in its time-step data by using its previous value, or sampling predictions according to the distribution of what values that variable has taken so far. But you could also imagine giving this extra computer access to time series data of other variables such as &amp;ldquo;a&amp;rdquo; and &amp;ldquo;d&amp;rdquo;. If the predictor computer is sufficiently accurate, the base computer might accept its predictions and get to next stable state of the system more rapidly. We would save serial compute steps by short-circuiting the intermediate computations. We&amp;rsquo;d be in an even better situation if the predictor computers could learn which inputs to pay attention to. We&amp;rsquo;d be in a better situation still if the base computer program could learn when to accept predictions from the prediction.&lt;/p&gt;
&lt;p&gt;In this analogy, the original program is TH, and the predictor functions are F;TM;G. Indeed it only makes sense to talk about F;TM;G as opposed to learning a function T′H:SH→SH when there is another pre-existing useful function around, such as pre-trained language model. If TH is some supervisory signal, autoencoders are the special case of this diagram where TM is the identity function. The sparsity of the autoencoder depends on the relative dimension of SH and SM. I sometimes think of transformers as using linear F and G to embed and unembed, and TM here is computation in the residual stream. I would greatly appreciate comments to this post with machine learning instantiations of this structure.&lt;/p&gt;
&lt;p&gt;I am thinking of a story where the universe consists of a bunch of functions, and each of these functions is being replaced by faster functions that do the same thing. That same thing is essentially getting to the stable point of the universes dynamics faster &amp;ndash; so the functions are replaced by others that are better at eating negentropy. See the figure below for a rough illustration of this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538005/mirroredImages/HpHyERTmsmhDiRHtY/fq0g2ll7kbhfrvcl0hhb.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I also think this shape of predictors replacing predictors is like the relationship between the steering and learning subsystems in Steven Byrnes&amp;rsquo; Intro to Brain-Like-AGI Safety.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538005/mirroredImages/HpHyERTmsmhDiRHtY/rvlisgumz2dvcnbmv0ju.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-to-do-with-the-nice-shape&#34;&gt;What To Do with the Nice Shape&lt;/h2&gt;
&lt;p&gt;Ok, so maybe some functions can learn some other functions. Why are we here, and why have I taken up so much of your time to say such an obvious thing? If this is supposed to represent a training loop where the human is represented by TH, don&amp;rsquo;t you need a version of a human that is in a computer in the first place? Additionally, if uploading actually worked in time, then wouldn&amp;rsquo;t we already have a win condition for alignment?&lt;/p&gt;
&lt;p&gt;My response to the &amp;ldquo;needing an upload first&amp;rdquo; concern used to be that TH could be a very partial version of your full transition function, such as the heart rate monitor on your Apple watch, or GPT fine-tuned on your writing. And GPT fine-tuned on your writing is definitely closer to the TH I want to put through this protocol, but I think it is probably not enough to capture what I care about.&lt;/p&gt;
&lt;p&gt;So the natural next step here is doing a better job of putting a person in a computer. This is bottlenecked on what kind of data you can use to train a predictive model of a person. So in this post I talk about getting brain activation data via many small electrodes, because this might have a higher change of capturing important aspects of myself for a pseudo-uploading procedure.&lt;/p&gt;
&lt;p&gt;I will describe the pseudo-uploading procedure in terms of my diagram. We start with a pretrained model (TM) such as GPT, potentially fine-tuned on pre-existing EEG data. We read brain data via electrode and translate into TM&amp;rsquo;s latent space using F. TM makes a prediction of the brain&amp;rsquo;s future activations. G translates that prediction and writes the prediction back into the brain via electrode stimulation. Note that in this story F, TM, and G kind of blend into each other since TM is being trained online.&lt;/p&gt;
&lt;p&gt;As I mentioned in the intro, I only expect this to work because the brain is trying to predict the neural prosthesis as the neural prosthesis is trying to predict the brain. So this makes it seem like we are now talking about a conversation:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538005/mirroredImages/HpHyERTmsmhDiRHtY/juhh7prq5e8sa0bguqjh.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This whole story has essentially been about how to increase the bandwidth between agents, and the answer is something like mutual prediction. Each agent is learning how to write and read to the other agent in order to speed up / parallelize their own computation. These thoughts about commutative diagrams are how I prefer to design the software for Neuralink.&lt;/p&gt;
&lt;p&gt;But using only a single &amp;ldquo;F&amp;rdquo;, &amp;ldquo;G&amp;rdquo; pair for Neuralink seems lopsided and asymmetric. For example, I wanted to train a &amp;ldquo;predictor&amp;rdquo; neural network to predict a &amp;ldquo;base&amp;rdquo; neural network&amp;rsquo;s activations. I would do this by training the &amp;ldquo;base&amp;rdquo; neural network, and letting gradients propagate back into the &amp;ldquo;predictor&amp;rdquo; network. I would prefer if it was that the AI and the human in the loop both had something to gain from the conversation, and learned to call each other as API&amp;rsquo;s when they desired.&lt;/p&gt;
&lt;h2 id=&#34;an-alternative-kind-of-feedback&#34;&gt;An Alternative Kind of Feedback&lt;/h2&gt;
&lt;p&gt;Recently, I have been internally debating on whether to label my recent thoughts as a particular flavor of 
&lt;a href=&#34;https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cyborgism&lt;/a&gt;, using Conjecture&amp;rsquo;s terminology. I have come to feel that there is an important distinction I want to make reflecting the bidirectional nature of conversation.&lt;/p&gt;
&lt;p&gt;I will sketch a different way to align an AGI than RLHF or conditional pre-training to give a sense of my aesthetic. Raising a child via a yes/no reward signal would be very low bandwidth, not to mention unkind. Instead I could imagine starting with a toddler GPT-J, and training it via minimizing prediction error on conversation with a select set of parents. As it gets older, it can ask to read parts of the internet or chat with particular people from a wider set of less vetted parents. When it reaches school age, it can be trained via conversations with similarly competent language models. This process can potentially be sped up by learning particular F and G&amp;rsquo;s to speak directly into each other&amp;rsquo;s latent spaces. At this point it is competent enough to be useful to a broad group of people, who can pay to talk to it. It can use some of that money to pay to talk to particular people and language models, at which point it is a functioning member of society.&lt;/p&gt;
&lt;p&gt;The most obvious way that this can go wrong is if the initial high-bandwith parenting via conversation is too expensive. So we could fine-tune GPT on high quality parenting scripts. This GPT-Nanny already seems like a better (higher bandwith) version of feedback than reward modeling of human annotators in RLHF. The role of alignment reaserchers would to push back the pareto-frontier of cost versus annotation quality.&lt;/p&gt;
&lt;p&gt;I would like to call this style of approach conversationism, where we especially highlight the creation of a bidirectional relationship and cultural integration of AI. I know the reader is thinking it is naive think that the best way to build a nice AI is to treat it nicely, but I am having trouble imagining any other way. I have more thoughts on the philosophy of this approach, but that will have to wait for another post.&lt;/p&gt;
&lt;p&gt;An aside: Parents (and all agents really) can prepend their messages with a secret hash so that the language model knows who it is talking to. It would be unfortunate if someone could easily pretend to be your mom while calling you on the phone.&lt;/p&gt;
&lt;h2 id=&#34;some-related-work&#34;&gt;Some Related Work&lt;/h2&gt;
&lt;p&gt;The most obvious related work is Erik Jenner&amp;rsquo;s here and here, linked again for convenience.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538005/mirroredImages/HpHyERTmsmhDiRHtY/aim9tecgxxztpuujntq0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;He uses the abstraction diagram from the machine&amp;rsquo;s state into S, which I am thinking of a general human interpretation language. He also is trying to discover TS jointly with F, if I understand correctly. I labelled the bottom states S to denote a common language between humans (SH for many different values of H). I am thinking more about the prediction ontology map, because I am especially interested in interpretability that works in both directions. I would like to know how to compose individual agents into a super-agent via conversation and communication.&lt;/p&gt;
&lt;p&gt;The abstraction ontology map is like viewing one agent&amp;rsquo;s state as an abstraction of another agent&amp;rsquo;s state, and various choices of abstraction are discussed in 
&lt;a href=&#34;http://rbr.cs.umass.edu/aimath06/proceedings/P21.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards a Unified Theory of State Abstraction for MDPs&lt;/a&gt;. This looks similar to bisimulation of labelled transition systems, as well as another that looks like 
&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2815493.2815501&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Probabilistic Bisimulation&lt;/a&gt;. Other related works include 
&lt;a href=&#34;https://aclanthology.org/2022.naacl-main.318/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Distillation for Language Models&lt;/a&gt;, 
&lt;a href=&#34;http://proceedings.mlr.press/v115/beckers20a.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Approximate Causal Abstractions&lt;/a&gt;, and 
&lt;a href=&#34;https://arxiv.org/abs/2106.02997&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Abstractions of Neural Networks&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;some-light-category-theory&#34;&gt;Some Light Category Theory&lt;/h2&gt;
&lt;p&gt;The prediction diagrams form a category:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An object is a state space and a transition function ⟨S,T:S→S⟩&lt;/li&gt;
&lt;li&gt;The morphisms between objects ⟨S,T:S→S⟩ and ⟨U,V:U→U⟩ are pairs ⟨F:S→U,G:U→S⟩ such that T=F;V;G&lt;/li&gt;
&lt;li&gt;The morphisms are indeed transitive and associative. There are identity morphisms for each object.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The objects are dynamical systems, and I am thinking of an agents as a special kind of dynamical system. The morphisms are choices of read-eval-print loops in the sense of 
&lt;a href=&#34;https://www.lesswrong.com/posts/kN2cFPaLQhExEzgeZ/repl-s-a-type-signature-for-agents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my repl post&lt;/a&gt;. It is interesting that this representation allows a single agent to have multiple different ways of interacting with the world. A morphism from A to B semantically means that agent B predicts agent A, or that agent A uses agent B.&lt;/p&gt;
&lt;p&gt;Also, if we allow the distinct domains and co-domain in the transition function, then the abstraction diagram category is called the arrow category. And the prediction diagram category is called the twisted arrow category.&lt;/p&gt;
&lt;p&gt;A two-sided conversation would correspond to a pair of morphism back and forth between two objects in the twisted arrow category. But having such a pair of morphisms is a very strong condition. So going forward we may want to formalize two agents as communicating in some shared part of their ontologies. I have some thoughts in this direction thanks to 
&lt;a href=&#34;https://www.lesswrong.com/users/davidad?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@davidad&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;thanks&#34;&gt;Thanks&lt;/h2&gt;
&lt;p&gt;A very incomplete list of thanks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/victor-lecomte?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Victor Lecomte&lt;/a&gt;  &amp;ndash; for pushing me away from the single F story, and back to the F and G story&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/sudo?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@sudo -i&lt;/a&gt;   &amp;ndash; for proof-reading and bouncing these ideas back and forth with me&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/davidad?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@davidad&lt;/a&gt; &amp;ndash; for telling me names of the categories I have been poking at, and for providing a direction for future theoretical (and hopefully empirical) work&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/victor-warlop?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Victor Warlop&lt;/a&gt; &amp;ndash; mentioned to me the idea of thinking of current AI&amp;rsquo;s as children&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/erik-jenner?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Erik Jenner&lt;/a&gt;  &amp;ndash; for suggesting that particular category (in the math sense) of the diagram doesn&amp;rsquo;t matter&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/malcolmocean?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@MalcolmOcean&lt;/a&gt;  &amp;ndash; sketched a view of opening up the AI to interaction with the world via REPLit, and commiserated with me about using an impoverished yes/no training signal&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/samuel-chen?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Samuel Chen&lt;/a&gt;  &amp;ndash; helped me brainstorm names for the aesthetic I want to cultivate in the alignment community. Pointed out that the limit of high bandwidth communication will create a symbiorganism.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/vivek-1?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Vivek Hebbar&lt;/a&gt;  &amp;ndash; for conversations about whether F and G should be inverses, and suggestions about some probabilistic state space content&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/aryan-bhattarai?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Aryan Bhattarai&lt;/a&gt;  &amp;ndash; Gave the idea of linear F and G.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Be the Agi</title>
      <link>http://scottviteri.github.io/post/be-the-agi/</link>
      <pubDate>Sat, 04 Feb 2023 11:54:02 -0700</pubDate>
      <guid>http://scottviteri.github.io/post/be-the-agi/</guid>
      <description>&lt;p&gt;Crossposted at 
&lt;a href=&#34;https://www.lesswrong.com/posts/FnfAnsAH6dva3kCHS/research-direction-be-the-agi-you-want-to-see-in-the-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.lesswrong.com/posts/FnfAnsAH6dva3kCHS/research-direction-be-the-agi-you-want-to-see-in-the-world&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Be the AGI you want to see in the world.&lt;/p&gt;
&lt;p&gt;Epistemic status: highly speculative, authors are not neuroscientists.&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;It may be possible to enhance human intelligence via a brain-computer interface (BCI). We could put electrodes into a human brain, connect those electrodes to an artificial neural network, and train that network to predict and write neural activations.&lt;/li&gt;
&lt;li&gt;This may present a technique for gradual uploading of human minds to computers that doesn’t require technology sufficient to create AGI in the first place.&lt;/li&gt;
&lt;li&gt;The purpose of this article is to elicit feedback on this idea and, if it seems promising, encourage more work in this area.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;introduction-goal-and-idea&#34;&gt;Introduction: Goal and Idea&lt;/h1&gt;
&lt;h2 id=&#34;goal&#34;&gt;Goal&lt;/h2&gt;
&lt;p&gt;We suspect that it may be possible to develop an enhancement system for human brains which satisfies the following desiderata:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Competitiveness: Our enhancement system must be powerful enough to help humans escape the acute AGI risk period. This desideratum may be satisfied if enhanced humans are able to find a solution to the prosaic AI alignment problem.&lt;/li&gt;
&lt;li&gt;Timeliness: Our enhancement system must be fully developed before the advent of AGI.&lt;/li&gt;
&lt;li&gt;Value preservation: Our enhancement system must not severely distort the core values of the enhanced human.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;idea&#34;&gt;Idea&lt;/h2&gt;
&lt;p&gt;Human brains are sharply constrained in size. Indeed, human birth is difficult because our brains (and thus skulls) are 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Obstetrical_dilemma&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;unusually large&lt;/a&gt; compared to other primates.&lt;/p&gt;
&lt;p&gt;If evolution progressed such that human brains evolved until it hit a size constraint, then it seems likely that further increasing human brain size would yield more than modest gains to human intelligence.&lt;/p&gt;
&lt;p&gt;One possible approach to increasing human brain size is by predicting real-time activations in the neocortex. The guiding intuition is that this is already how the brain extends itself, like how the 
&lt;a href=&#34;https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/Y3bkJ59j4dciiLYyw#4_6__Short_term_predictor__example__1__The_cerebellum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cerebellum currently predicts activations in the telencephalon&lt;/a&gt;. Namely, Neuralink hardware currently supports 
&lt;a href=&#34;https://www.jmir.org/2019/10/e16194/PDF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;three thousand EEG probes&lt;/a&gt; which can read from the brain, and Neuralink has the intention to add write access as well. Write access in this context means that the Neuralink probes could pipe brain activation data into a neural network which is trained to predict future brain activations, and the write probes could send those predictions as signals back into the brain. Scott believes that other hypotheses such as 
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2010.00019/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spike-timing dependent plasticity&lt;/a&gt; and 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2666703/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;predictive coding&lt;/a&gt; further support the hypothesis that the brain will learn to harness predictive signals from the EEG probes.&lt;/p&gt;
&lt;p&gt;Human brains are optimized for low energy 
&lt;a href=&#34;https://www.semanticscholar.org/paper/Cognitive-systems-optimize-energy-rather-than-Markman-Otto/bc0c4e091933fd00d17d0df0bd64247ae7d237aa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;expenditure rather than speed&lt;/a&gt;, and it can take 
&lt;a href=&#34;https://academic.oup.com/cercor/article/14/8/851/268309&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tens of milliseconds&lt;/a&gt; for a single serial communication between neurons. Artificial neural networks could run faster. For instance, if a language model the size of Chinchilla could predict neural activations, then current neural network hardware could speed serial computations in the brain by two orders of magnitude (0.7ms to 70ms, of course depending on the distance between the neurons).&lt;/p&gt;
&lt;p&gt;However, 3e3 probes is essentially nothing in the context of a brain with 8e10 neurons. To keep the orders of magnitude consistent, this is like expecting to read and predict 10 or 100 activations out of a neural network with 100M. Our core hypothesis is that this is not the correct neural network analogy: since both the neural prosthesis and your brain are simultaneously learning, it is more like jointly training the two networks. In this setting, the loss could flow into the helper network through the EEG probes, and the main network could learn to supply inputs to the helper network such that it is especially helpful. For instance, if the Neuralink network has access to a compass, the internet, or 4D-shape rotation software, the brain could potentially learn to take advantage of the new hardware.&lt;/p&gt;
&lt;p&gt;So suppose we could wire into a computational substrate in this way. Then not only could we get access to new senses, but also we might discover an increase in raw processing ability. One might experience this as a speed up of serial computation in the brain, resulting in substantial improvements in intelligence.&lt;/p&gt;
&lt;p&gt;This architecture seems highly scalable. Indeed, any advancement in the general class of predictive computers could be leveraged to improve our brain state predictor. If humans could stay roughly competitive with AI for longer, then humanity would have a better shot at survival into the long term future. An additional win condition here would involve enough modeling of the brain&amp;rsquo;s dynamics that we could essentially use this as an incremental uploading procedure.&lt;/p&gt;
&lt;h1 id=&#34;details-on-competitiveness&#34;&gt;Details on competitiveness&lt;/h1&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;Since we are bandwidth constrained, it may make sense to start out with a helper network that has been pre-trained on other brain data and/or the internet. But there is some reason to believe that predicting brain activations might be feasible even without pre-training, or the brain rerouting to make use of the helper network.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s Fermi estimate how much data we might need to predict neural activations. Maybe predicting sufficiently dense probes in the visual cortex is analogous to predicting next pixels on YouTube, and predicting sufficiently dense probes in Broca&amp;rsquo;s area is analogous to predicting the next word. Let&amp;rsquo;s naively assume that we need as much data as was used to train GPT3 Davinci, which used 
&lt;a href=&#34;https://arxiv.org/pdf/2005.14165.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;570GB&lt;/a&gt; (around 5.7e11 bytes or 5e12 bits) of text data after filtering. Suppose we read from 3e3 neurons. Each reading is in the form of a boolean array of length 3e3, denoting whether that neuron is currently undergoing an activation potential. Let&amp;rsquo;s take the readings 10 times per second to reduce correlation, giving us 3e4 bits/seconds. So we need 5e12 bits / 3e4 bits/sec ~ 1.7e8 seconds = 17 years. So this would need an order of magnitude improvement in the (very preliminary) Neuralink probe count or an order of magnitude improvement in data efficient of neural networks to be feasible.&lt;/p&gt;
&lt;p&gt;Some ways in which this could be false:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Predicting insufficiently dense probes is akin to predicting a single random pixel on YouTube &amp;ndash; this could be much harder than the original task, since we don&amp;rsquo;t have direct access to global information in the image&lt;/li&gt;
&lt;li&gt;Concepts in the brain are not fixed to neurons, but rather might move around faster than continual helper network learning can keep up with (again we are really not neuroscientists)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One reason to believe that 3,000 faster neurons could meaningfully help your cognition is that the current state-of-the-art artificial neural networks have hidden dimension only 
&lt;a href=&#34;https://arxiv.org/pdf/2005.14165.pdf#page=8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;~10,000 neurons wide&lt;/a&gt; (d_model), and can do very impressive things (e.g. strongly superhuman Go play, roughly human-level language abilities, memorizing large amounts of knowledge on the internet).&lt;/p&gt;
&lt;p&gt;Even in the case of an unlearnable signal, it is possible to get partial credit. GPT3 probably has some human-like conceptual structures since it was trained on so much human-generated, information-bearing signal. We could imagine just adding more human-generated data &amp;ndash; EEG, iEEG, fMRI, YouTube, Spotify, speech and movement data, eye tracking data, etc. Reinforcement learning from human feedback also jams more human information into the model. For relevant information for this idea, see Gwern&amp;rsquo;s post 
&lt;a href=&#34;https://www.reddit.com/r/reinforcementlearning/comments/9pwy2f/wbe_and_drl_a_middle_way_of_imitation_learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.reddit.com/r/reinforcementlearning/comments/9pwy2f/wbe_and_drl_a_middle_way_of_imitation_learning/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some particularly relevant examples:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/1906.11759&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Low-dimensional Embodied Semantics for Music and Language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.cs.cmu.edu/%7Eafyshe/papers/acl2014/jnnse_acl2014.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interpretable Semantic Vectors from a Joint Model of Brain- and Text-Based Meaning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://aclanthology.org/D16-1064.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Exploring Semantic Representation in Brain Activity Using Word Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These at least provide evidence for the possibility of using fMRI data, which provides spatially coarse (typically 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3073717/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3-4 mm&lt;/a&gt;) read access, but not write access. Even if we used a different kind of write access in conjunction with fMRI reading, the authors suspect that predicting fMRI data would not yield sufficiently detailed modeling of human cognition. Also, some of these papers talk about tracking the contents of attention (usually via eye movement and surface electrodes). Anecdotally, humans seem to fruitfully exploit low bandwidth channels by tracking each others&amp;rsquo; attention.&lt;/p&gt;
&lt;p&gt;In the scenario where we do not have enough direct write access, we could harness auxillary pathways such as projecting information onto a Google glass equivalent or via a 
&lt;a href=&#34;https://eagleman.com/science/sensory-substitution/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vest with targeted vibrations&lt;/a&gt;, potentially using a joint brain-image embedding space trained similarly to 
&lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CLIP&lt;/a&gt;. 
&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0306452221000129&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&lt;/a&gt; is an example of gaining hearing abilities through a vibrating wristband. The key challenge here is that the brain may be slow to learn to use this new signal. For example, it generally takes an adult 
&lt;a href=&#34;https://blindlowvision.org.nz/information/braille/learning-braille/#:~:text=Like%20any%20new%20skill%2C%20braille,ve%20got%20it%20for%20life.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;4 months&lt;/a&gt; to learn braille, and predicted brain data would be a much more complicated signal. It took 1 month for vibrating wristband users to distinguish between three sounds with 70% average accuracy.&lt;/p&gt;
&lt;p&gt;A key aspect of competitiveness is that humans cannot be expected to think as quickly as future AI&amp;rsquo;s if any part of the human is running on a biological neuron substrate. The authors imagine that adding more predictive capacity and bandwidth will lead to more and more of the human being implemented on the computer. At this point, perhaps the ML predictive model will contain enough of you that transitioning to fully digital would be more like a software port between between ruby and python than a full rewrite. The authors hope that this can be done in such a way to provide continuity of self. Much more research is needed here to flesh out what such a transition might look like. Continuity of self seems more tenuous without direct write access to the brain, even with indirect neural predictions such as through the visual system.&lt;/p&gt;
&lt;h2 id=&#34;current-state-of-the-technology&#34;&gt;Current state of the technology&lt;/h2&gt;
&lt;p&gt;As mentioned earlier, Neuralink currently has the capability to read from 
&lt;a href=&#34;https://www.jmir.org/2019/10/e16194/PDF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;three thousand neurons&lt;/a&gt;. The status of write access is unclear &amp;ndash;  
&lt;a href=&#34;https://www.jmir.org/2019/10/e16194/PDF#page=10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the paper says&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Modulating neural activity will be an important part of next-generation clinical brain-machine interfaces [39], for example, to provide a sense of touch or proprioception to neuroprosthetic movement control [40,41]. Therefore, we designed the Neuralink ASIC to be capable of electrical stimulation on every channel, although we have not demonstrated these capabilities here.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Existing solutions can both read and write with 
&lt;a href=&#34;https://www.nature.com/articles/s41551-018-0323-x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;128 electrodes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.nature.com/articles/s41551-022-00941-y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zhao et al&lt;/a&gt; recently harnessed advances in electrode probe flexibility to stably record 1000 neurons in a regular grid over 1 cubic millimeter.&lt;/p&gt;
&lt;p&gt;Many other brain measurement technologies exist besides electrodes, but electrodes are especially nice because they offer high spatial (10s of micrometers) and temporal (ms) resolution.&lt;/p&gt;
&lt;p&gt;Some EEG signals are predicted in 
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2018.00056/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Modern Machine Learning as a Benchmark for Fitting Neural Responses&lt;/a&gt; but it is from 2018 and uses LSTM&amp;rsquo;s, so it would be interesting to see how accurately we can do prediction on 
&lt;a href=&#34;https://github.com/KordingLab/spykesML&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/KordingLab/spykesML&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2206.11417.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Self-Supervised Learning of Brain Dynamics from Broad Neuroimaging Data&lt;/a&gt; predicts fMRI signals via pre-training on 
&lt;a href=&#34;https://openneuro.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenNeuro.org&lt;/a&gt; and 
&lt;a href=&#34;https://wiki.humanconnectome.org/display/PublicData/How&amp;#43;to&amp;#43;Access&amp;#43;Data&amp;#43;on&amp;#43;ConnectomeDB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Human Connectome Project&lt;/a&gt; datasets. This model achieves an F1 score of 0.9 on the downstream task of predicting mental states such as story-telling vs math, fear vs neural, and left vs right finger.&lt;/p&gt;
&lt;h1 id=&#34;details-on-alignment&#34;&gt;Details on alignment&lt;/h1&gt;
&lt;p&gt;In the extreme, this is a post about brain uploading, values and all. To avoid the failure mode of uploading your least favorite person, it may be possible to upload large numbers of people if this technology is sufficiently scalable. While Neuralink has automated the surgical process to allow greater scaling, many people are likely to gawk at the notion of invasive surgery. Fortunately, there are a whole spectrum of possible merging techniques possible, from Neuralink to fine-tuning a language model on your own speech and writing.&lt;/p&gt;
&lt;p&gt;But the farther we are from Neuralink on this spectrum, the more suspect the alignment properties. Even within the Neuralink story, alignment might depend on details of the training procedure. If the prediction is trained from scratch, then it may be analogous to simply having more brain. But if, for sample efficiency reasons, the neural activations are first embedded into a pre-trained language model&amp;rsquo;s latent space, a person&amp;rsquo;s internal models may end up distorted. We might be able to mitigate this concern by using contrastive learning to encode the Neuralink user&amp;rsquo;s brain activations into a joint latent space with internet text. We would welcome theory work to understand how these and other options affect alignment properties.&lt;/p&gt;
&lt;h1 id=&#34;future-work&#34;&gt;Future Work&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Run main and helper neural networks and see how many connection points are needed to improve performance
a. Try this with and without training the helper and main networks jointly
b. See how the difficulty of prediction changes as a function of the distance between the inputs and outputs of the helper network&lt;/li&gt;
&lt;li&gt;Find online repositories of EEG data and experiment to check the difficulty of the prediction task
a. Check the usefulness of pre-training in this regime&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.nature.com/articles/d41586-022-03229-y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dishbrain&lt;/a&gt; is the name of an experiment where a plate of biological neurons learned to play pong in five minutes by controlling the paddle height with spiking patterns. We would like to know if neural predictions would allow the neurons to learn the game more quickly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The authors would like to thank 
&lt;a href=&#34;https://www.lesswrong.com/users/adam-shai?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Adam Shai&lt;/a&gt; and 
&lt;a href=&#34;https://www.lesswrong.com/users/steve2152?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Steven Byrnes&lt;/a&gt; for feedback on earlier drafts, as well as Joscha Bach, 
&lt;a href=&#34;https://www.lesswrong.com/users/d0themath?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Garrett Baker&lt;/a&gt;, 
&lt;a href=&#34;https://www.lesswrong.com/users/nicholaskees?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@NicholasKees&lt;/a&gt;, 
&lt;a href=&#34;https://www.lesswrong.com/users/erik-jenner?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Erik Jenner&lt;/a&gt;, Rylan Schaeffer, and Victor Lecomte for fruitful conversations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Repls and Elk</title>
      <link>http://scottviteri.github.io/post/repls-and-elk/</link>
      <pubDate>Wed, 16 Feb 2022 12:02:47 -0700</pubDate>
      <guid>http://scottviteri.github.io/post/repls-and-elk/</guid>
      <description>&lt;p&gt;Crossposted at 
&lt;a href=&#34;https://www.lesswrong.com/posts/C5PZNi5fueH2RC6aF/repl-s-and-elk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.lesswrong.com/posts/C5PZNi5fueH2RC6aF/repl-s-and-elk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In my previous post I talked about read-eval-print loops as providing a type signature for agents. I will now explain how you can quickly transition from this framework to an ELK solution. Notation is imported from that post.&lt;/p&gt;
&lt;p&gt;Imagine we have two agents, a human and a strong AI, denoted H and M respectively. They both interact with the environment in lockstep, according to the following diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/0f9ecde8a04090365f96c563e8ba07c710666d50ae9e3152.png/w_454&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have the human&amp;rsquo;s utility function UH:SH→Q, which is defined on the human&amp;rsquo;s model of reality. We would like to lift UH to a version UM:SM→Q that the machine can use to influence the world in way that is agreeable to the human, which we can do by learning a mapping F : SM→SH and deriving UM=F∘UH.&lt;/p&gt;
&lt;p&gt;But we haven&amp;rsquo;t yet said what properties we want the ontology map F to have. I want to call two concepts sh and sm equal if they act the same with respect to transformation: ∀f. f(sh) = f(sm) → sh = sm. The issue is that since the concepts have different types we cannot feed them as arguments to the same function. So instead let&amp;rsquo;s say that ∀s:S, EvalH(sh, ReadH(s)) = EvalM(sm, ReadM(s)) → sh = sm. But now we are back to the same problem where we are trying to compare concepts in two different ontologies. But this does give us a kind of inductive step where we can transfer evidence of equality between concept pairs (sh, sm) and (sh&amp;rsquo;, sm&amp;rsquo;). I also believe that this kind of a coherence argument is the best we can do, since we are not allowed to peer into the semantic content of particular machine or human states when constructing the ontology map.&lt;/p&gt;
&lt;p&gt;Consider the following two graphs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2a481b191941494551eaa75a90c30557a1f979c43150ede0.png/w_483&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;My intuition is that even if I don&amp;rsquo;t know the labels of the above graphs, I can still infer that the bottom nodes correspond to each other. And the arrows that I get in the context of ELK are the agents&amp;rsquo; Eval transitions, leading to the following commutative diagram specification for F.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/12dcebccea5df896b167763089b06164805244b0e4ba9700.png/w_340&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can learn an ontology map F : SM→SH by minimizing the difference between two paths from a state sm, one in which the machine&amp;rsquo;s prediction function is used and one in which the human&amp;rsquo;s prediction function is used.  Concretely, I propose minimizing Dist(sh1,sh2) + λ |U(sh1)-U(sh2)| where sh1 = F(EvalM(sm,om)) and sh2 = EvalH(F(sm),oh), Dist is a distance metric in SH, and observations om and oh are generated by the same underlying state S.&lt;/p&gt;
&lt;p&gt;If you are interested in getting more detail and why I believe this circumvents existing counterexamples, please check out the full proposal.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Repls</title>
      <link>http://scottviteri.github.io/post/repls/</link>
      <pubDate>Tue, 15 Feb 2022 12:06:43 -0700</pubDate>
      <guid>http://scottviteri.github.io/post/repls/</guid>
      <description>&lt;p&gt;Cross-posted at 
&lt;a href=&#34;https://www.lesswrong.com/posts/kN2cFPaLQhExEzgeZ/repl-s-a-type-signature-for-agents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.lesswrong.com/posts/kN2cFPaLQhExEzgeZ/repl-s-a-type-signature-for-agents&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Read-eval-print loops are used to interact with programming languages as well as in the shell to interact with your computer. I claim that they are a reasonable starting point for reasoning about agents. This is a framing that all but wrote my ELK proposal for me, and has been paying rent in other research domains as well.&lt;/p&gt;
&lt;p&gt;Let S be the set of world states, and assume a environment transition function T : S × A → S, where Aₓ is a set of actions.
We define an agent X to be a 3-tuple of functions Readₓ, Evalₓ, and Printₓ, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Readₓ is a surjective &amp;ldquo;abstraction&amp;rdquo; function of type S → Oₓ, where Oₓ is a type for X&amp;rsquo;s abstracted observations.&lt;/li&gt;
&lt;li&gt;Evalₓ is a prediction function of type Sₓ × Oₓ → Sₓ, where Sₓ represents the type of X&amp;rsquo;s internal knowledge of the environment.&lt;/li&gt;
&lt;li&gt;Printₓ is a function from Sₓ to Aₓ, where Aₓ denotes X&amp;rsquo;s actions on the environment. If there are multiple agents, we assume that an action Aₓ contains one action from each agent.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The environment and agent start in initial states S⁰ and Sₓ⁰, respectively. At each time-step, the agent observes the current state of the environment with its sensors, updates its worldview using its prediction function, produces an action, and then the universe uses that action to produce a next universe state. This process is depicted in the figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f2026cdc159ea5ff4a0a672250cca96cdb32c0015bc7ec80.png/w_648&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Notice that in the above diagram, the transition functions of the agent and the environment look similar. In fact they are dual, and we can show this by considering the agent&amp;rsquo;s Readₓ to be the environment&amp;rsquo;s Print, and the agent&amp;rsquo;s observation type Oₓ to be the environment&amp;rsquo;s action type A.&lt;/p&gt;
&lt;p&gt;Env       := { Print  : S → Aₓ,   Eval  : S × Aₓ → S }
Agentₓ := { Printₓ : Sₓ → Aₓ, Evalₓ : Sₓ × A → Sₓ }&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a27cbc6a042caf4ab560dbbb72afd834ade6e59fb159663d.png/w_559&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Both the agent and environment start in a given initial state, pick an action based on that state, feed that action to each other&amp;rsquo;s transition functions, and transition to the next state.&lt;/p&gt;
&lt;p&gt;This interaction can be drawn as a game tree where the agent and the environment are selecting which of each other&amp;rsquo;s next branches to follow.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c4e5303c127547c49a07b55f5a5bda889a6d3d4eff33cd86.png/w_657&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The agent and environment run in lockstep, each round simultaneously printing and thereby choosing their partner&amp;rsquo;s next branch. If you want to think of the environment as giving a reward, that reward can be a function of your whole action history, which put you in a particular branch of the physics game tree.&lt;/p&gt;
&lt;p&gt;There is much more to say about this setup. I may add future posts on how this leads to an ELK proposal, how it helps bring to light common pitfalls in others&amp;rsquo; ELK proposals that I have seen so far, a similar setup in the embedded agency setting, details about nested compositional REPL&amp;rsquo;s, the connection to polynomial functors and coinductive datatypes, and maybe even a diagrammatic REPL programming language. Please let me know which if any you are interested in and I can post accordingly.&lt;/p&gt;
&lt;p&gt;Huge thanks to David Spivak for helping me with these ideas, as well as Gabriel Poesia and John Wentworth.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
