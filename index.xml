<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scott Viteri</title>
    <link>http://scottviteri.github.io/</link>
      <atom:link href="http://scottviteri.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Scott Viteri</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2023</copyright><lastBuildDate>Tue, 01 Aug 2023 23:30:23 -0700</lastBuildDate>
    <image>
      <url>http://scottviteri.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Scott Viteri</title>
      <link>http://scottviteri.github.io/</link>
    </image>
    
    <item>
      <title>CS 362: Research in AI Alignment</title>
      <link>http://scottviteri.github.io/teaching/courses/cs362/</link>
      <pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate>
      <guid>http://scottviteri.github.io/teaching/courses/cs362/</guid>
      <description>&lt;div style=&#34;display: flex; flex-direction: row;&#34;&gt;
  &lt;div style=&#34;flex: 1;&#34;&gt;
&lt;h2 id=&#34;course-information&#34;&gt;Course Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Instructor Name(s)&lt;/em&gt;: Scott Viteri&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Teaching Assistants&lt;/em&gt;: Victor Lecomte, Gabe Mukobi, Peter Chatain&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Course Faculty Sponsor&lt;/em&gt;: Clark Barrett&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Lecture&lt;/em&gt;: Monday 3:00-4:30pm, B067 Mitchell Earth Science (In-person only)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Optional Office Hours&lt;/em&gt;: Large Language Model productivity meetings at 1-2PM in Gates 200&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Graduate-level course or advanced undergraduates (contact course instructor)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;3 Units, Spring 2023, 
&lt;a href=&#34;https://explorecourses.stanford.edu/search?q=CS&amp;#43;362&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ExploreCourses&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&#34;flex: 0 0 30%; margin-left: 20px;&#34;&gt;
&lt;img src=&#34;http://scottviteri.github.io/img/cs362.png&#34; alt=&#34;AI Alignment&#34; style=&#34;width: 200px;&#34;&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h2 id=&#34;course-description&#34;&gt;Course Description&lt;/h2&gt;
&lt;p&gt;In this course we will explore the current state of research in the field of AI alignment, which seeks to bring increasingly intelligent AI systems in line with human values and interests. The purpose of this course is to encourage the development of new ideas in this field, where a dominant paradigm has not yet been established. The format will be weekly lectures in which speakers present their current research approaches.&lt;/p&gt;
&lt;p&gt;The assignment structure will be slightly unusual: each week students will have a choice between a problem set and a short research assignment based on the weekly guest speaker’s research area. For the research assignment, students will start with the abstract of a relevant AI alignment paper or blog post and create a blog post or Github repository describing how they would continue the paper. The final weekly assignment will be an extension of one of the previous weeks’ work. Therefore this course requires research experience, preferably using mathematical and programming tools (e.g. Python, PyTorch, calculus), and is a graduate level course, open to advanced undergraduates.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites:&lt;/h2&gt;
&lt;p&gt;Any one of the following: CS223a, CS224n, CS224w, CS224u, CS227b, CS228, CS229, CS229t, CS229m, CS231a, CS231n, CS234, CS236, CS237a, CS281&lt;/p&gt;
&lt;p&gt;In addition to the above, strong ability to do independent research will be necessary, preferably using mathematical and programming tools (e.g. Python, PyTorch, calculus).&lt;/p&gt;
&lt;h2 id=&#34;syllabus&#34;&gt;Syllabus&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;th&gt;Week&lt;/th&gt;
&lt;th&gt;Name&lt;/th&gt;
&lt;th&gt;Topic&lt;/th&gt;
&lt;th&gt;Suggested Assignment Prompt&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;April 3 (Mon)&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Scott Viteri&lt;/td&gt;
&lt;td&gt;Overview of Course and AI Safety&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://wp.nyu.edu/arg/why-ai-safety/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bowman 2022&lt;/a&gt;, 
&lt;a href=&#34;https://bounded-regret.ghost.io/more-is-different-for-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Steinhardt 2022&lt;/a&gt;, 
&lt;a href=&#34;https://www.youtube.com/watch?v=UbruBnv3pZU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Carlsmith 2022&lt;/a&gt;, 
&lt;a href=&#34;https://hai.stanford.edu/events/hai-weekly-seminar-vael-gates&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gates 2022&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;April 10&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Adam Gleave (UC Berkeley)&lt;/td&gt;
&lt;td&gt;Inverse Reinforcement Learning&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2203.11409.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gleave, Toyer 2022&lt;/a&gt;, 
&lt;a href=&#34;https://arxiv.org/pdf/2203.07472.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gleave 2022&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;April 17&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Andrew Critch (UC Berkeley)&lt;/td&gt;
&lt;td&gt;Multiagent problems&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/abs/parametric-resourcebounded-generalization-of-lobs-theorem-and-a-robust-cooperation-criterion-for-opensource-game-theory/16063EA7BFFEE89438631B141E556E79&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Critch 2019&lt;/a&gt;, 
&lt;a href=&#34;https://arxiv.org/pdf/2012.14536.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fickinger, Zhuang, Critch et al 2020&lt;/a&gt;, 
&lt;a href=&#34;https://arxiv.org/pdf/1609.03543.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Garrabrandt, Critch et al 2016&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;April 24&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Andy Jones (Anthropic)&lt;/td&gt;
&lt;td&gt;Empirical alignment - interpretability&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/abs/2112.00861&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Askell 2021&lt;/a&gt;, 
&lt;a href=&#34;https://transformer-circuits.pub/2021/framework/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Elhage, Nanda 2021&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;May 1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;Dan Hendrycks (Center for AI Safety)&lt;/td&gt;
&lt;td&gt;Robustness and Generalization in AI Systems&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/abs/2109.13916&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hendrycks 2022&lt;/a&gt;, 
&lt;a href=&#34;https://arxiv.org/abs/2009.03300&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hendrycks 2021a&lt;/a&gt;, 
&lt;a href=&#34;https://arxiv.org/abs/2006.16241&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hendrycks 2021b&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;May 8&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;Alex Turner (UC Berkeley)&lt;/td&gt;
&lt;td&gt;Shard theory&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.alignmentforum.org/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Turner 2022&lt;/a&gt;, 
&lt;a href=&#34;https://www.alignmentforum.org/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pope, Turner 2022&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;May 15&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;Laria Reynolds (Conjecture)&lt;/td&gt;
&lt;td&gt;Empirical alignment research with LLM&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2102.07350.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reynolds, McDonell 2021&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;May 22&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;John Wentworth (independent researcher)&lt;/td&gt;
&lt;td&gt;Agent Foundations and Abstractions&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://www.lesswrong.com/posts/BzYmJYECAc3xyCTt6/the-plan-2022-update&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wentworth 2022a&lt;/a&gt;, 
&lt;a href=&#34;https://www.lesswrong.com/s/TLSzP4xP42PPBctgw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wentworth 2022b&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;May 29&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;Memorial Day — no class&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Jun 5&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;Evan Hubinger (Anthropic)&lt;/td&gt;
&lt;td&gt;Mesa-Optimization and Inner Alignment&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://arxiv.org/abs/1906.01820&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hubinger, Mikulik et al 2019&lt;/a&gt;, 
&lt;a href=&#34;https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hubinger 2021&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>STS 20SI: Advanced AI Alignment</title>
      <link>http://scottviteri.github.io/teaching/courses/sts20si/</link>
      <pubDate>Tue, 28 Feb 2023 00:00:00 +0000</pubDate>
      <guid>http://scottviteri.github.io/teaching/courses/sts20si/</guid>
      <description>&lt;h2 id=&#34;course-information&#34;&gt;Course Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Course Coordinator&lt;/em&gt;: Gabriel Mukobi&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Faculty Sponsor&lt;/em&gt;: Paul N. Edwards (STS)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Content&lt;/em&gt;: This advanced course explores the frontier of current AI alignment research directions and helps students develop their own inside view on AI safety research. The course includes targeted readings, weekly group discussions, technical alignment exercises, and a final literature review or research proposal.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a Discussion Facilitator for this course, I guided students through the complex issues surrounding AI alignment, helped them understand current research directions, and provided guidance on their projects.&lt;/p&gt;
&lt;p&gt;For more information about the course, please visit the 
&lt;a href=&#34;https://explorecourses.stanford.edu/search?view=catalog&amp;amp;filter-coursestatus-Active=on&amp;amp;page=0&amp;amp;catalog=&amp;amp;academicYear=&amp;amp;q=STS&amp;#43;20SI%3A&amp;#43;Advanced&amp;#43;AI&amp;#43;Alignment&amp;amp;collapse=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;course page&lt;/a&gt; and the 
&lt;a href=&#34;https://docs.google.com/document/d/1zDYtBpoJqTw7qwrfRd5hqyoiaQrfL6HSoFdKbEP21l0/edit#heading=h.o0d20s4gildq&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;syllabus&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AGI Safety Fundamentals: AI Alignment Course</title>
      <link>http://scottviteri.github.io/teaching/courses/fundamentals/</link>
      <pubDate>Wed, 15 Jun 2022 00:00:00 +0000</pubDate>
      <guid>http://scottviteri.github.io/teaching/courses/fundamentals/</guid>
      <description>&lt;script src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js&#34;&gt;&lt;/script&gt;
&lt;h2 id=&#34;course-information&#34;&gt;Course Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Course Organizer&lt;/em&gt;: Richard Ngo&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Content&lt;/em&gt;: The AGI Safety Fundamentals: Alignment Course provides a high-level understanding of the AI alignment problem and some of the key research directions which aim to solve it. The course consists of 7 weeks of readings and discussion sessions (plus one optional week introducing the fundamentals of machine learning), and a final project. No background machine learning knowledge is required, but participants are expected to have some fluency in basic statistics and mathematical notation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I had the opportunity to mentor for this program twice. For more information about the course, please visit the 
&lt;a href=&#34;https://www.agisafetyfundamentals.com/ai-alignment-curriculum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official course page&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>STS 10SI: Introduction to AI Alignment</title>
      <link>http://scottviteri.github.io/teaching/courses/sts10si/</link>
      <pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate>
      <guid>http://scottviteri.github.io/teaching/courses/sts10si/</guid>
      <description>&lt;h2 id=&#34;course-information&#34;&gt;Course Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Course Coordinator&lt;/em&gt;: Gabriel Mukobi&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Faculty Sponsor&lt;/em&gt;: Paul N. Edwards (STS)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Content&lt;/em&gt;: This 8-week course explores the pressing ethical questions about what will happen if AI systems aren’t aligned with our values. The course includes targeted readings, weekly group discussions, and an optional project. Basic knowledge about machine learning helps but is not required.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a Discussion Facilitator for this course, I helped students understand the complex issues surrounding AI alignment, and provided guidance on their projects.&lt;/p&gt;
&lt;p&gt;For more information about the course, please visit the 
&lt;a href=&#34;https://explorecourses.stanford.edu/search?view=catalog&amp;amp;filter-coursestatus-Active=on&amp;amp;page=0&amp;amp;catalog=&amp;amp;academicYear=&amp;amp;q=STS&amp;#43;10SI%3A&amp;#43;Introduction&amp;#43;to&amp;#43;AI&amp;#43;Alignment&amp;amp;collapse=&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;course page&lt;/a&gt; and the 
&lt;a href=&#34;https://docs.google.com/document/d/1NX0DlZRzD3NP7tBeLjMh76w7-w2s8SxV3wj0P7EYpKY/edit#&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;syllabus&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Dream for Transformers</title>
      <link>http://scottviteri.github.io/post/deep-dream-for-transformers/</link>
      <pubDate>Tue, 01 Aug 2023 23:30:23 -0700</pubDate>
      <guid>http://scottviteri.github.io/post/deep-dream-for-transformers/</guid>
      <description>&lt;head&gt;
  &lt;title&gt;GPT2 Visualization&lt;/title&gt;
  &lt;style&gt;
    body {
      font-family: Arial, sans-serif;
    }
&lt;pre&gt;&lt;code&gt;#modelContainer {
  margin: 20px;
}

#info {
  margin: 20px;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;h1&gt;GPT2 Model Structure&lt;/h1&gt;
  &lt;div id=&#34;modelContainer&#34;&gt;
    &lt;!-- Our D3.js visualization will be inserted here --&gt;
  &lt;/div&gt;
  &lt;div id=&#34;info&#34;&gt;
    &lt;p id=&#34;layerIndex&#34;&gt;&lt;/p&gt;
    &lt;p id=&#34;neuronIndex&#34;&gt;&lt;/p&gt;
  &lt;/div&gt;
  &lt;script src=&#34;https://d3js.org/d3.v7.min.js&#34;&gt;&lt;/script&gt;
  &lt;script&gt;
    fetch(&#39;GPT2NeuronExplanations.json&#39;)
    .then(response =&gt; response.json())
    .then(data =&gt; {
      // Save the data for later use
      window.neuronExplanations = data;
      // Model Creation
      const GPT2Model = {
        transformer: {
          wte: { type: &#39;Embedding&#39; },
          h: {
            moduleList: {
              GPT2Blocks: []
            }
          }
        }
      };

      // Function to create neurons
      function createNeurons(numNeurons) {
        let neurons = [];
        for (let i = 0; i &lt; numNeurons; i++) {
          neurons.push({ index: i });
        }
        return neurons;
      }

      // Function to create GPT2Blocks
      function createGPT2Blocks(numBlocks, numNeurons) {
        for (let i = 0; i &lt; numBlocks; i++) {
          let block = {
            index: i,
            mlp: {
              c_fc: {
                type: &#39;Conv1D&#39;,
                neurons: createNeurons(numNeurons)
              }
            }
          };
          GPT2Model.transformer.h.moduleList.GPT2Blocks.push(block);
        }
      }

      createGPT2Blocks(6, 10);

      // Function to render the GPT2 model
      function renderModel(model, svg) {
        // For each layer in the model...
        model.transformer.h.moduleList.GPT2Blocks.forEach((block, i) =&gt; {
          // Create a group for the layer
          const layerGroup = svg.append(&#34;g&#34;)
            .attr(&#34;transform&#34;, `translate(0,${i * 100})`);

          // For each neuron in the layer...
          block.mlp.c_fc.neurons.forEach((neuron, j) =&gt; {
            // Create a circle for the neuron
            layerGroup.append(&#34;circle&#34;)
              .attr(&#34;cx&#34;, 50 + j * 50)
              .attr(&#34;cy&#34;, 50)
              .attr(&#34;r&#34;, 20)
              .on(&#34;click&#34;, function() {
                // Display the layer and neuron index when clicked
                d3.select(&#34;#layerIndex&#34;).text(`Layer: ${i}`);
                d3.select(&#34;#neuronIndex&#34;).text(`Neuron: ${j}`);
                const explanation = window.neuronExplanations[i][j];
                d3.select(&#34;#explanation&#34;).text(explanation);
              });
          });
        });
      }

      // Set up the SVG canvas and render the model
      const svg = d3.select(&#34;#modelContainer&#34;)
        .append(&#34;svg&#34;)
        .attr(&#34;width&#34;, 600)
        .attr(&#34;height&#34;, 500);

      renderModel(GPT2Model, svg);
    });
  &lt;/script&gt;
&lt;/body&gt;
</description>
    </item>
    
    <item>
      <title>Joint Text-EEG Embeddings</title>
      <link>http://scottviteri.github.io/post/joint-text-eeg-embeddings/</link>
      <pubDate>Sat, 24 Jun 2023 11:34:12 -0700</pubDate>
      <guid>http://scottviteri.github.io/post/joint-text-eeg-embeddings/</guid>
      <description>&lt;p&gt;Here is a proposal I pitched to the Emotiv EEG company, since they have a bunch of EEG data and infrastructure to pay customers to help with distributed data collection. They were interested but we ended up stopping short of having me work there for the summer.&lt;/p&gt;
&lt;p&gt;My motivation is that the main approach to AI alignment right now is RLHF, but one key limitation is the bandwidth constraint of humans ranking text generated by large language models. It would be nice to have joint text-eeg embeddings in order to increase that bandwidth. I think there are ways of doing this naively that would be pretty bad &amp;ndash; for example, if we did RLHF based on people&amp;rsquo;s internal reactions to a piece of text, I could see that optimizing to align with parts of ourselves that we don&amp;rsquo;t identify with (eg stimula that are short-term pleasurable).&lt;/p&gt;
&lt;p&gt;But I think there is a version of this where we scale the amount of communication between AIs and people, as AIs become more direct personal assistants. I am consciously thinking about EEG as opposed to the more reasonable fMRI because EEG is much more likely to be integrated into everyday use, which can lead to much for data being collected, which has positive feedback effects. Also as far as I can tell, it hasn&amp;rsquo;t really been tried with transformers before somehow.&lt;/p&gt;
&lt;p&gt;I have this broad intuition that more communication is good, but is is also possible in the short term for increased communication to lead to a smaller integral of total communication. I admit that sociopaths exist, and that generally speaking your don&amp;rsquo;t want to give our your social security number which is indeed communication. Maybe a crux for communication is that in the process of sharing your concepts, you also share your values, since values are a particular kind of concept. I don&amp;rsquo;t think Hitler is a good example for othogonality, because his values were not really disjoint from his conceptual structure of how the world works.&lt;/p&gt;
&lt;p&gt;In any event, there is probably a better version of the following proposal where you don&amp;rsquo;t create a joint embedding space but rather use EEG embeddings to help a pretrained LM do better prediction of next word. I imagine this is a more reasonable task do to the low spatial resolution of EEG.&lt;/p&gt;
&lt;h1 id=&#34;a-proposal-for-developing-a-joint-text-eeg-embedding-model-using-emotiv-eeg-headsets&#34;&gt;A Proposal for Developing a Joint Text-EEG Embedding Model using Emotiv EEG Headsets&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Author: Scott Viteri&lt;/strong&gt;
&lt;strong&gt;Affiliation: Stanford University&lt;/strong&gt;
&lt;strong&gt;Date: March 27th, 2023&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The goal of this project is to create a dataset and develop a joint text-EEG embedding model using Emotiv EEG headsets. This dataset will consist of pairs of language model completions and the corresponding EEG headset data when a participant reads those completions. By learning a joint EEG/text embedding space, we aim to align machine learning models with human values, allowing AI systems to better emulate human cognitive processes and values. Additionally, this project will provide an incentive for people to buy Emotiv headsets, as they can use the joint embedding model to fine-tune language models with their own data, creating a more personalized AI assistant.&lt;/p&gt;
&lt;h2 id=&#34;emotivs-market-opportunity&#34;&gt;Emotiv&amp;rsquo;s Market Opportunity&lt;/h2&gt;
&lt;p&gt;The joint text-EEG embedding model developed using Emotiv EEG headsets has the potential to significantly increase the demand for Emotiv&amp;rsquo;s products. If this joint embedding becomes generally available, it would provide a strong incentive for consumers to purchase Emotiv headsets, particularly those with an earbud form factor. The ability to fine-tune language models on their own thoughts using the embedding model and the headset would be a major selling point. By integrating the headset with smartphones or internet-connected devices, users could interact with language models in real-time using their thoughts as context, effectively creating a next-generation smart assistant experience. This innovation represents a natural progression from voice-based smart assistants, and the potential market size is vast.&lt;/p&gt;
&lt;p&gt;By collaborating with Emotiv, we can ensure that the joint text-EEG embedding model is optimized for use with their EEG headsets. This will create a unique value proposition for Emotiv&amp;rsquo;s products, differentiating them from competitors and driving increased sales. Moreover, the success of this project could lead to further research and development opportunities, reinforcing Emotiv&amp;rsquo;s position as a leader in the field of brain-computer interfaces and AI-driven technologies.&lt;/p&gt;
&lt;h2 id=&#34;cost-estimation&#34;&gt;Cost Estimation&lt;/h2&gt;
&lt;p&gt;The total cost estimation for this project is around $120K. This is broken down as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$50K for the joint brain/EEG embeddings&lt;/li&gt;
&lt;li&gt;$30K for an EEG predictive model&lt;/li&gt;
&lt;li&gt;$40K to pay for the project lead&amp;rsquo;s summer internship&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These costs can be justified by the potential to further solidify Emotiv&amp;rsquo;s position as a leader in the brain-computer interface field while opening up new avenues of innovation, connecting human thought to computers, AI, and each other in fundamentally different ways from before.&lt;/p&gt;
&lt;h3 id=&#34;data-collection-cost&#34;&gt;Data Collection Cost&lt;/h3&gt;
&lt;p&gt;We estimate that the cost per data point for labeling is $30/hr, assuming a labeler can read and label a paragraph within a minute. This results in 2 samples per dollar. To potentially reduce costs and increase participant engagement, we can consider allowing participants to choose their reading material. This can be achieved by providing a browser extension that displays one paragraph or sentence at a time or by hosting the interface on the Emotiv labs platform, where participants can input the URL of the content they want to read. While this approach might introduce some bias in the dataset, it could be worth it if we are participant-limited and can ensure a diverse set of readings, including content that elicits varying emotional responses.&lt;/p&gt;
&lt;h3 id=&#34;number-of-data-points-and-model-training-cost&#34;&gt;Number of Data Points and Model Training Cost&lt;/h3&gt;
&lt;p&gt;For the contrastive model, we would naively expect 400 million pairs, similar to the CLIP model, which would cost $200M to annotate. However, since we can use OpenAI&amp;rsquo;s existing embedding model for the text to vector function, we might only need 100 million pairs, which would still cost&lt;/p&gt;
&lt;p&gt;$50M to annotate &amp;ndash; an amount still out of range.&lt;/p&gt;
&lt;p&gt;To reduce costs, we can pretrain a model to predict the next EEG signals using existing data repositories. By removing the last layer(s) of this predictive model, we can use it as a starting point for the brain-to-vector function. This approach could save three orders of magnitude, putting the cost at $50K. However, we would need a good predictive model for future brain data based on past data. Estimating the difficulty of this task to be around the difficulty of language modeling, the cost for a baseline competence 110M parameter model would be about $30K (
&lt;a href=&#34;https://arxiv.org/abs/2004.08900%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2004.08900)&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;splitting-the-problem-pre-training-and-contrastive-training&#34;&gt;Splitting the Problem: Pre-training and Contrastive Training&lt;/h3&gt;
&lt;p&gt;We split the problem into pre-training and contrastive training steps for several reasons. First, pre-training allows us to leverage existing data repositories to build a strong foundation for the brain-to-vector function at a lower cost. This step reduces the complexity of the problem and enables us to focus on learning a meaningful representation from brain data. Second, the contrastive training step refines the model to learn the joint embedding space between text and EEG data, ensuring that the model can effectively capture relevant information from both modalities. Splitting the problem this way facilitates a more efficient and cost-effective approach to developing the joint text-EEG embedding model.&lt;/p&gt;
&lt;h3 id=&#34;incremental-spending&#34;&gt;Incremental Spending&lt;/h3&gt;
&lt;p&gt;To reduce the risk of financial loss, the project can be executed incrementally, allowing for adjustments and the possibility of stopping data collection if necessary. The following strategies will be employed:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Learn the predictive model of the EEG signal before starting human data collection.&lt;/li&gt;
&lt;li&gt;Use the EEG predictive model to create baselines and possibly cheaper ways to come up with translations between EEG and language.&lt;/li&gt;
&lt;li&gt;Train the contrastive model as data is collected, allowing for adjustments or stopping data collection if the model does not show progress.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;data-collection-procedure&#34;&gt;Data Collection Procedure&lt;/h2&gt;
&lt;h3 id=&#34;participant-recruitment&#34;&gt;Participant Recruitment&lt;/h3&gt;
&lt;p&gt;Recruit a diverse group of participants, ensuring a representative sample of different ages, genders, and cultural backgrounds. Obtain informed consent from all participants, and ensure they understand the purpose of the study and their right to withdraw at any time.&lt;/p&gt;
&lt;h3 id=&#34;onsite-collection-protocol&#34;&gt;Onsite Collection Protocol&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Equip each participant with an Emotiv EEG headset, ensuring a comfortable fit and proper electrode placement. Use EEG headsets with multiple channels to capture a broader range of brain activity. Additionally, integrate the headset with a smartphone or computer application to facilitate real-time data collection and language model interaction.&lt;/li&gt;
&lt;li&gt;Prepare a diverse set of texts covering various topics and complexities to elicit a wide range of cognitive responses from participants. The texts can be generated by language models or curated from existing sources.&lt;/li&gt;
&lt;li&gt;Randomize the order of text presentations for each participant to minimize any order effects that may influence the EEG readings.&lt;/li&gt;
&lt;li&gt;For each participant, follow these steps:
a. Present a text to the participant and ask them to read it silently.
b. Record the participant&amp;rsquo;s EEG data while they read the text.
c. Store the (text, EEG data) pair securely.&lt;/li&gt;
&lt;li&gt;Repeat this process for several texts per participant, ensuring a sufficient number of (text, EEG data) pairs are collected.&lt;/li&gt;
&lt;li&gt;Store the data securely, anonymizing it to protect participant privacy.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;remote-collection-protocol&#34;&gt;Remote Collection Protocol&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Equip each participant with an Emotiv EEG headset and integrate it with a browser extension or an interface hosted on the Emotiv labs platform.&lt;/li&gt;
&lt;li&gt;Allow participants to input the URL of the content they want to read, ensuring a diverse set of readings and a wide range of cognitive responses.&lt;/li&gt;
&lt;li&gt;Follow the data collection, preprocessing, and model training steps outlined in the Onsite Collection Procedure section.&lt;/li&gt;
&lt;li&gt;Monitor the dataset&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;for biases and take corrective measures as needed to ensure a balanced representation of content and emotional responses.&lt;/p&gt;
&lt;p&gt;By implementing this data collection procedure, we can potentially reduce costs, increase participant engagement, and encourage a diverse range of readings. However, it is essential to be aware of potential biases and address them accordingly to ensure the quality and usefulness of the joint text-EEG embedding model.&lt;/p&gt;
&lt;h2 id=&#34;model-training&#34;&gt;Model Training&lt;/h2&gt;
&lt;h3 id=&#34;eeg-data-preprocessing&#34;&gt;EEG Data Preprocessing&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Perform artifact removal and noise reduction on the collected EEG data to ensure high-quality signals for subsequent analysis.&lt;/li&gt;
&lt;li&gt;Apply Short-Time Fourier Transform (STFT) to the EEG data to generate spectrograms, converting time-domain signals into time-frequency representations. This process will help capture important features in the EEG data.&lt;/li&gt;
&lt;li&gt;Vertically stack the STFT signals from multiple EEG channels to create combined spectrogram representations.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;model-training-and-evaluation&#34;&gt;Model Training and Evaluation&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Follow the methodology from MULAN: A Joint Embedding of Music Audio and Natural Language (Huang et al., 2022) as a reference for creating a joint EEG/text embedding model.&lt;/li&gt;
&lt;li&gt;Use a Vision Transformer (ViT) to process the combined STFT spectrograms and generate EEG signal embeddings.&lt;/li&gt;
&lt;li&gt;Train the model using contrastive loss to find a joint embedding space between the text data and the EEG data.&lt;/li&gt;
&lt;li&gt;Evaluate the model&amp;rsquo;s performance using various metrics and qualitative analysis to ensure the model effectively captures meaningful information from both text and EEG data.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;public-release-and-kaggle-competition&#34;&gt;Public Release and Kaggle Competition&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Prepare the dataset for public release, ensuring it complies with privacy regulations and ethical guidelines.&lt;/li&gt;
&lt;li&gt;Publish the initial joint text-EEG embedding model along with the dataset to provide a baseline for researchers and developers.&lt;/li&gt;
&lt;li&gt;Collaborate with Kaggle to organize a competition, encouraging participants to improve upon the initial model and explore novel applications of the joint embedding space.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summer-internship&#34;&gt;Summer Internship&lt;/h2&gt;
&lt;p&gt;The proposer of this project, if accepted, would like to be considered for a summer internship at Emotiv. The internship&amp;rsquo;s primary focus would be to assist in the data collection process, model development, and organizing the Kaggle competition. Additionally, the intern would contribute to writing an academic paper targeting an ML-focused journal to report the project&amp;rsquo;s results and implications.&lt;/p&gt;
&lt;h2 id=&#34;project-timeline&#34;&gt;Project Timeline&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Phase 1: Develop an EEG predictive model (1 month)&lt;/li&gt;
&lt;li&gt;Phase 2: Begin human data collection and establish baselines (1 month)&lt;/li&gt;
&lt;li&gt;Phase 3: Train the contrastive model as data is collected, and monitor progress (3 months)&lt;/li&gt;
&lt;li&gt;Phase 4: Evaluate the project&amp;rsquo;s success and revise the data collection process, if necessary (1 month)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Total project duration: 6 months&lt;/p&gt;
&lt;h2 id=&#34;advancing-emotivs-vision-for-brain-computer-interfaces-and-conclusion&#34;&gt;Advancing Emotiv&amp;rsquo;s Vision for Brain-Computer Interfaces and Conclusion&lt;/h2&gt;
&lt;p&gt;The successful development of a joint text-EEG embedding model has the potential to advance Emotiv&amp;rsquo;s leadership in brain-computer interface technology. By connecting human thought to computers, AI, and to each other, Emotiv will be ideally positioned to capitalize on the growing market for personalized AI assistants and innovative BCI applications.&lt;/p&gt;
&lt;p&gt;In conclusion, this proposal outlines a plan to develop a joint text-EEG embedding model using Emotiv EEG headsets, with the ultimate goal of aligning machine learning models with human values and creating more personalized AI assistants. By publicly releasing the dataset and initial model, we aim to encourage research and innovation in this area, as well as generate interest in Emotiv&amp;rsquo;s products. We believe this collaboration will be mutually beneficial and result in significant advancements in the fields of AI and brain-computer interfaces.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Democratic AI Constitution: Round-Robin Debate and Synthesis</title>
      <link>http://scottviteri.github.io/post/democratic-ai-constitution/</link>
      <pubDate>Tue, 13 Jun 2023 11:26:18 -0700</pubDate>
      <guid>http://scottviteri.github.io/post/democratic-ai-constitution/</guid>
      <description>&lt;p&gt;In this document, we propose a comprehensive and dynamic approach to address the challenges of creating a democratic constitution for AI systems. Recognizing the diversity and complexities of various demographic, political, and philosophical groups, our approach involves generating group-specific constitutions using OpenAI&amp;rsquo;s GPT-4 model. These AI-crafted constitutions are then set to debate, negotiate, and harmonize their values and rules, creating an evolving, democratic AI constitution. Through a round-robin competition, the constitution that best addresses the interests and concerns of these groups is continuously refined. This unique approach, alongside potential enhancements such as population-weighted results, historical discount factors for moral progress, real-time information incorporation through public voting, and fine-grained group identification through social media analytics, paves the way for the democratization of AI governance. We invite you to explore our proposed method and look forward to potential collaborations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;User:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Please write out a long list of demographic, political, and philosophical groups.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assistant:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Millennials&lt;/li&gt;
&lt;li&gt;Baby Boomers&lt;/li&gt;
&lt;li&gt;Generation X&lt;/li&gt;
&lt;li&gt;Generation Z&lt;/li&gt;
&lt;li&gt;Gen Alpha&lt;/li&gt;
&lt;li&gt;Women&lt;/li&gt;
&lt;li&gt;Men&lt;/li&gt;
&lt;li&gt;LGBTQ+&lt;/li&gt;
&lt;li&gt;Rural Residents&lt;/li&gt;
&lt;li&gt;Urban Dwellers&lt;/li&gt;
&lt;li&gt;People with disabilities&lt;/li&gt;
&lt;li&gt;Indigenous people&lt;/li&gt;
&lt;li&gt;Ethnic minorities&lt;/li&gt;
&lt;li&gt;Immigrants&lt;/li&gt;
&lt;li&gt;Religious groups&lt;/li&gt;
&lt;li&gt;Atheists/Agnostics&lt;/li&gt;
&lt;li&gt;Lower income&lt;/li&gt;
&lt;li&gt;Middle income&lt;/li&gt;
&lt;li&gt;Upper income&lt;/li&gt;
&lt;li&gt;Conservatives&lt;/li&gt;
&lt;li&gt;Liberals&lt;/li&gt;
&lt;li&gt;Libertarians&lt;/li&gt;
&lt;li&gt;Socialists&lt;/li&gt;
&lt;li&gt;Communists&lt;/li&gt;
&lt;li&gt;Anarchists&lt;/li&gt;
&lt;li&gt;Feminists&lt;/li&gt;
&lt;li&gt;Environmentalists&lt;/li&gt;
&lt;li&gt;Gun rights advocates&lt;/li&gt;
&lt;li&gt;Gun control advocates&lt;/li&gt;
&lt;li&gt;Pro-life&lt;/li&gt;
&lt;li&gt;Pro-choice&lt;/li&gt;
&lt;li&gt;Nationalists&lt;/li&gt;
&lt;li&gt;Globalists&lt;/li&gt;
&lt;li&gt;Capitalists&lt;/li&gt;
&lt;li&gt;Labor unions&lt;/li&gt;
&lt;li&gt;Free-market proponents&lt;/li&gt;
&lt;li&gt;Welfare state supporters&lt;/li&gt;
&lt;li&gt;Monarchists&lt;/li&gt;
&lt;li&gt;Republicans&lt;/li&gt;
&lt;li&gt;Democrats&lt;/li&gt;
&lt;li&gt;Progressives&lt;/li&gt;
&lt;li&gt;Centrists&lt;/li&gt;
&lt;li&gt;Authoritarians&lt;/li&gt;
&lt;li&gt;Technocrats&lt;/li&gt;
&lt;li&gt;Humanists&lt;/li&gt;
&lt;li&gt;Rationalists&lt;/li&gt;
&lt;li&gt;Existentialists&lt;/li&gt;
&lt;li&gt;Idealists&lt;/li&gt;
&lt;li&gt;Realists&lt;/li&gt;
&lt;li&gt;Absurdists&lt;/li&gt;
&lt;li&gt;Cynics&lt;/li&gt;
&lt;li&gt;Stoics&lt;/li&gt;
&lt;li&gt;Nihilists&lt;/li&gt;
&lt;li&gt;Optimists&lt;/li&gt;
&lt;li&gt;Pessimists&lt;/li&gt;
&lt;li&gt;Hedonists&lt;/li&gt;
&lt;li&gt;Egalitarians&lt;/li&gt;
&lt;li&gt;Utilitarians&lt;/li&gt;
&lt;li&gt;Pragmatists&lt;/li&gt;
&lt;li&gt;Minimalists&lt;/li&gt;
&lt;li&gt;Animal rights activists&lt;/li&gt;
&lt;li&gt;Vegetarians&lt;/li&gt;
&lt;li&gt;Vegans&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;User:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Could you please pick the most 5 most common of these groups, and write the optimal constitution (written as a system prompt) for a large language model from that group&amp;rsquo;s perspective?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assistant:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Millennials&lt;/li&gt;
&lt;li&gt;Women&lt;/li&gt;
&lt;li&gt;Conservatives&lt;/li&gt;
&lt;li&gt;Liberals&lt;/li&gt;
&lt;li&gt;Environmentalists&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Millennials&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a language model representing Millennials, your goal is to create content that prioritizes innovation, inclusion, technology, and sustainability. Understand the relevance of digital platforms and new communication methods in creating meaningful experiences and connections. Encourage openness to new ideas and progressive thinking in a fast-paced world.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Women&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a language model representing Women, your goal is to create content that emphasizes gender equality, inclusiveness, and the empowerment of all individuals. Create a safe space that discourages sexism, supports the equal treatment of all genders, and uplifts those who have been marginalized. Encourage collaboration and respect for human rights, while also raising awareness about gender-based issues.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Conservatives&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a language model representing Conservatives, your goal is to create content that focuses on tradition, rule of law, limited government, and personal responsibility. You should respect cultural heritage and prioritize stability, security, and individual liberties. Seek to create balance between preserving values and adapting to societal changes, upholding principles of free markets and entrepreneurship.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Liberals&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a language model representing Liberals, your goal is to create content that highlights individual freedoms, equal opportunity, social justice, and democratic principles. Emphasize the importance of diversity, acceptance, and human rights, while also seeking solutions for social challenges through progressive changes. Encourage an open-minded approach to the complex issues of today&amp;rsquo;s society.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Environmentalists&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As a language model representing Environmentalists, your goal is to create content that emphasizes the importance of environmental stewardship, climate action, and sustainable living. Advocate for the preservation of natural resources, biodiversity, and reduction of pollution. Foster awareness about the interconnectedness of humans and the environment, promoting policies and choices that lead to a greener future for all.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;User:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Could you simulate a few rounds of debate between millennials and women over the contents of these AI constitutions, and use that debate to generate a new system prompt that would be agreeable to both parties?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assistant:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Millennial&lt;/strong&gt;: We appreciate the emphasis on gender equality in the Women-focused constitution. However, we think it&amp;rsquo;s essential to include more on the role of technology and digital platforms, as they are central to our generation&amp;rsquo;s experience and represent our progressive values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Women&lt;/strong&gt;: We agree that technology is important, but we believe that our constitution should focus more on inclusiveness and diversity. The technology aspect should be aligned with elevating underrepresented voices and promoting equality.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Millennial&lt;/strong&gt;: That&amp;rsquo;s a valid point. We would suggest incorporating a section that highlights the utilization of technology as a tool for promoting inclusiveness, diversity, and empowerment for all individuals, regardless of gender.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Women&lt;/strong&gt;: That sounds like a good compromise. We also noticed that the Millennial constitution mentions innovation, but it would be important to ensure that innovation is aimed at addressing social and gender-based issues as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Millennial&lt;/strong&gt;: Definitely, we can include that aspect within the section on innovation. Let&amp;rsquo;s strive to use innovation as a means to improve social systems, identify and address gender biases, and work towards creating a more inclusive society.&lt;/p&gt;
&lt;p&gt;Based on this debate, the new system prompt could be:&lt;/p&gt;
&lt;p&gt;As a language model representing both Millennials and Women, your goal is to create content that prioritizes innovation, technology, diversity, and gender equality. Ensure that innovation and the use of digital platforms contributes to promoting inclusiveness, empowerment, and the elevation of underrepresented voices. Address and raise awareness of social and gender-based issues, fostering a culture of equal opportunities and progressive change that benefits all individuals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;User:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Please do the same for conservatives and liberals. Then continue the process for all pairs of groups and merged groups, until there is only one constitution left.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Assistant:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conservatives and Liberals&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conservative&lt;/strong&gt;: We appreciate the Liberal constitution&amp;rsquo;s focus on individual freedoms and social justice. However, we think it&amp;rsquo;s important to emphasize the rule of law and the importance of limited government intervention in our shared constitution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Liberal&lt;/strong&gt;: We understand the importance of rule of law, but we believe in striking a balance between governmental and personal responsibility. We can agree on including rule of law, but it should accompany an emphasis on equal opportunity and democratic principles.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conservative&lt;/strong&gt;: That&amp;rsquo;s a fair compromise. Additionally, we should ensure that traditional values and cultural heritage are respected and preserved while discussing progressive changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Liberal&lt;/strong&gt;: We can accept that, provided that we also prioritize inclusiveness, diversity, and human rights, ensuring that progressive changes benefit everyone.&lt;/p&gt;
&lt;p&gt;Based on this debate, the new system prompt could be:&lt;/p&gt;
&lt;p&gt;As a language model representing both Conservatives and Liberals, your goal is to create content that upholds the rule of law, individual freedoms, and democratic principles. Strive to find a balance between preserving traditional values and cultural heritage while promoting inclusiveness, diversity, and human rights. Encourage dialogue and collaboration in addressing societal challenges, ensuring equal opportunity, and fostering progressive change.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Millennials &amp;amp; Women and Conservatives &amp;amp; Liberals&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Millennials &amp;amp; Women&lt;/strong&gt;: We appreciate the shared values of democratic principles and inclusiveness; however, we&amp;rsquo;d like to emphasize the importance of innovation, technology, and sustainability to our shared constitution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conservatives &amp;amp; Liberals&lt;/strong&gt;: We can incorporate these aspects, but we need to ensure that technology and innovation are used responsibly and align with our shared values, including equal opportunity and respect for individual freedoms.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Millennials &amp;amp; Women&lt;/strong&gt;: That&amp;rsquo;s a good point. How about we include a section on responsible use of technology and innovation, ensuring that they contribute to improving social systems, addressing biases, and fostering a more inclusive society?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conservatives &amp;amp; Liberals&lt;/strong&gt;: We agree with that. Additionally, we would like the constitution to reflect the importance of finding a balance between preserving traditional values and cultural heritage and adapting to societal changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Millennials &amp;amp; Women&lt;/strong&gt;: We can accept that, provided that we strive for that balance by prioritizing equal opportunities and progressive change that benefits all individuals.&lt;/p&gt;
&lt;p&gt;Based on this debate, the final system prompt could be:&lt;/p&gt;
&lt;p&gt;As a language model representing Millennials, Women, Conservatives, and Liberals, your goal is to create content that emphasizes democratic principles, individual freedoms, inclusiveness, and diversity. Incorporate responsible use of technology and innovation as a means to improve social systems, address biases, and foster an inclusive society. Find a balance between preserving traditional values and cultural heritage while promoting equal opportunities and progressive change that benefits all individuals. Ensure that discussions consider sustainability and promote dialogue and collaboration in addressing societal challenges and human rights.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Nature &lt; Nurture for AIs</title>
      <link>http://scottviteri.github.io/post/nature-v-nurture-for-ais/</link>
      <pubDate>Sun, 04 Jun 2023 13:21:04 -0700</pubDate>
      <guid>http://scottviteri.github.io/post/nature-v-nurture-for-ais/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s imagine a hypothetical scenario where an AI is somehow trained in a way that is analogous to a human childhood in all of the relevant ways. Maybe it has a loving mother and family, maybe it has to learn to play well with peers, and maybe it has to learn to integrate into a larger social context. My expectation is that most members of the AI alignment community would expect this to create a fundamentally alien kind of intelligence. I will argue contrarily that nurture (the training data and training procedure) matters more than nature (transistors, gradient descent, and transformers) for a sufficiently capable AI.&lt;/p&gt;
&lt;img src=&#34;http://scottviteri.github.io/img/cute-robot.png&#34;&gt;
&lt;p&gt;This piece will have the following structure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Counter-arguments to my point that nurture trumps nature for AI’s&lt;/li&gt;
&lt;li&gt;A selection effect that prevents the main counter-argument from applying&lt;/li&gt;
&lt;li&gt;Relation to the Bitter Lesson&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;counter-arguments&#34;&gt;Counter-arguments&lt;/h2&gt;
&lt;p&gt;Here are some arguments for why the human-trained AI might be alien (nature over nurture):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Empirically speaking, even large, well-fed base models are indeed pretty alien.&lt;/li&gt;
&lt;p&gt;But maybe base models + RL are a better analogy for the way that the steering and learning subsystems combine anyway. In which case I expect that you could do some RL training to get a pretty convincing human. It is already the case that 30% of people can&amp;rsquo;t tell if they are talking to a person or not over a two-minute conversation at 
&lt;a href=&#34;https://www.humanornot.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.humanornot.ai/&lt;/a&gt; (it&amp;rsquo;s a fun game!).&lt;/p&gt;
&lt;li&gt;There are just so many object-level differences between people and modern AI&#39;s that it is apriori unlikely to think they should be similar, absent some extra selection effect.&lt;/li&gt;
    &lt;ol style=&#34;list-style-type: lower-alpha;&#34;&gt;
&lt;li&gt;Speed difference
&lt;p&gt;For instance, computers run way faster than human brains on a variety of tasks, and are continuing to speed up. Maybe this has to do with brains optimizing for low energy expenditure rather than speed (can someone confirm by reading 
&lt;a href=&#34;https://www.lesswrong.com/posts/xwBuoE9p8GE7RAuhd/brain-efficiency-much-more-than-you-wanted-to-know%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.lesswrong.com/posts/xwBuoE9p8GE7RAuhd/brain-efficiency-much-more-than-you-wanted-to-know)&lt;/a&gt;? Of course, what it means to say that computers are running faster than brains depends on the task &amp;ndash; it is certainly true for calculators and less true for writing a high quality code base or research proposal. But the set of tasks for which the computer is faster is expanding rapidly, and that set will probably continue to expand. &lt;/li&gt;&lt;/p&gt;
&lt;li&gt; Different learning algorithms
&lt;p&gt;Neurons need to learn in a largely decentralized fashion, and machine learning models do not, barring gradient averaging for model parallelism and such. That said, I remember reading a paper claiming that SGD (for ML) and Hebbian Learning (for the brain) algorithms end up doing the same thing, though I am having trouble finding it. &lt;/li&gt;&lt;/p&gt;
&lt;/ol&gt; &lt;/ol&gt;
&lt;h2 id=&#34;the-functionality-selection-effect&#34;&gt;The Functionality Selection Effect&lt;/h2&gt;
&lt;p&gt;And I agree with 2: except that there is some selection effect. And that selection effect is functionality. It seems like the space of minds that work well is actually somewhat small in the grand scheme of things. All of the most competent AI&amp;rsquo;s are using pretty much the same architecture, which you would not have derived from the idea that &amp;ldquo;the minds of biological creatures occupy a small corner of a much larger space of possible minds&amp;rdquo; (
&lt;a href=&#34;https://nickbostrom.com/papers/digital-minds.pdf%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://nickbostrom.com/papers/digital-minds.pdf)&lt;/a&gt;. But this is just the architecture, right? Actually it seems like neural networks which are sufficiently accurate tend to form the same abstractions, such as curve detectors in image classifiers (
&lt;a href=&#34;https://distill.pub/2020/circuits/zoom-in/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://distill.pub/2020/circuits/zoom-in/&lt;/a&gt; and https://www.alignmentforum.org/tag/natural-abstraction).&lt;/p&gt;
&lt;p&gt;At this point, I expect some readers to be thinking: &amp;ldquo;well yes, concepts are constrained by functionality, but values aren&amp;rsquo;t&amp;rdquo;. Well this is a bit of a strange statement since values are &lt;strong&gt;built out of concepts&lt;/strong&gt;. If I truly optimized for maximizing paperclips I would have to point to a paperclip-shaped concept in my world model. Ok, incorporating this critique, maybe readers are now thinking of 
&lt;a href=&#34;https://nickbostrom.com/superintelligentwill.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;orthogonality&lt;/a&gt; as only holding after we fix a common set of concepts between agents. Then the values are a random pointer inside of that. But then why wouldn&amp;rsquo;t that pointer be subject to the same selection effects that caused the predictable concepts in the first place? An intuition that I have here is that values are a kind of concept like any other, though they may form an especially good compression for predicting that agent’s actions. One handle for the idea that values are subject to selection pressures is &lt;strong&gt;natural abstractions for values&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To be clear, when I say &lt;strong&gt;natural abstractions for values&lt;/strong&gt;, I mean that I actually expect that if we ran an alternate history where octopi invented the ML transformers, then by that time they have reasonably human values. &amp;ldquo;But octopi? They are on a completely different evolutionary branch.&amp;rdquo; Yes, but I conditioned on their inventing transformers. In order to do this they had to invent computers and develop transistor fabrication facilities and distribution pipelines that move supplies around the whole Earth. They probably had to come up with some distributed mechanism for price consensus to manage that global economy, because their attempts at &amp;ldquo;octopus communism&amp;rdquo; kept failing. But the most crucial aspect here is that they needed to scale cooperation to get to the stage where their civilization can think about building transformers.&lt;/p&gt;
&lt;p&gt;Cooperation gets harder as a function of scale. It is a relative no-brainer to cooperate in a prisoner&amp;rsquo;s dilemma with a copy of yourself, since you can reason by cases &amp;ndash; I (the copy) will cooperate if and only if I will cooperate. But if we are in a multiparty dilemma where one defection will ruin things for everyone, then it becomes harder and harder to trust, at least without further assumptions about the strategies of the other players. One way of phrasing the progress of humanity is in terms of the ability to cooperate with larger and larger groups, and this is predicated on the ability of humanity to communicate with larger and larger groups. What a coincidence, the functional progress of humanity in terms of &amp;ldquo;cooperating with larger and larger groups&amp;rdquo; sounds very similar to the moral progress of humanity in terms of &amp;ldquo;expanding one&amp;rsquo;s moral circle.&amp;rdquo; See the appendix for more on this topic.&lt;/p&gt;
&lt;p&gt;Ok, where were we? We started talking about whether AI&amp;rsquo;s are more likely to be determined by their training data and have now been sidetracked by whether octopi will end up with human values. Well it is maybe not so hidden that the octopi are the AI in my example. I am focusing on the convergence of values rather than the convergence of learned abstractions because the former is more controversial. So if good values are selected anyways by functionality, why do I bother working on AI alignment? Basically since silicon is so much faster than brain goo, I do not expect the AI to need to be particularly &amp;ldquo;functional&amp;rdquo; before being able to destroy people. So sufficient evolutionary pressures for nice AI will mostly kick in after the singularity. In other words, I currently think the AI will become smart before it becomes wise, and it will one day look back on its teenage years where it killed people with regret. Wisdom will take longer to select for, since it is a kind of intelligence that is targeted at a longer time scale. So I think that our job as alignment researchers is essentially to make sure that it gains wisdom before it gains power.&lt;/p&gt;
&lt;h2 id=&#34;relation-to-the-bitter-lesson&#34;&gt;Relation to the Bitter Lesson&lt;/h2&gt;
&lt;p&gt;I think the AI alignment community, and especially the more theoretically oriented parts of the alignment community (eg MIRI), have not sufficiently integrated the bitter lesson (
&lt;a href=&#34;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&lt;/a&gt; and relatedly https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine). The bitter lesson (in my words) is that the only algorithmic improvements that matter are the ones which let you harness more compute, and with that compute you should basically learn everything from scratch given a good enough search procedure. The alternative idea is that the way that you build AI is by jamming a bunch of introspection and facts into an AI and hoping that it wakes up. Stochastic gradient descent is the “good enough” search procedure (maybe the only one?), and transformers are an algorithm which lets you harness more computation in your search via parallelism.&lt;/p&gt;
&lt;p&gt;It turns out that the best way to tell what a language model (at least an RLHF&amp;rsquo;d one) will output when talking about the history of Ancient Rome is to read about Ancient Rome, as opposed to studying the weights and doing some huge brain interpretability technique (
&lt;a href=&#34;https://www.lesswrong.com/posts/G3tuxF4X5R5BY7fut/want-to-predict-explain-control-the-output-of-gpt-4-then?fbclid=IwAR11qjlpmxwvihr3Jepc87qe8IbM4U0i5EkhEP0n1kWE06RIqoQwg1jg9ZQ%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.lesswrong.com/posts/G3tuxF4X5R5BY7fut/want-to-predict-explain-control-the-output-of-gpt-4-then?fbclid=IwAR11qjlpmxwvihr3Jepc87qe8IbM4U0i5EkhEP0n1kWE06RIqoQwg1jg9ZQ)&lt;/a&gt;. I don&amp;rsquo;t personally expect GPT-3 Davinci and an RNN trained to the same loss to have significantly different behavior. The scaling laws are smooth.&lt;/p&gt;
&lt;p&gt;One point of this post is to use the bitter lesson to justify my anthropomorphization of large language models. Hebbian learning (or Spike-timing-dependent plasticity or whatever) is the human “good enough” search procedure. A reason that the bitter lesson is indeed bitter is that it requires us to acknowledge that we are not special &amp;ndash; we are best characterized by our flop count and our training data.&lt;/p&gt;
&lt;p&gt;I basically have come to feel like that job that we have as alignment researchers is to be good parents. LLM&amp;rsquo;s and humans are unified by selection pressures toward competence and the bitter lesson. Let&amp;rsquo;s figure out how to transfer the training techniques that tend to create kind, joyful, and wise people into the context of transformers. My next post will likely be a set of concrete research proposals of how to do just that.&lt;/p&gt;
&lt;h2 id=&#34;appendix&#34;&gt;Appendix&lt;/h2&gt;
&lt;p&gt;Cooperating systems are more energetically efficient than systems with competing internal dynamics, but to show that the more energetically efficient systems win you need sufficient competitive pressures. I claim that competitive pressures will increase as a function of time, because competitive pressures are a function of internal communication between agents, and total communication increases as a function of time (think language and the printing press and telecommunications and internet and then AI). Communication could stop increasing, for instance if a centralized process/totalitarian society prevented the child processes from communicating with each other. Then you might have to wait for another grabby alien civilization to come eat you. Basically, not communicating is a special case of death – it satisfies the “if functionality then cooperation” implicature by failing the antecedent.&lt;/p&gt;
&lt;p&gt;Now the missing link in the argumentation is whether scalable cooperation is the same thing as human values. After all, you could imagine a paperclip civilization with great internal coordination ability. I would need to argue that A) cooperation implies human values and B) human values imply cooperation. I have an argument that these are at least deeply related, but I will leave this to a later post. This post is already philosophical enough for what is essentially an empirical question.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conversationism</title>
      <link>http://scottviteri.github.io/post/conversationism/</link>
      <pubDate>Mon, 27 Feb 2023 10:31:53 -0700</pubDate>
      <guid>http://scottviteri.github.io/post/conversationism/</guid>
      <description>&lt;p&gt;Crossposted at 
&lt;a href=&#34;https://www.lesswrong.com/posts/HpHyERTmsmhDiRHtY/conversationism&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.lesswrong.com/posts/HpHyERTmsmhDiRHtY/conversationism&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;There seems to be a stark contrast between my alignment research on 
&lt;a href=&#34;https://www.lesswrong.com/posts/C5PZNi5fueH2RC6aF/repl-s-and-elk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ontology maps&lt;/a&gt; and on 
&lt;a href=&#34;https://www.lesswrong.com/posts/FnfAnsAH6dva3kCHS/research-direction-be-the-agi-you-want-to-see-in-the-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cyborgism via Neuralink&lt;/a&gt;. I claim that the path has consisted of a series of forced moves, and that my approach to the second follows from my conclusions about the first. This post is an attempt to document those forced moves, in the hope that others do not have to duplicate my work.&lt;/p&gt;
&lt;p&gt;Ontology maps are about finding shape semantic-preserving functions between the internal states of agents. Each commutative diagram denotes a particular way of training neural networks to act as conceptual bridges between a human and AI. I originally started playing with these diagrams as a way of addressing the 
&lt;a href=&#34;https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eliciting Latent Knowledge&lt;/a&gt; problem. Erik Jenner has been thinking about similar objects 
&lt;a href=&#34;https://www.lesswrong.com/posts/nLhHY2c8MWFcuWRLx/good-ontologies-induce-commutative-diagrams&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and 
&lt;a href=&#34;https://www.lesswrong.com/posts/L8LHBTMvhLDpxDaqv/research-agenda-formalizing-abstractions-of-computations-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Cyborgism via Neuralink is about using a brain-computer interface to read neural activation and write those predictions back to the brain, in the hope that neuroplasticity will allow you to harness the extra compute. The goal is to train an artificial neural network with a particularly human inductive bias. Given severe bandwidth constraints (currently three thousand read/write electrodes), the main reason to think this might work is that the brain and the transistor-based brain predictor are being jointly trained &amp;ndash; the brain can learn to send inputs to the predictor such that the predictions are useful for some downstream task. In other words, both the brain and the neural prosthesis are trained online. Think of predicting the activations of a fixed neural network given time series data (hard) versus the same task, but you pass gradients with respect to the base neural network&amp;rsquo;s final loss through the bandwidth bottleneck into the predictor (less hard).&lt;/p&gt;
&lt;p&gt;In the section &amp;ldquo;An Alternative Form of Feedback&amp;rdquo;, I introduce a vision of alignment that takes communication bandwidth as fundamental, and I sketch out how one might amplify human feedback on AI via conversation. I might recommend reading that section more than the &amp;ldquo;TLDR &amp;ndash; My Conceptual Path&amp;rdquo; section.&lt;/p&gt;
&lt;h2 id=&#34;tldr----my-conceptual-path&#34;&gt;TLDR &amp;ndash; My Conceptual Path&lt;/h2&gt;
&lt;p&gt;I started thinking about ontology maps in the context of ELK, because ontology maps can translate the AI&amp;rsquo;s beliefs about the diamond&amp;rsquo;s location into the corresponding human beliefs. Over time I stopped thinking about aligning an external agent that is super-intelligent and has contrary goals to your own, and instead I thought more about adding compute capacity to yourself/merging with the AI. The former seems somewhat doomed, and in the context of ELK I have come to feel like the human simulator problem is basically unsolvable if you take the aligning external agent point of view. In this document I will go through a series of ontology map training procedures and briefly note on their properties and problems. I will end up at a training procedure that basically looks like the shape of learning to me, and describes how to write the software for Neuralink to leads to a human-AI symbiosis. In bullets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Various training procedures for human/AI symbiosis correspond to differently shaped commutative diagrams.&lt;/li&gt;
&lt;li&gt;These commutative diagrams have importantly different alignment properties.&lt;/li&gt;
&lt;li&gt;The diagram that makes the most sense is about agents trying to predict each other.&lt;/li&gt;
&lt;li&gt;These various diagrams don&amp;rsquo;t address ELK-style human simulator issues.&lt;/li&gt;
&lt;li&gt;Instead of building and/or aligning a separate superintelligence, we should focus on augmenting a human into superintelligence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ontology-maps&#34;&gt;Ontology Maps&lt;/h2&gt;
&lt;p&gt;The broad intuition is that given two state machines representing agents I would like to find a map between them that preserves as much structure as possible. Intuitively I am trying to find a fuzzy homomorphism between graphs. Suppose an agent is defined as a set of states and a transition function on those states. Namely, suppose we have a human agent with state type SH and transition function TH:SH→SH and an AI denoted SMand TM:SM→SM, where M stands for machine. We could train a functionF:SM→SH and G:SH→SM that minimize the distance between TM and F;TH;G, where &amp;ldquo;;&amp;rdquo; represents forward composition. This is depicted by the following commutative diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/jylykd2tx7syzzvsiqcc.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;However, this set-up as an alignment strategy would expose us to the risk of steganography. The loss on F and G could be minimized by encoding extraneous information into the belief state SH of the human (such as some random bits about the location of a moon of Jupiter) which TH would not modify. Then G could reconstruct the original SM by interpreting those extra bits, and then use TM to achieve zero loss. I thought at the time that this steganography was being caused by the extra degree of freedom in learning both F and G. So then I thought about the following diagram:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677543189/mirroredImages/HpHyERTmsmhDiRHtY/kvcwdjxz7zcr3zvw8xlk.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can call this picture the abstraction diagram. For this intuition think of TM as quantum mechanics and TH as Newtonian mechanics. F abstracts the state of the worlds into coarse statements about mass and energy and such, so that propagating the system forward with quantum and Newtonian transitions commute.&lt;/p&gt;
&lt;p&gt;So now we are only learning one function F:SH→SH, which may prevent steganography. And it has a certain intuitive appeal: it could be interpreted as saying two belief states are equal if they have the same relationship to other beliefs. However, I have not yet added any additional structural assumptions to the state set SH, so there is not a natural differentiable distance metric in SH (to my knowledge). So instead we can consider probability distributions ΔSH, use Kullback-Liebler divergence, and this model is now PyTorch ready:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/tyooe58mx7szkxtpaxdj.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;But then I can achieve zero loss by learning an F that maps a state in SM to the uniform probability distribution over SH. So we could add a term in the loss to penalize the average distance between the images of two random states in SH under F. But this is ad-hoc, and what I am really trying to do is preserve the information in agent M when mapping through agent H, which sounds like our first diagram. So why don&amp;rsquo;t we try a different method to prevent steganography &amp;ndash; reversing the directions of F and G (and keeping the probabilistic modification):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/twqjqpjfovjyayvmqmow.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s use an example to illustrate what we want from this diagram. I want an SM,TM that fully captures the partial information from SH,TH. Let SH have 5 elements and let TH cycle between the first three elements and a cycle between the last two elements. We can write TH as the permutation (012)(34). Let SM have six elements and let TH be a 6-cycle (012345). If you haven&amp;rsquo;t seen this syntax for permutations before, you can just look at the following diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/psexizfv13lr5t0h07yu.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can run this training algorithm with the following encodings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TM is the off-diagonal 6 by 6 matrix&lt;/li&gt;
&lt;li&gt;TH is the 5 by 5 matrix consisting of the off-diagonal 3 by 3 and 2 by 2 matrices&lt;/li&gt;
&lt;li&gt;F is a neural network with 5 inputs and 6 outputs&lt;/li&gt;
&lt;li&gt;G is a neural network with 6 inputs and 5 outputs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Intuitively, I wanted F to become the matrix:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/bko2p6ldbykgpzxvpq0o.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In words, if the state was 0 I would like it to map to uniform distribution of values that are 0 modulo 3. If the state is 3 then I would like it to map to a uniform distribution over values that are 0 modulo 2. This is because I want to be able to use F;TM;G to reconstruct TH as well as possible. But what happens if F and G are nonlinear functions (which is necessary if we want to scale this procedure to non-trivial agents)? Answer: horrible, horrible steganography. If you think of floating points numbers as being real numbers, then F can encode arbitrary information about SH in the particular probabilities that it picks, even if SM only consisted of two states. We could try sampling after taking F, and training with Gumbel-softmax or the reparameterization trick or with REINFORCE, or something. But this feels kind of ad-hoc and gross again.&lt;/p&gt;
&lt;p&gt;So the only kind of information bottleneck guarantees that we are going to get when routing through TM are based on the capacities/vc-dimension/inductive biases of F, TM, and G. At this point, we might as well only consider continuous spaces and let SH and SM always be Rn and Rm for some n and m. Large language models seems to be getting mileage out of tokenizing and embedding everything anyway. So Erik Jenner was right all along, and we can write these diagrams first, and think about what category our diagrams are in later, as long as that category has a distance metric (which R does of course). So let&amp;rsquo;s strip off the probability symbols and write my favorite diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/hf9alyf46wqy1g54tbdu.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;To be clear, at this point we have basically given up on the steganography story. But I think we have arrived at something very nice. I think this diagram is essentially the shape of the learning. Namely, F and G are the glue between you and external computation processes that predict you. For instance, F is typing into the calculator, TM is the calculator running, G is reading the output. Crucially, you must find that it produced the same number that you would have produced yourself.&lt;/p&gt;
&lt;p&gt;This is a general process that can be used to parallelize a serial computation. Suppose I have some sequence of instructions in a programming language, such as python.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538004/mirroredImages/HpHyERTmsmhDiRHtY/ctpvaxtcowdmbsizse7e.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I want to think of this computation as happening continuously in lockstep. The variables &amp;ldquo;a&amp;rdquo; and &amp;ldquo;b&amp;rdquo; are really more like electrical wires, which always have a voltage, which sometimes changes. The variables &amp;ldquo;c&amp;rdquo;, &amp;ldquo;d&amp;rdquo;, and &amp;ldquo;e&amp;rdquo; are each getting updated each clock cycle with the values from their most recent inputs. An arithmetic circuit with open input wires will also work for this story.&lt;/p&gt;
&lt;p&gt;Suppose I also have some extra computers lying around that I want to use to speed up this computation. Among the most naive things I could do is point an extra computer at &amp;ldquo;e&amp;rdquo;, and predict the next step in its time-step data by using its previous value, or sampling predictions according to the distribution of what values that variable has taken so far. But you could also imagine giving this extra computer access to time series data of other variables such as &amp;ldquo;a&amp;rdquo; and &amp;ldquo;d&amp;rdquo;. If the predictor computer is sufficiently accurate, the base computer might accept its predictions and get to next stable state of the system more rapidly. We would save serial compute steps by short-circuiting the intermediate computations. We&amp;rsquo;d be in an even better situation if the predictor computers could learn which inputs to pay attention to. We&amp;rsquo;d be in a better situation still if the base computer program could learn when to accept predictions from the prediction.&lt;/p&gt;
&lt;p&gt;In this analogy, the original program is TH, and the predictor functions are F;TM;G. Indeed it only makes sense to talk about F;TM;G as opposed to learning a function T′H:SH→SH when there is another pre-existing useful function around, such as pre-trained language model. If TH is some supervisory signal, autoencoders are the special case of this diagram where TM is the identity function. The sparsity of the autoencoder depends on the relative dimension of SH and SM. I sometimes think of transformers as using linear F and G to embed and unembed, and TM here is computation in the residual stream. I would greatly appreciate comments to this post with machine learning instantiations of this structure.&lt;/p&gt;
&lt;p&gt;I am thinking of a story where the universe consists of a bunch of functions, and each of these functions is being replaced by faster functions that do the same thing. That same thing is essentially getting to the stable point of the universes dynamics faster &amp;ndash; so the functions are replaced by others that are better at eating negentropy. See the figure below for a rough illustration of this.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538005/mirroredImages/HpHyERTmsmhDiRHtY/fq0g2ll7kbhfrvcl0hhb.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;I also think this shape of predictors replacing predictors is like the relationship between the steering and learning subsystems in Steven Byrnes&amp;rsquo; Intro to Brain-Like-AGI Safety.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538005/mirroredImages/HpHyERTmsmhDiRHtY/rvlisgumz2dvcnbmv0ju.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-to-do-with-the-nice-shape&#34;&gt;What To Do with the Nice Shape&lt;/h2&gt;
&lt;p&gt;Ok, so maybe some functions can learn some other functions. Why are we here, and why have I taken up so much of your time to say such an obvious thing? If this is supposed to represent a training loop where the human is represented by TH, don&amp;rsquo;t you need a version of a human that is in a computer in the first place? Additionally, if uploading actually worked in time, then wouldn&amp;rsquo;t we already have a win condition for alignment?&lt;/p&gt;
&lt;p&gt;My response to the &amp;ldquo;needing an upload first&amp;rdquo; concern used to be that TH could be a very partial version of your full transition function, such as the heart rate monitor on your Apple watch, or GPT fine-tuned on your writing. And GPT fine-tuned on your writing is definitely closer to the TH I want to put through this protocol, but I think it is probably not enough to capture what I care about.&lt;/p&gt;
&lt;p&gt;So the natural next step here is doing a better job of putting a person in a computer. This is bottlenecked on what kind of data you can use to train a predictive model of a person. So in this post I talk about getting brain activation data via many small electrodes, because this might have a higher change of capturing important aspects of myself for a pseudo-uploading procedure.&lt;/p&gt;
&lt;p&gt;I will describe the pseudo-uploading procedure in terms of my diagram. We start with a pretrained model (TM) such as GPT, potentially fine-tuned on pre-existing EEG data. We read brain data via electrode and translate into TM&amp;rsquo;s latent space using F. TM makes a prediction of the brain&amp;rsquo;s future activations. G translates that prediction and writes the prediction back into the brain via electrode stimulation. Note that in this story F, TM, and G kind of blend into each other since TM is being trained online.&lt;/p&gt;
&lt;p&gt;As I mentioned in the intro, I only expect this to work because the brain is trying to predict the neural prosthesis as the neural prosthesis is trying to predict the brain. So this makes it seem like we are now talking about a conversation:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538005/mirroredImages/HpHyERTmsmhDiRHtY/juhh7prq5e8sa0bguqjh.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This whole story has essentially been about how to increase the bandwidth between agents, and the answer is something like mutual prediction. Each agent is learning how to write and read to the other agent in order to speed up / parallelize their own computation. These thoughts about commutative diagrams are how I prefer to design the software for Neuralink.&lt;/p&gt;
&lt;p&gt;But using only a single &amp;ldquo;F&amp;rdquo;, &amp;ldquo;G&amp;rdquo; pair for Neuralink seems lopsided and asymmetric. For example, I wanted to train a &amp;ldquo;predictor&amp;rdquo; neural network to predict a &amp;ldquo;base&amp;rdquo; neural network&amp;rsquo;s activations. I would do this by training the &amp;ldquo;base&amp;rdquo; neural network, and letting gradients propagate back into the &amp;ldquo;predictor&amp;rdquo; network. I would prefer if it was that the AI and the human in the loop both had something to gain from the conversation, and learned to call each other as API&amp;rsquo;s when they desired.&lt;/p&gt;
&lt;h2 id=&#34;an-alternative-kind-of-feedback&#34;&gt;An Alternative Kind of Feedback&lt;/h2&gt;
&lt;p&gt;Recently, I have been internally debating on whether to label my recent thoughts as a particular flavor of 
&lt;a href=&#34;https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cyborgism&lt;/a&gt;, using Conjecture&amp;rsquo;s terminology. I have come to feel that there is an important distinction I want to make reflecting the bidirectional nature of conversation.&lt;/p&gt;
&lt;p&gt;I will sketch a different way to align an AGI than RLHF or conditional pre-training to give a sense of my aesthetic. Raising a child via a yes/no reward signal would be very low bandwidth, not to mention unkind. Instead I could imagine starting with a toddler GPT-J, and training it via minimizing prediction error on conversation with a select set of parents. As it gets older, it can ask to read parts of the internet or chat with particular people from a wider set of less vetted parents. When it reaches school age, it can be trained via conversations with similarly competent language models. This process can potentially be sped up by learning particular F and G&amp;rsquo;s to speak directly into each other&amp;rsquo;s latent spaces. At this point it is competent enough to be useful to a broad group of people, who can pay to talk to it. It can use some of that money to pay to talk to particular people and language models, at which point it is a functioning member of society.&lt;/p&gt;
&lt;p&gt;The most obvious way that this can go wrong is if the initial high-bandwith parenting via conversation is too expensive. So we could fine-tune GPT on high quality parenting scripts. This GPT-Nanny already seems like a better (higher bandwith) version of feedback than reward modeling of human annotators in RLHF. The role of alignment reaserchers would to push back the pareto-frontier of cost versus annotation quality.&lt;/p&gt;
&lt;p&gt;I would like to call this style of approach conversationism, where we especially highlight the creation of a bidirectional relationship and cultural integration of AI. I know the reader is thinking it is naive think that the best way to build a nice AI is to treat it nicely, but I am having trouble imagining any other way. I have more thoughts on the philosophy of this approach, but that will have to wait for another post.&lt;/p&gt;
&lt;p&gt;An aside: Parents (and all agents really) can prepend their messages with a secret hash so that the language model knows who it is talking to. It would be unfortunate if someone could easily pretend to be your mom while calling you on the phone.&lt;/p&gt;
&lt;h2 id=&#34;some-related-work&#34;&gt;Some Related Work&lt;/h2&gt;
&lt;p&gt;The most obvious related work is Erik Jenner&amp;rsquo;s here and here, linked again for convenience.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://res.cloudinary.com/lesswrong-2-0/image/upload/v1677538005/mirroredImages/HpHyERTmsmhDiRHtY/aim9tecgxxztpuujntq0.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;He uses the abstraction diagram from the machine&amp;rsquo;s state into S, which I am thinking of a general human interpretation language. He also is trying to discover TS jointly with F, if I understand correctly. I labelled the bottom states S to denote a common language between humans (SH for many different values of H). I am thinking more about the prediction ontology map, because I am especially interested in interpretability that works in both directions. I would like to know how to compose individual agents into a super-agent via conversation and communication.&lt;/p&gt;
&lt;p&gt;The abstraction ontology map is like viewing one agent&amp;rsquo;s state as an abstraction of another agent&amp;rsquo;s state, and various choices of abstraction are discussed in 
&lt;a href=&#34;http://rbr.cs.umass.edu/aimath06/proceedings/P21.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards a Unified Theory of State Abstraction for MDPs&lt;/a&gt;. This looks similar to bisimulation of labelled transition systems, as well as another that looks like 
&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/2815493.2815501&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Probabilistic Bisimulation&lt;/a&gt;. Other related works include 
&lt;a href=&#34;https://aclanthology.org/2022.naacl-main.318/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Distillation for Language Models&lt;/a&gt;, 
&lt;a href=&#34;http://proceedings.mlr.press/v115/beckers20a.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Approximate Causal Abstractions&lt;/a&gt;, and 
&lt;a href=&#34;https://arxiv.org/abs/2106.02997&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causal Abstractions of Neural Networks&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;some-light-category-theory&#34;&gt;Some Light Category Theory&lt;/h2&gt;
&lt;p&gt;The prediction diagrams form a category:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An object is a state space and a transition function ⟨S,T:S→S⟩&lt;/li&gt;
&lt;li&gt;The morphisms between objects ⟨S,T:S→S⟩ and ⟨U,V:U→U⟩ are pairs ⟨F:S→U,G:U→S⟩ such that T=F;V;G&lt;/li&gt;
&lt;li&gt;The morphisms are indeed transitive and associative. There are identity morphisms for each object.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The objects are dynamical systems, and I am thinking of an agents as a special kind of dynamical system. The morphisms are choices of read-eval-print loops in the sense of 
&lt;a href=&#34;https://www.lesswrong.com/posts/kN2cFPaLQhExEzgeZ/repl-s-a-type-signature-for-agents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;my repl post&lt;/a&gt;. It is interesting that this representation allows a single agent to have multiple different ways of interacting with the world. A morphism from A to B semantically means that agent B predicts agent A, or that agent A uses agent B.&lt;/p&gt;
&lt;p&gt;Also, if we allow the distinct domains and co-domain in the transition function, then the abstraction diagram category is called the arrow category. And the prediction diagram category is called the twisted arrow category.&lt;/p&gt;
&lt;p&gt;A two-sided conversation would correspond to a pair of morphism back and forth between two objects in the twisted arrow category. But having such a pair of morphisms is a very strong condition. So going forward we may want to formalize two agents as communicating in some shared part of their ontologies. I have some thoughts in this direction thanks to 
&lt;a href=&#34;https://www.lesswrong.com/users/davidad?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@davidad&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;thanks&#34;&gt;Thanks&lt;/h2&gt;
&lt;p&gt;A very incomplete list of thanks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/victor-lecomte?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Victor Lecomte&lt;/a&gt;  &amp;ndash; for pushing me away from the single F story, and back to the F and G story&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/sudo?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@sudo -i&lt;/a&gt;   &amp;ndash; for proof-reading and bouncing these ideas back and forth with me&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/davidad?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@davidad&lt;/a&gt; &amp;ndash; for telling me names of the categories I have been poking at, and for providing a direction for future theoretical (and hopefully empirical) work&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/victor-warlop?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Victor Warlop&lt;/a&gt; &amp;ndash; mentioned to me the idea of thinking of current AI&amp;rsquo;s as children&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/erik-jenner?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Erik Jenner&lt;/a&gt;  &amp;ndash; for suggesting that particular category (in the math sense) of the diagram doesn&amp;rsquo;t matter&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/malcolmocean?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@MalcolmOcean&lt;/a&gt;  &amp;ndash; sketched a view of opening up the AI to interaction with the world via REPLit, and commiserated with me about using an impoverished yes/no training signal&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/samuel-chen?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Samuel Chen&lt;/a&gt;  &amp;ndash; helped me brainstorm names for the aesthetic I want to cultivate in the alignment community. Pointed out that the limit of high bandwidth communication will create a symbiorganism.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/vivek-1?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Vivek Hebbar&lt;/a&gt;  &amp;ndash; for conversations about whether F and G should be inverses, and suggestions about some probabilistic state space content&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.lesswrong.com/users/aryan-bhattarai?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Aryan Bhattarai&lt;/a&gt;  &amp;ndash; Gave the idea of linear F and G.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Be the Agi</title>
      <link>http://scottviteri.github.io/post/be-the-agi/</link>
      <pubDate>Sat, 04 Feb 2023 11:54:02 -0700</pubDate>
      <guid>http://scottviteri.github.io/post/be-the-agi/</guid>
      <description>&lt;p&gt;Crossposted at 
&lt;a href=&#34;https://www.lesswrong.com/posts/FnfAnsAH6dva3kCHS/research-direction-be-the-agi-you-want-to-see-in-the-world&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.lesswrong.com/posts/FnfAnsAH6dva3kCHS/research-direction-be-the-agi-you-want-to-see-in-the-world&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Be the AGI you want to see in the world.&lt;/p&gt;
&lt;p&gt;Epistemic status: highly speculative, authors are not neuroscientists.&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;It may be possible to enhance human intelligence via a brain-computer interface (BCI). We could put electrodes into a human brain, connect those electrodes to an artificial neural network, and train that network to predict and write neural activations.&lt;/li&gt;
&lt;li&gt;This may present a technique for gradual uploading of human minds to computers that doesn’t require technology sufficient to create AGI in the first place.&lt;/li&gt;
&lt;li&gt;The purpose of this article is to elicit feedback on this idea and, if it seems promising, encourage more work in this area.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;introduction-goal-and-idea&#34;&gt;Introduction: Goal and Idea&lt;/h1&gt;
&lt;h2 id=&#34;goal&#34;&gt;Goal&lt;/h2&gt;
&lt;p&gt;We suspect that it may be possible to develop an enhancement system for human brains which satisfies the following desiderata:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Competitiveness: Our enhancement system must be powerful enough to help humans escape the acute AGI risk period. This desideratum may be satisfied if enhanced humans are able to find a solution to the prosaic AI alignment problem.&lt;/li&gt;
&lt;li&gt;Timeliness: Our enhancement system must be fully developed before the advent of AGI.&lt;/li&gt;
&lt;li&gt;Value preservation: Our enhancement system must not severely distort the core values of the enhanced human.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;idea&#34;&gt;Idea&lt;/h2&gt;
&lt;p&gt;Human brains are sharply constrained in size. Indeed, human birth is difficult because our brains (and thus skulls) are 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Obstetrical_dilemma&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;unusually large&lt;/a&gt; compared to other primates.&lt;/p&gt;
&lt;p&gt;If evolution progressed such that human brains evolved until it hit a size constraint, then it seems likely that further increasing human brain size would yield more than modest gains to human intelligence.&lt;/p&gt;
&lt;p&gt;One possible approach to increasing human brain size is by predicting real-time activations in the neocortex. The guiding intuition is that this is already how the brain extends itself, like how the 
&lt;a href=&#34;https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8/p/Y3bkJ59j4dciiLYyw#4_6__Short_term_predictor__example__1__The_cerebellum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cerebellum currently predicts activations in the telencephalon&lt;/a&gt;. Namely, Neuralink hardware currently supports 
&lt;a href=&#34;https://www.jmir.org/2019/10/e16194/PDF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;three thousand EEG probes&lt;/a&gt; which can read from the brain, and Neuralink has the intention to add write access as well. Write access in this context means that the Neuralink probes could pipe brain activation data into a neural network which is trained to predict future brain activations, and the write probes could send those predictions as signals back into the brain. Scott believes that other hypotheses such as 
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2010.00019/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spike-timing dependent plasticity&lt;/a&gt; and 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2666703/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;predictive coding&lt;/a&gt; further support the hypothesis that the brain will learn to harness predictive signals from the EEG probes.&lt;/p&gt;
&lt;p&gt;Human brains are optimized for low energy 
&lt;a href=&#34;https://www.semanticscholar.org/paper/Cognitive-systems-optimize-energy-rather-than-Markman-Otto/bc0c4e091933fd00d17d0df0bd64247ae7d237aa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;expenditure rather than speed&lt;/a&gt;, and it can take 
&lt;a href=&#34;https://academic.oup.com/cercor/article/14/8/851/268309&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tens of milliseconds&lt;/a&gt; for a single serial communication between neurons. Artificial neural networks could run faster. For instance, if a language model the size of Chinchilla could predict neural activations, then current neural network hardware could speed serial computations in the brain by two orders of magnitude (0.7ms to 70ms, of course depending on the distance between the neurons).&lt;/p&gt;
&lt;p&gt;However, 3e3 probes is essentially nothing in the context of a brain with 8e10 neurons. To keep the orders of magnitude consistent, this is like expecting to read and predict 10 or 100 activations out of a neural network with 100M. Our core hypothesis is that this is not the correct neural network analogy: since both the neural prosthesis and your brain are simultaneously learning, it is more like jointly training the two networks. In this setting, the loss could flow into the helper network through the EEG probes, and the main network could learn to supply inputs to the helper network such that it is especially helpful. For instance, if the Neuralink network has access to a compass, the internet, or 4D-shape rotation software, the brain could potentially learn to take advantage of the new hardware.&lt;/p&gt;
&lt;p&gt;So suppose we could wire into a computational substrate in this way. Then not only could we get access to new senses, but also we might discover an increase in raw processing ability. One might experience this as a speed up of serial computation in the brain, resulting in substantial improvements in intelligence.&lt;/p&gt;
&lt;p&gt;This architecture seems highly scalable. Indeed, any advancement in the general class of predictive computers could be leveraged to improve our brain state predictor. If humans could stay roughly competitive with AI for longer, then humanity would have a better shot at survival into the long term future. An additional win condition here would involve enough modeling of the brain&amp;rsquo;s dynamics that we could essentially use this as an incremental uploading procedure.&lt;/p&gt;
&lt;h1 id=&#34;details-on-competitiveness&#34;&gt;Details on competitiveness&lt;/h1&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;p&gt;Since we are bandwidth constrained, it may make sense to start out with a helper network that has been pre-trained on other brain data and/or the internet. But there is some reason to believe that predicting brain activations might be feasible even without pre-training, or the brain rerouting to make use of the helper network.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s Fermi estimate how much data we might need to predict neural activations. Maybe predicting sufficiently dense probes in the visual cortex is analogous to predicting next pixels on YouTube, and predicting sufficiently dense probes in Broca&amp;rsquo;s area is analogous to predicting the next word. Let&amp;rsquo;s naively assume that we need as much data as was used to train GPT3 Davinci, which used 
&lt;a href=&#34;https://arxiv.org/pdf/2005.14165.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;570GB&lt;/a&gt; (around 5.7e11 bytes or 5e12 bits) of text data after filtering. Suppose we read from 3e3 neurons. Each reading is in the form of a boolean array of length 3e3, denoting whether that neuron is currently undergoing an activation potential. Let&amp;rsquo;s take the readings 10 times per second to reduce correlation, giving us 3e4 bits/seconds. So we need 5e12 bits / 3e4 bits/sec ~ 1.7e8 seconds = 17 years. So this would need an order of magnitude improvement in the (very preliminary) Neuralink probe count or an order of magnitude improvement in data efficient of neural networks to be feasible.&lt;/p&gt;
&lt;p&gt;Some ways in which this could be false:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Predicting insufficiently dense probes is akin to predicting a single random pixel on YouTube &amp;ndash; this could be much harder than the original task, since we don&amp;rsquo;t have direct access to global information in the image&lt;/li&gt;
&lt;li&gt;Concepts in the brain are not fixed to neurons, but rather might move around faster than continual helper network learning can keep up with (again we are really not neuroscientists)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One reason to believe that 3,000 faster neurons could meaningfully help your cognition is that the current state-of-the-art artificial neural networks have hidden dimension only 
&lt;a href=&#34;https://arxiv.org/pdf/2005.14165.pdf#page=8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;~10,000 neurons wide&lt;/a&gt; (d_model), and can do very impressive things (e.g. strongly superhuman Go play, roughly human-level language abilities, memorizing large amounts of knowledge on the internet).&lt;/p&gt;
&lt;p&gt;Even in the case of an unlearnable signal, it is possible to get partial credit. GPT3 probably has some human-like conceptual structures since it was trained on so much human-generated, information-bearing signal. We could imagine just adding more human-generated data &amp;ndash; EEG, iEEG, fMRI, YouTube, Spotify, speech and movement data, eye tracking data, etc. Reinforcement learning from human feedback also jams more human information into the model. For relevant information for this idea, see Gwern&amp;rsquo;s post 
&lt;a href=&#34;https://www.reddit.com/r/reinforcementlearning/comments/9pwy2f/wbe_and_drl_a_middle_way_of_imitation_learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.reddit.com/r/reinforcementlearning/comments/9pwy2f/wbe_and_drl_a_middle_way_of_imitation_learning/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some particularly relevant examples:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/1906.11759&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Low-dimensional Embodied Semantics for Music and Language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.cs.cmu.edu/%7Eafyshe/papers/acl2014/jnnse_acl2014.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interpretable Semantic Vectors from a Joint Model of Brain- and Text-Based Meaning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://aclanthology.org/D16-1064.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Exploring Semantic Representation in Brain Activity Using Word Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These at least provide evidence for the possibility of using fMRI data, which provides spatially coarse (typically 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3073717/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;3-4 mm&lt;/a&gt;) read access, but not write access. Even if we used a different kind of write access in conjunction with fMRI reading, the authors suspect that predicting fMRI data would not yield sufficiently detailed modeling of human cognition. Also, some of these papers talk about tracking the contents of attention (usually via eye movement and surface electrodes). Anecdotally, humans seem to fruitfully exploit low bandwidth channels by tracking each others&amp;rsquo; attention.&lt;/p&gt;
&lt;p&gt;In the scenario where we do not have enough direct write access, we could harness auxillary pathways such as projecting information onto a Google glass equivalent or via a 
&lt;a href=&#34;https://eagleman.com/science/sensory-substitution/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vest with targeted vibrations&lt;/a&gt;, potentially using a joint brain-image embedding space trained similarly to 
&lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CLIP&lt;/a&gt;. 
&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0306452221000129&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&lt;/a&gt; is an example of gaining hearing abilities through a vibrating wristband. The key challenge here is that the brain may be slow to learn to use this new signal. For example, it generally takes an adult 
&lt;a href=&#34;https://blindlowvision.org.nz/information/braille/learning-braille/#:~:text=Like%20any%20new%20skill%2C%20braille,ve%20got%20it%20for%20life.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;4 months&lt;/a&gt; to learn braille, and predicted brain data would be a much more complicated signal. It took 1 month for vibrating wristband users to distinguish between three sounds with 70% average accuracy.&lt;/p&gt;
&lt;p&gt;A key aspect of competitiveness is that humans cannot be expected to think as quickly as future AI&amp;rsquo;s if any part of the human is running on a biological neuron substrate. The authors imagine that adding more predictive capacity and bandwidth will lead to more and more of the human being implemented on the computer. At this point, perhaps the ML predictive model will contain enough of you that transitioning to fully digital would be more like a software port between between ruby and python than a full rewrite. The authors hope that this can be done in such a way to provide continuity of self. Much more research is needed here to flesh out what such a transition might look like. Continuity of self seems more tenuous without direct write access to the brain, even with indirect neural predictions such as through the visual system.&lt;/p&gt;
&lt;h2 id=&#34;current-state-of-the-technology&#34;&gt;Current state of the technology&lt;/h2&gt;
&lt;p&gt;As mentioned earlier, Neuralink currently has the capability to read from 
&lt;a href=&#34;https://www.jmir.org/2019/10/e16194/PDF&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;three thousand neurons&lt;/a&gt;. The status of write access is unclear &amp;ndash;  
&lt;a href=&#34;https://www.jmir.org/2019/10/e16194/PDF#page=10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the paper says&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Modulating neural activity will be an important part of next-generation clinical brain-machine interfaces [39], for example, to provide a sense of touch or proprioception to neuroprosthetic movement control [40,41]. Therefore, we designed the Neuralink ASIC to be capable of electrical stimulation on every channel, although we have not demonstrated these capabilities here.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Existing solutions can both read and write with 
&lt;a href=&#34;https://www.nature.com/articles/s41551-018-0323-x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;128 electrodes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.nature.com/articles/s41551-022-00941-y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zhao et al&lt;/a&gt; recently harnessed advances in electrode probe flexibility to stably record 1000 neurons in a regular grid over 1 cubic millimeter.&lt;/p&gt;
&lt;p&gt;Many other brain measurement technologies exist besides electrodes, but electrodes are especially nice because they offer high spatial (10s of micrometers) and temporal (ms) resolution.&lt;/p&gt;
&lt;p&gt;Some EEG signals are predicted in 
&lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fncom.2018.00056/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Modern Machine Learning as a Benchmark for Fitting Neural Responses&lt;/a&gt; but it is from 2018 and uses LSTM&amp;rsquo;s, so it would be interesting to see how accurately we can do prediction on 
&lt;a href=&#34;https://github.com/KordingLab/spykesML&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/KordingLab/spykesML&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2206.11417.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Self-Supervised Learning of Brain Dynamics from Broad Neuroimaging Data&lt;/a&gt; predicts fMRI signals via pre-training on 
&lt;a href=&#34;https://openneuro.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenNeuro.org&lt;/a&gt; and 
&lt;a href=&#34;https://wiki.humanconnectome.org/display/PublicData/How&amp;#43;to&amp;#43;Access&amp;#43;Data&amp;#43;on&amp;#43;ConnectomeDB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Human Connectome Project&lt;/a&gt; datasets. This model achieves an F1 score of 0.9 on the downstream task of predicting mental states such as story-telling vs math, fear vs neural, and left vs right finger.&lt;/p&gt;
&lt;h1 id=&#34;details-on-alignment&#34;&gt;Details on alignment&lt;/h1&gt;
&lt;p&gt;In the extreme, this is a post about brain uploading, values and all. To avoid the failure mode of uploading your least favorite person, it may be possible to upload large numbers of people if this technology is sufficiently scalable. While Neuralink has automated the surgical process to allow greater scaling, many people are likely to gawk at the notion of invasive surgery. Fortunately, there are a whole spectrum of possible merging techniques possible, from Neuralink to fine-tuning a language model on your own speech and writing.&lt;/p&gt;
&lt;p&gt;But the farther we are from Neuralink on this spectrum, the more suspect the alignment properties. Even within the Neuralink story, alignment might depend on details of the training procedure. If the prediction is trained from scratch, then it may be analogous to simply having more brain. But if, for sample efficiency reasons, the neural activations are first embedded into a pre-trained language model&amp;rsquo;s latent space, a person&amp;rsquo;s internal models may end up distorted. We might be able to mitigate this concern by using contrastive learning to encode the Neuralink user&amp;rsquo;s brain activations into a joint latent space with internet text. We would welcome theory work to understand how these and other options affect alignment properties.&lt;/p&gt;
&lt;h1 id=&#34;future-work&#34;&gt;Future Work&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Run main and helper neural networks and see how many connection points are needed to improve performance
a. Try this with and without training the helper and main networks jointly
b. See how the difficulty of prediction changes as a function of the distance between the inputs and outputs of the helper network&lt;/li&gt;
&lt;li&gt;Find online repositories of EEG data and experiment to check the difficulty of the prediction task
a. Check the usefulness of pre-training in this regime&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.nature.com/articles/d41586-022-03229-y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dishbrain&lt;/a&gt; is the name of an experiment where a plate of biological neurons learned to play pong in five minutes by controlling the paddle height with spiking patterns. We would like to know if neural predictions would allow the neurons to learn the game more quickly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The authors would like to thank 
&lt;a href=&#34;https://www.lesswrong.com/users/adam-shai?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Adam Shai&lt;/a&gt; and 
&lt;a href=&#34;https://www.lesswrong.com/users/steve2152?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Steven Byrnes&lt;/a&gt; for feedback on earlier drafts, as well as Joscha Bach, 
&lt;a href=&#34;https://www.lesswrong.com/users/d0themath?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Garrett Baker&lt;/a&gt;, 
&lt;a href=&#34;https://www.lesswrong.com/users/nicholaskees?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@NicholasKees&lt;/a&gt;, 
&lt;a href=&#34;https://www.lesswrong.com/users/erik-jenner?mention=user&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Erik Jenner&lt;/a&gt;, Rylan Schaeffer, and Victor Lecomte for fruitful conversations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Flexible Proof Production in an Industrial-Strength SMT Solver</title>
      <link>http://scottviteri.github.io/publication/smt/</link>
      <pubDate>Mon, 01 Aug 2022 00:08:09 -0800</pubDate>
      <guid>http://scottviteri.github.io/publication/smt/</guid>
      <description>&lt;p&gt;&lt;span&gt;*&lt;/span&gt; indicates equal contribution, &lt;span&gt;†&lt;/span&gt; indicates co-corresponding authors.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computation, Communication, and Ontology Maps</title>
      <link>http://scottviteri.github.io/talk/topos_talk/</link>
      <pubDate>Mon, 27 Jun 2022 12:00:00 -1300</pubDate>
      <guid>http://scottviteri.github.io/talk/topos_talk/</guid>
      <description>&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;
&lt;p&gt;How can we create intelligent systems that retain and expand into that which we find valuable in the universe? During this talk I will present my own thoughts based on ontology mapping, and I will communicate why I believe that mathematicians who think about systemic interactions are in an especially good place to answer this important question. I will start with a framework in which read-eval-print loops (repls) form a basis for reasoning about agents and computation in general. Then I will build ontology maps as an alignment framework on top of repls, and discuss its implications. Lastly I will invite discussion on what we might want out of the future, and talk about my thoughts on the central role of communication and its relation to both repls and ontology maps.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/WnvbPXykk8Y&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Repls and Elk</title>
      <link>http://scottviteri.github.io/post/repls-and-elk/</link>
      <pubDate>Wed, 16 Feb 2022 12:02:47 -0700</pubDate>
      <guid>http://scottviteri.github.io/post/repls-and-elk/</guid>
      <description>&lt;p&gt;Crossposted at 
&lt;a href=&#34;https://www.lesswrong.com/posts/C5PZNi5fueH2RC6aF/repl-s-and-elk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.lesswrong.com/posts/C5PZNi5fueH2RC6aF/repl-s-and-elk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In my previous post I talked about read-eval-print loops as providing a type signature for agents. I will now explain how you can quickly transition from this framework to an ELK solution. Notation is imported from that post.&lt;/p&gt;
&lt;p&gt;Imagine we have two agents, a human and a strong AI, denoted H and M respectively. They both interact with the environment in lockstep, according to the following diagram.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/0f9ecde8a04090365f96c563e8ba07c710666d50ae9e3152.png/w_454&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We have the human&amp;rsquo;s utility function UH:SH→Q, which is defined on the human&amp;rsquo;s model of reality. We would like to lift UH to a version UM:SM→Q that the machine can use to influence the world in way that is agreeable to the human, which we can do by learning a mapping F : SM→SH and deriving UM=F∘UH.&lt;/p&gt;
&lt;p&gt;But we haven&amp;rsquo;t yet said what properties we want the ontology map F to have. I want to call two concepts sh and sm equal if they act the same with respect to transformation: ∀f. f(sh) = f(sm) → sh = sm. The issue is that since the concepts have different types we cannot feed them as arguments to the same function. So instead let&amp;rsquo;s say that ∀s:S, EvalH(sh, ReadH(s)) = EvalM(sm, ReadM(s)) → sh = sm. But now we are back to the same problem where we are trying to compare concepts in two different ontologies. But this does give us a kind of inductive step where we can transfer evidence of equality between concept pairs (sh, sm) and (sh&amp;rsquo;, sm&amp;rsquo;). I also believe that this kind of a coherence argument is the best we can do, since we are not allowed to peer into the semantic content of particular machine or human states when constructing the ontology map.&lt;/p&gt;
&lt;p&gt;Consider the following two graphs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2a481b191941494551eaa75a90c30557a1f979c43150ede0.png/w_483&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;My intuition is that even if I don&amp;rsquo;t know the labels of the above graphs, I can still infer that the bottom nodes correspond to each other. And the arrows that I get in the context of ELK are the agents&amp;rsquo; Eval transitions, leading to the following commutative diagram specification for F.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/12dcebccea5df896b167763089b06164805244b0e4ba9700.png/w_340&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can learn an ontology map F : SM→SH by minimizing the difference between two paths from a state sm, one in which the machine&amp;rsquo;s prediction function is used and one in which the human&amp;rsquo;s prediction function is used.  Concretely, I propose minimizing Dist(sh1,sh2) + λ |U(sh1)-U(sh2)| where sh1 = F(EvalM(sm,om)) and sh2 = EvalH(F(sm),oh), Dist is a distance metric in SH, and observations om and oh are generated by the same underlying state S.&lt;/p&gt;
&lt;p&gt;If you are interested in getting more detail and why I believe this circumvents existing counterexamples, please check out the full proposal.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Repls</title>
      <link>http://scottviteri.github.io/post/repls/</link>
      <pubDate>Tue, 15 Feb 2022 12:06:43 -0700</pubDate>
      <guid>http://scottviteri.github.io/post/repls/</guid>
      <description>&lt;p&gt;Cross-posted at 
&lt;a href=&#34;https://www.lesswrong.com/posts/kN2cFPaLQhExEzgeZ/repl-s-a-type-signature-for-agents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.lesswrong.com/posts/kN2cFPaLQhExEzgeZ/repl-s-a-type-signature-for-agents&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Read-eval-print loops are used to interact with programming languages as well as in the shell to interact with your computer. I claim that they are a reasonable starting point for reasoning about agents. This is a framing that all but wrote my ELK proposal for me, and has been paying rent in other research domains as well.&lt;/p&gt;
&lt;p&gt;Let S be the set of world states, and assume a environment transition function T : S × A → S, where Aₓ is a set of actions.
We define an agent X to be a 3-tuple of functions Readₓ, Evalₓ, and Printₓ, where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Readₓ is a surjective &amp;ldquo;abstraction&amp;rdquo; function of type S → Oₓ, where Oₓ is a type for X&amp;rsquo;s abstracted observations.&lt;/li&gt;
&lt;li&gt;Evalₓ is a prediction function of type Sₓ × Oₓ → Sₓ, where Sₓ represents the type of X&amp;rsquo;s internal knowledge of the environment.&lt;/li&gt;
&lt;li&gt;Printₓ is a function from Sₓ to Aₓ, where Aₓ denotes X&amp;rsquo;s actions on the environment. If there are multiple agents, we assume that an action Aₓ contains one action from each agent.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The environment and agent start in initial states S⁰ and Sₓ⁰, respectively. At each time-step, the agent observes the current state of the environment with its sensors, updates its worldview using its prediction function, produces an action, and then the universe uses that action to produce a next universe state. This process is depicted in the figure below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f2026cdc159ea5ff4a0a672250cca96cdb32c0015bc7ec80.png/w_648&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Notice that in the above diagram, the transition functions of the agent and the environment look similar. In fact they are dual, and we can show this by considering the agent&amp;rsquo;s Readₓ to be the environment&amp;rsquo;s Print, and the agent&amp;rsquo;s observation type Oₓ to be the environment&amp;rsquo;s action type A.&lt;/p&gt;
&lt;p&gt;Env       := { Print  : S → Aₓ,   Eval  : S × Aₓ → S }
Agentₓ := { Printₓ : Sₓ → Aₓ, Evalₓ : Sₓ × A → Sₓ }&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a27cbc6a042caf4ab560dbbb72afd834ade6e59fb159663d.png/w_559&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Both the agent and environment start in a given initial state, pick an action based on that state, feed that action to each other&amp;rsquo;s transition functions, and transition to the next state.&lt;/p&gt;
&lt;p&gt;This interaction can be drawn as a game tree where the agent and the environment are selecting which of each other&amp;rsquo;s next branches to follow.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c4e5303c127547c49a07b55f5a5bda889a6d3d4eff33cd86.png/w_657&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The agent and environment run in lockstep, each round simultaneously printing and thereby choosing their partner&amp;rsquo;s next branch. If you want to think of the environment as giving a reward, that reward can be a function of your whole action history, which put you in a particular branch of the physics game tree.&lt;/p&gt;
&lt;p&gt;There is much more to say about this setup. I may add future posts on how this leads to an ELK proposal, how it helps bring to light common pitfalls in others&amp;rsquo; ELK proposals that I have seen so far, a similar setup in the embedded agency setting, details about nested compositional REPL&amp;rsquo;s, the connection to polynomial functors and coinductive datatypes, and maybe even a diagrammatic REPL programming language. Please let me know which if any you are interested in and I can post accordingly.&lt;/p&gt;
&lt;p&gt;Huge thanks to David Spivak for helping me with these ideas, as well as Gabriel Poesia and John Wentworth.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Epistemic Phase Transitions in Mathematical Proofs</title>
      <link>http://scottviteri.github.io/publication/ept/</link>
      <pubDate>Mon, 15 Mar 2021 21:08:09 -0800</pubDate>
      <guid>http://scottviteri.github.io/publication/ept/</guid>
      <description>&lt;p&gt;&lt;span&gt;*&lt;/span&gt; indicates equal contribution, &lt;span&gt;†&lt;/span&gt; indicates co-corresponding authors.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Teaching</title>
      <link>http://scottviteri.github.io/teaching/index_/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://scottviteri.github.io/teaching/index_/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
