[{"authors":["admin"],"categories":null,"content":"I am a PhD candidate at the Center for Automated Reasoning at Stanford University, under the guidance of Prof. Clark Barrett. My research focuses on improving the prosocial tendencies of language models (LMs) through a series of unique developmental approaches. This includes the introduction of communication channels during autoregressive training (akin to a kindergarten setting), allowing a parent LM to guide a child LM by curating its training data, and enhancing human feedback on LMs via a combined embedding of EEG data and speech.\nDuring my PhD journey, my focus has evolved from formal verification and programming languages to AI alignment, prompted by my belief that robust AI represents a substantial existential threat to humanity. Prior to this, I majored in computer science and electrical engineering at MIT, contributing to AI and robotics research. After MIT, I explored interactive theorem proving at CMU with Simon Dedeo. During this period, I published research on the application of abduction in mathematics in the Cognition journal. I am currently in the 4th year of my PhD, where I have engaged in various projects related to SMT solving and interactive theorem proving. I have also probed into ontology mapping as a technique for targeted neural network interpretability.\nMy primary character trait is curiosity, and I really love math.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://scottviteri.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD candidate at the Center for Automated Reasoning at Stanford University, under the guidance of Prof. Clark Barrett. My research focuses on improving the prosocial tendencies of language models (LMs) through a series of unique developmental approaches.","tags":null,"title":"Scott Viteri","type":"authors"},{"authors":null,"categories":null,"content":" Course Information Instructor Name(s): Scott Viteri\nTeaching Assistants: Victor Lecomte, Gabe Mukobi, Peter Chatain\nCourse Faculty Sponsor: Clark Barrett\nLecture: Monday 3:00-4:30pm, B067 Mitchell Earth Science (In-person only)\nOptional Office Hours: Large Language Model productivity meetings at 1-2PM in Gates 200\nGraduate-level course or advanced undergraduates (contact course instructor)\n3 Units, Spring 2023, ExploreCourses\nCourse Description In this course we will explore the current state of research in the field of AI alignment, which seeks to bring increasingly intelligent AI systems in line with human values and interests. The purpose of this course is to encourage the development of new ideas in this field, where a dominant paradigm has not yet been established. The format will be weekly lectures in which speakers present their current research approaches.\nThe assignment structure will be slightly unusual: each week students will have a choice between a problem set and a short research assignment based on the weekly guest speaker’s research area. For the research assignment, students will start with the abstract of a relevant AI alignment paper or blog post and create a blog post or Github repository describing how they would continue the paper. The final weekly assignment will be an extension of one of the previous weeks’ work. Therefore this course requires research experience, preferably using mathematical and programming tools (e.g. Python, PyTorch, calculus), and is a graduate level course, open to advanced undergraduates.\nPrerequisites: Any one of the following: CS223a, CS224n, CS224w, CS224u, CS227b, CS228, CS229, CS229t, CS229m, CS231a, CS231n, CS234, CS236, CS237a, CS281\nIn addition to the above, strong ability to do independent research will be necessary, preferably using mathematical and programming tools (e.g. Python, PyTorch, calculus).\nSyllabus Date Week Name Topic Suggested Assignment Prompt April 3 (Mon) 1 Scott Viteri Overview of Course and AI Safety Bowman 2022, Steinhardt 2022, Carlsmith 2022, Gates 2022 April 10 4 Adam Gleave (UC Berkeley) Inverse Reinforcement Learning Gleave, Toyer 2022, Gleave 2022 April 17 3 Andrew Critch (UC Berkeley) Multiagent problems Critch 2019, Fickinger, Zhuang, Critch et al 2020, Garrabrandt, Critch et al 2016 April 24 2 Andy Jones (Anthropic) Empirical alignment - interpretability Askell 2021, Elhage, Nanda 2021 May 1 5 Dan Hendrycks (Center for AI Safety) Robustness and Generalization in AI Systems Hendrycks 2022, Hendrycks 2021a, Hendrycks 2021b May 8 6 Alex Turner (UC Berkeley) Shard theory Turner 2022, Pope, Turner 2022 May 15 7 Laria Reynolds (Conjecture) Empirical alignment research with LLM Reynolds, McDonell 2021 May 22 8 John Wentworth (independent researcher) Agent Foundations and Abstractions Wentworth 2022a, Wentworth 2022b May 29 9 Memorial Day — no class Jun 5 10 Evan Hubinger (Anthropic) Mesa-Optimization and Inner Alignment Hubinger, Mikulik et al 2019, Hubinger 2021 ","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677628800,"objectID":"970083d613a4a151e9c7bfaa5edf9e83","permalink":"http://scottviteri.github.io/teaching/courses/cs362/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/teaching/courses/cs362/","section":"Teaching","summary":"Course Organizer","tags":null,"title":"CS 362: Research in AI Alignment","type":"teaching"},{"authors":null,"categories":null,"content":"Course Information Course Coordinator: Gabriel Mukobi Faculty Sponsor: Paul N. Edwards (STS) Content: This advanced course explores the frontier of current AI alignment research directions and helps students develop their own inside view on AI safety research. The course includes targeted readings, weekly group discussions, technical alignment exercises, and a final literature review or research proposal. As a Discussion Facilitator for this course, I guided students through the complex issues surrounding AI alignment, helped them understand current research directions, and provided guidance on their projects.\nFor more information about the course, please visit the course page and the syllabus.\n","date":1677542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677542400,"objectID":"1a91816a7bf1376ee5fde9c875523238","permalink":"http://scottviteri.github.io/teaching/courses/sts20si/","publishdate":"2023-02-28T00:00:00Z","relpermalink":"/teaching/courses/sts20si/","section":"Teaching","summary":"Discussion Facilitator","tags":null,"title":"STS 20SI: Advanced AI Alignment","type":"teaching"},{"authors":null,"categories":null,"content":"Course Information Course Organizer: Richard Ngo Content: The AGI Safety Fundamentals: Alignment Course provides a high-level understanding of the AI alignment problem and some of the key research directions which aim to solve it. The course consists of 7 weeks of readings and discussion sessions (plus one optional week introducing the fundamentals of machine learning), and a final project. No background machine learning knowledge is required, but participants are expected to have some fluency in basic statistics and mathematical notation. I had the opportunity to mentor for this program twice. For more information about the course, please visit the official course page.\n","date":1655251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655251200,"objectID":"04e208f1933fb1003d677c79188f59e3","permalink":"http://scottviteri.github.io/teaching/courses/fundamentals/","publishdate":"2022-06-15T00:00:00Z","relpermalink":"/teaching/courses/fundamentals/","section":"Teaching","summary":"Mentor","tags":null,"title":"AGI Safety Fundamentals: AI Alignment Course","type":"teaching"},{"authors":null,"categories":null,"content":"Course Information Course Coordinator: Gabriel Mukobi Faculty Sponsor: Paul N. Edwards (STS) Content: This 8-week course explores the pressing ethical questions about what will happen if AI systems aren’t aligned with our values. The course includes targeted readings, weekly group discussions, and an optional project. Basic knowledge about machine learning helps but is not required. As a Discussion Facilitator for this course, I helped students understand the complex issues surrounding AI alignment, and provided guidance on their projects.\nFor more information about the course, please visit the course page and the syllabus.\n","date":1669593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669593600,"objectID":"c6358e4abdcc19db930ebf0e421a29d0","permalink":"http://scottviteri.github.io/teaching/courses/sts10si/","publishdate":"2022-11-28T00:00:00Z","relpermalink":"/teaching/courses/sts10si/","section":"Teaching","summary":"Discussion Facilitator","tags":null,"title":"STS 10SI: Introduction to AI Alignment","type":"teaching"},{"authors":["Haniel Barbosa","Andrew Reynolds","Gereon Kremer","Hanna Lachnitt","Aina Niemetz","Andres Nötzli","Alex Ozdemir","Mathias Preiner","Arjun Viswanathan","Scott Viteri","Yoni Zohar","Cesare Tinelli \u0026 Clark Barrett"],"categories":[],"content":"* indicates equal contribution, † indicates co-corresponding authors.\n","date":1659341289,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659341289,"objectID":"ab660aea8a2345807fec6b08d872f8bd","permalink":"http://scottviteri.github.io/publication/smt/","publishdate":"2022-08-01T00:08:09-08:00","relpermalink":"/publication/smt/","section":"publication","summary":"Proof production for SMT solvers is paramount to ensure their correctness independently from implementations, which are often prohibitively difficult to verify. Historically, however, SMT proof production has struggled with performance and coverage issues, resulting in the disabling of many crucial solving techniques and in coarse-grained (and thus hard to check) proofs. We present a flexible proof-production architecture designed to handle the complexity of versatile, industrial-strength SMT solvers and show how we leverage it to produce detailed proofs, including for components previously unsupported by any solver. The architecture allows proofs to be produced modularly, lazily, and with numerous safeguards for correctness. This architecture has been implemented in the state-of-the-art SMT solver cvc5. We evaluate its proofs for SMT-LIB benchmarks and show that the new architecture produces better coverage than previous approaches, has acceptable performance overhead, and supports detailed proofs for most solving components.","tags":[],"title":"Flexible Proof Production in an Industrial-Strength SMT Solver","type":"publication"},{"authors":[],"categories":null,"content":"Abstract: How can we create intelligent systems that retain and expand into that which we find valuable in the universe? During this talk I will present my own thoughts based on ontology mapping, and I will communicate why I believe that mathematicians who think about systemic interactions are in an especially good place to answer this important question. I will start with a framework in which read-eval-print loops (repls) form a basis for reasoning about agents and computation in general. Then I will build ontology maps as an alignment framework on top of repls, and discuss its implications. Lastly I will invite discussion on what we might want out of the future, and talk about my thoughts on the central role of communication and its relation to both repls and ontology maps.\n","date":1656378000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656378000,"objectID":"dfa4750d9b3f73c7d94d1be94833cf57","permalink":"http://scottviteri.github.io/talk/topos_talk/","publishdate":"2022-06-27T12:00:00-13:00","relpermalink":"/talk/topos_talk/","section":"talk","summary":"Abstract: How can we create intelligent systems that retain and expand into that which we find valuable in the universe? During this talk I will present my own thoughts based on ontology mapping, and I will communicate why I believe that mathematicians who think about systemic interactions are in an especially good place to answer this important question.","tags":["tutorial"],"title":"Computation, Communication, and Ontology Maps","type":"talk"},{"authors":["Scott Viteri","Simon DeDeo"],"categories":[],"content":"* indicates equal contribution, † indicates co-corresponding authors.\n","date":1615871289,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615871289,"objectID":"3acca4949164a883d8d169e28049b8f1","permalink":"http://scottviteri.github.io/publication/ept/","publishdate":"2022-08-22T00:08:09-08:00","relpermalink":"/publication/ept/","section":"publication","summary":"Mathematical proofs are both paradigms of certainty and some of the most explicitly-justified arguments that we have in the cultural record. Their very explicitness, however, leads to a paradox, because the probability of error grows exponentially as the argument expands. When a mathematician encounters a proof, how does she come to believe it? Here we show that, under a cognitively-plausible belief formation mechanism combining deductive and abductive reasoning, belief in mathematical arguments can undergo what we call an epistemic phase transition: a dramatic and rapidly-propagating jump from uncertainty to near-complete confidence at reasonable levels of claim-to-claim error rates. To show this, we analyze an unusual dataset of forty-eight machine-aided proofs from the formalized reasoning system Coq, including major theorems ranging from ancient to 21st Century mathematics, along with five hand-constructed cases including Euclid, Apollonius, Hernstein's Topics in Algebra, and Andrew Wiles's proof of Fermat's Last Theorem. Our results bear both on recent work in the history and philosophy of mathematics on how we understand proofs, and on a question, basic to cognitive science, of how we justify complex beliefs.","tags":[],"title":"Epistemic Phase Transitions in Mathematical Proofs","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"443ba1582dc4811a9c20089ef25fb885","permalink":"http://scottviteri.github.io/teaching/index_/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/teaching/index_/","section":"Teaching","summary":"","tags":null,"title":"Teaching","type":"Teaching"}]