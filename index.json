[{"authors":["admin"],"categories":null,"content":"I am a PhD candidate at the Center for Automated Reasoning at Stanford University, under the guidance of Prof. Clark Barrett. My research focuses on improving the prosocial tendencies of language models (LMs) through a series of unique developmental approaches. This includes the introduction of communication channels during autoregressive training (akin to a kindergarten setting), allowing a parent LM to guide a child LM by curating its training data, and enhancing human feedback on LMs via a combined embedding of EEG data and speech.\nDuring my PhD journey, my focus has evolved from formal verification and programming languages to AI alignment, prompted by my belief that robust AI represents a substantial existential threat to humanity. Prior to this, I majored in computer science and electrical engineering at MIT, contributing to AI and robotics research. After MIT, I explored interactive theorem proving at CMU with Simon Dedeo. During this period, I published research on the application of abduction in mathematics in the Cognition journal. I am currently in the 4th year of my PhD, where I have engaged in various projects related to SMT solving and interactive theorem proving. I have also probed into ontology mapping as a technique for targeted neural network interpretability.\nMy primary character trait is curiosity, and I really love math.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://scottviteri.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD candidate at the Center for Automated Reasoning at Stanford University, under the guidance of Prof. Clark Barrett. My research focuses on improving the prosocial tendencies of language models (LMs) through a series of unique developmental approaches.","tags":null,"title":"Scott Viteri","type":"authors"},{"authors":null,"categories":null,"content":" Course Information Instructor Name(s): Scott Viteri\nTeaching Assistants: Victor Lecomte, Gabe Mukobi, Peter Chatain\nCourse Faculty Sponsor: Clark Barrett\nLecture: Monday 3:00-4:30pm, B067 Mitchell Earth Science (In-person only)\nOptional Office Hours: Large Language Model productivity meetings at 1-2PM in Gates 200\nGraduate-level course or advanced undergraduates (contact course instructor)\n3 Units, Spring 2023, ExploreCourses\nCourse Description In this course we will explore the current state of research in the field of AI alignment, which seeks to bring increasingly intelligent AI systems in line with human values and interests. The purpose of this course is to encourage the development of new ideas in this field, where a dominant paradigm has not yet been established. The format will be weekly lectures in which speakers present their current research approaches.\nThe assignment structure will be slightly unusual: each week students will have a choice between a problem set and a short research assignment based on the weekly guest speaker’s research area. For the research assignment, students will start with the abstract of a relevant AI alignment paper or blog post and create a blog post or Github repository describing how they would continue the paper. The final weekly assignment will be an extension of one of the previous weeks’ work. Therefore this course requires research experience, preferably using mathematical and programming tools (e.g. Python, PyTorch, calculus), and is a graduate level course, open to advanced undergraduates.\nPrerequisites: Any one of the following: CS223a, CS224n, CS224w, CS224u, CS227b, CS228, CS229, CS229t, CS229m, CS231a, CS231n, CS234, CS236, CS237a, CS281\nIn addition to the above, strong ability to do independent research will be necessary, preferably using mathematical and programming tools (e.g. Python, PyTorch, calculus).\nSyllabus Date Week Name Topic Suggested Assignment Prompt April 3 (Mon) 1 Scott Viteri Overview of Course and AI Safety Bowman 2022, Steinhardt 2022, Carlsmith 2022, Gates 2022 April 10 4 Adam Gleave (UC Berkeley) Inverse Reinforcement Learning Gleave, Toyer 2022, Gleave 2022 April 17 3 Andrew Critch (UC Berkeley) Multiagent problems Critch 2019, Fickinger, Zhuang, Critch et al 2020, Garrabrandt, Critch et al 2016 April 24 2 Andy Jones (Anthropic) Empirical alignment - interpretability Askell 2021, Elhage, Nanda 2021 May 1 5 Dan Hendrycks (Center for AI Safety) Robustness and Generalization in AI Systems Hendrycks 2022, Hendrycks 2021a, Hendrycks 2021b May 8 6 Alex Turner (UC Berkeley) Shard theory Turner 2022, Pope, Turner 2022 May 15 7 Laria Reynolds (Conjecture) Empirical alignment research with LLM Reynolds, McDonell 2021 May 22 8 John Wentworth (independent researcher) Agent Foundations and Abstractions Wentworth 2022a, Wentworth 2022b May 29 9 Memorial Day — no class Jun 5 10 Evan Hubinger (Anthropic) Mesa-Optimization and Inner Alignment Hubinger, Mikulik et al 2019, Hubinger 2021 ","date":1677628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677628800,"objectID":"970083d613a4a151e9c7bfaa5edf9e83","permalink":"http://scottviteri.github.io/teaching/courses/cs362/","publishdate":"2023-03-01T00:00:00Z","relpermalink":"/teaching/courses/cs362/","section":"Teaching","summary":"Course Organizer","tags":null,"title":"CS 362: Research in AI Alignment","type":"teaching"},{"authors":null,"categories":null,"content":"Course Information Course Coordinator: Gabriel Mukobi Faculty Sponsor: Paul N. Edwards (STS) Content: This advanced course explores the frontier of current AI alignment research directions and helps students develop their own inside view on AI safety research. The course includes targeted readings, weekly group discussions, technical alignment exercises, and a final literature review or research proposal. As a Discussion Facilitator for this course, I guided students through the complex issues surrounding AI alignment, helped them understand current research directions, and provided guidance on their projects.\nFor more information about the course, please visit the course page and the syllabus.\n","date":1677542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677542400,"objectID":"1a91816a7bf1376ee5fde9c875523238","permalink":"http://scottviteri.github.io/teaching/courses/sts20si/","publishdate":"2023-02-28T00:00:00Z","relpermalink":"/teaching/courses/sts20si/","section":"Teaching","summary":"Discussion Facilitator","tags":null,"title":"STS 20SI: Advanced AI Alignment","type":"teaching"},{"authors":null,"categories":null,"content":" Course Information Course Organizer: Richard Ngo Content: The AGI Safety Fundamentals: Alignment Course provides a high-level understanding of the AI alignment problem and some of the key research directions which aim to solve it. The course consists of 7 weeks of readings and discussion sessions (plus one optional week introducing the fundamentals of machine learning), and a final project. No background machine learning knowledge is required, but participants are expected to have some fluency in basic statistics and mathematical notation. I had the opportunity to mentor for this program twice. For more information about the course, please visit the official course page.\n","date":1655251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655251200,"objectID":"04e208f1933fb1003d677c79188f59e3","permalink":"http://scottviteri.github.io/teaching/courses/fundamentals/","publishdate":"2022-06-15T00:00:00Z","relpermalink":"/teaching/courses/fundamentals/","section":"Teaching","summary":"Mentor","tags":null,"title":"AGI Safety Fundamentals: AI Alignment Course","type":"teaching"},{"authors":null,"categories":null,"content":"Course Information Course Coordinator: Gabriel Mukobi Faculty Sponsor: Paul N. Edwards (STS) Content: This 8-week course explores the pressing ethical questions about what will happen if AI systems aren’t aligned with our values. The course includes targeted readings, weekly group discussions, and an optional project. Basic knowledge about machine learning helps but is not required. As a Discussion Facilitator for this course, I helped students understand the complex issues surrounding AI alignment, and provided guidance on their projects.\nFor more information about the course, please visit the course page and the syllabus.\n","date":1669593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669593600,"objectID":"c6358e4abdcc19db930ebf0e421a29d0","permalink":"http://scottviteri.github.io/teaching/courses/sts10si/","publishdate":"2022-11-28T00:00:00Z","relpermalink":"/teaching/courses/sts10si/","section":"Teaching","summary":"Discussion Facilitator","tags":null,"title":"STS 10SI: Introduction to AI Alignment","type":"teaching"},{"authors":[],"categories":[],"content":"Crossposted at https://www.lesswrong.com/posts/HpHyERTmsmhDiRHtY/conversationism.\nIntro There seems to be a stark contrast between my alignment research on ontology maps and on cyborgism via Neuralink. I claim that the path has consisted of a series of forced moves, and that my approach to the second follows from my conclusions about the first. This post is an attempt to document those forced moves, in the hope that others do not have to duplicate my work.\nOntology maps are about finding shape semantic-preserving functions between the internal states of agents. Each commutative diagram denotes a particular way of training neural networks to act as conceptual bridges between a human and AI. I originally started playing with these diagrams as a way of addressing the Eliciting Latent Knowledge problem. Erik Jenner has been thinking about similar objects here and here.\nCyborgism via Neuralink is about using a brain-computer interface to read neural activation and write those predictions back to the brain, in the hope that neuroplasticity will allow you to harness the extra compute. The goal is to train an artificial neural network with a particularly human inductive bias. Given severe bandwidth constraints (currently three thousand read/write electrodes), the main reason to think this might work is that the brain and the transistor-based brain predictor are being jointly trained \u0026ndash; the brain can learn to send inputs to the predictor such that the predictions are useful for some downstream task. In other words, both the brain and the neural prosthesis are trained online. Think of predicting the activations of a fixed neural network given time series data (hard) versus the same task, but you pass gradients with respect to the base neural network\u0026rsquo;s final loss through the bandwidth bottleneck into the predictor (less hard).\nIn the section \u0026ldquo;An Alternative Form of Feedback\u0026rdquo;, I introduce a vision of alignment that takes communication bandwidth as fundamental, and I sketch out how one might amplify human feedback on AI via conversation. I might recommend reading that section more than the \u0026ldquo;TLDR \u0026ndash; My Conceptual Path\u0026rdquo; section.\nTLDR \u0026ndash; My Conceptual Path I started thinking about ontology maps in the context of ELK, because ontology maps can translate the AI\u0026rsquo;s beliefs about the diamond\u0026rsquo;s location into the corresponding human beliefs. Over time I stopped thinking about aligning an external agent that is super-intelligent and has contrary goals to your own, and instead I thought more about adding compute capacity to yourself/merging with the AI. The former seems somewhat doomed, and in the context of ELK I have come to feel like the human simulator problem is basically unsolvable if you take the aligning external agent point of view. In this document I will go through a series of ontology map training procedures and briefly note on their properties and problems. I will end up at a training procedure that basically looks like the shape of learning to me, and describes how to write the software for Neuralink to leads to a human-AI symbiosis. In bullets:\nVarious training procedures for human/AI symbiosis correspond to differently shaped commutative diagrams. These commutative diagrams have importantly different alignment properties. The diagram that makes the most sense is about agents trying to predict each other. These various diagrams don\u0026rsquo;t address ELK-style human simulator issues. Instead of building and/or aligning a separate superintelligence, we should focus on augmenting a human into superintelligence. Ontology Maps The broad intuition is that given two state machines representing agents I would like to find a map between them that preserves as much structure as possible. Intuitively I am trying to find a fuzzy homomorphism between graphs. Suppose an agent is defined as a set of states and a transition function on those states. Namely, suppose we have a human agent with state type SH and transition function TH:SH→SH and an AI denoted SMand TM:SM→SM, where M stands for machine. We could train a functionF:SM→SH and G:SH→SM that minimize the distance between TM and F;TH;G, where \u0026ldquo;;\u0026rdquo; represents forward composition. This is depicted by the following commutative diagram.\nHowever, this set-up as an alignment strategy would expose us to the risk of steganography. The loss on F and G could be minimized by encoding extraneous information into the belief state SH of the human (such as some random bits about the location of a moon of Jupiter) which TH would not modify. Then G could reconstruct the original SM by interpreting those extra bits, and then use TM to achieve zero loss. I thought at the time that this steganography was being caused by the extra degree of freedom in learning both F and G. So then I thought about the following diagram:\nWe can call this picture the abstraction diagram. For this intuition think of TM as quantum mechanics and TH as Newtonian mechanics. F abstracts the state of the worlds into coarse statements about mass and energy and such, so that propagating the system forward with quantum and Newtonian transitions commute.\nSo now we are only learning one function F:SH→SH, which may prevent steganography. And it has a certain intuitive appeal: it could be interpreted as saying two belief states are equal if they have the same relationship to other beliefs. However, I have not yet added any additional structural assumptions to the state set SH, so there is not a natural differentiable distance metric in SH (to my knowledge). So instead we can consider probability distributions ΔSH, use Kullback-Liebler divergence, and this model is now PyTorch ready:\nBut then I can achieve zero loss by learning an F that maps a state in SM to the uniform probability distribution over SH. So we could add a term in the loss to penalize the average distance between the images of two random states in SH under F. But this is ad-hoc, and what I am really trying to do is preserve the information in agent M when mapping through agent H, which sounds like our first diagram. So why don\u0026rsquo;t we try a different method to prevent steganography \u0026ndash; reversing the directions of F and G (and keeping the probabilistic modification):\nLet\u0026rsquo;s use an example to illustrate what we want from this diagram. I want an SM,TM that fully captures the partial information from SH,TH. Let SH have 5 elements and let TH cycle between the first three elements and a cycle between the last two elements. We can write TH as the permutation (012)(34). Let SM have six elements and let TH be a 6-cycle (012345). If you haven\u0026rsquo;t seen this syntax for permutations before, you can just look at the following diagram.\nWe can run this training algorithm with the following encodings:\nTM is the off-diagonal 6 by 6 matrix TH is the 5 by 5 matrix consisting of the off-diagonal 3 by 3 and 2 by 2 matrices F is a neural network with 5 inputs and 6 outputs G is a neural network with 6 inputs and 5 outputs Intuitively, I wanted F to become the matrix:\nIn words, if the state was 0 I would like it to map to uniform distribution of values that are 0 modulo 3. If the state is 3 then I would like it to map to a uniform distribution over values that are 0 modulo 2. This is because I want to be able to use F;TM;G to reconstruct TH as well as possible. But what happens if F and G are nonlinear functions (which is necessary if we want to scale this procedure to non-trivial agents)? Answer: horrible, horrible steganography. If you think of floating points numbers as being real numbers, then F can encode arbitrary information about SH in the particular probabilities that it picks, even if SM only consisted of two states. We could try sampling after taking F, and training with Gumbel-softmax or the reparameterization trick or with REINFORCE, or something. But this feels kind of ad-hoc and gross again.\nSo the only kind of information bottleneck guarantees that we are going to get when routing through TM are based on the capacities/vc-dimension/inductive biases of F, TM, and G. At this point, we might as well only consider continuous spaces and let SH and SM always be Rn and Rm for some n and m. Large language models seems to be getting mileage out of tokenizing and embedding everything anyway. So Erik Jenner was right all along, and we can write these diagrams first, and think about what category our diagrams are in later, as long as that category has a distance metric (which R does of course). So let\u0026rsquo;s strip off the probability symbols and write my favorite diagram.\nTo be clear, at this point we have basically given up on the steganography story. But I think we have arrived at something very nice. I think this diagram is essentially the shape of the learning. Namely, F and G are the glue between you and external computation processes that predict you. For instance, F is typing into the calculator, TM is the calculator running, G is reading the output. Crucially, you must find that it produced the same number that you would have produced yourself.\nThis is a general process that can be used to parallelize a serial computation. Suppose I have some sequence of instructions in a programming language, such as python.\nI want to think of this computation as happening continuously in lockstep. The variables \u0026ldquo;a\u0026rdquo; and \u0026ldquo;b\u0026rdquo; are really more like electrical wires, which always have a voltage, which sometimes changes. The variables \u0026ldquo;c\u0026rdquo;, \u0026ldquo;d\u0026rdquo;, and \u0026ldquo;e\u0026rdquo; are each getting updated each clock cycle with the values from their most recent inputs. An arithmetic circuit with open input wires will also work for this story.\nSuppose I also have some extra computers lying around that I want to use to speed up this computation. Among the most naive things I could do is point an extra computer at \u0026ldquo;e\u0026rdquo;, and predict the next step in its time-step data by using its previous value, or sampling predictions according to the distribution of what values that variable has taken so far. But you could also imagine giving this extra computer access to time series data of other variables such as \u0026ldquo;a\u0026rdquo; and \u0026ldquo;d\u0026rdquo;. If the predictor computer is sufficiently accurate, the base computer might accept its predictions and get to next stable state of the system more rapidly. We would save serial compute steps by short-circuiting the intermediate computations. We\u0026rsquo;d be in an even better situation if the predictor computers could learn which inputs to pay attention to. We\u0026rsquo;d be in a better situation still if the base computer program could learn when to accept predictions from the prediction.\nIn this analogy, the original program is TH, and the predictor functions are F;TM;G. Indeed it only makes sense to talk about F;TM;G as opposed to learning a function T′H:SH→SH when there is another pre-existing useful function around, such as pre-trained language model. If TH is some supervisory signal, autoencoders are the special case of this diagram where TM is the identity function. The sparsity of the autoencoder depends on the relative dimension of SH and SM. I sometimes think of transformers as using linear F and G to embed and unembed, and TM here is computation in the residual stream. I would greatly appreciate comments to this post with machine learning instantiations of this structure.\nI am thinking of a story where the universe consists of a bunch of functions, and each of these functions is being replaced by faster functions that do the same thing. That same thing is essentially getting to the stable point of the universes dynamics faster \u0026ndash; so the functions are replaced by others that are better at eating negentropy. See the figure below for a rough illustration of this.\nI also think this shape of predictors replacing predictors is like the relationship between the steering and learning subsystems in Steven Byrnes\u0026rsquo; Intro to Brain-Like-AGI Safety.\nWhat To Do with the Nice Shape Ok, so maybe some functions can learn some other functions. Why are we here, and why have I taken up so much of your time to say such an obvious thing? If this is supposed to represent a training loop where the human is represented by TH, don\u0026rsquo;t you need a version of a human that is in a computer in the first place? Additionally, if uploading actually worked in time, then wouldn\u0026rsquo;t we already have a win condition for alignment?\nMy response to the \u0026ldquo;needing an upload first\u0026rdquo; concern used to be that TH could be a very partial version of your full transition function, such as the heart rate monitor on your Apple watch, or GPT fine-tuned on your writing. And GPT fine-tuned on your writing is definitely closer to the TH I want to put through this protocol, but I think it is probably not enough to capture what I care about.\nSo the natural next step here is doing a better job of putting a person in a computer. This is bottlenecked on what kind of data you can use to train a predictive model of a person. So in this post I talk about getting brain activation data via many small electrodes, because this might have a higher change of capturing important aspects of myself for a pseudo-uploading procedure.\nI will describe the pseudo-uploading procedure in terms of my diagram. We start with a pretrained model (TM) such as GPT, potentially fine-tuned on pre-existing EEG data. We read brain data via electrode and translate into TM\u0026rsquo;s latent space using F. TM makes a prediction of the brain\u0026rsquo;s future activations. G translates that prediction and writes the prediction back into the brain via electrode stimulation. Note that in this story F, TM, and G kind of blend into each other since TM is being trained online.\nAs I mentioned in the intro, I only expect this to work because the brain is trying to predict the neural prosthesis as the neural prosthesis is trying to predict the brain. So this makes it seem like we are now talking about a conversation:\nThis whole story has essentially been about how to increase the bandwidth between agents, and the answer is something like mutual prediction. Each agent is learning how to write and read to the other agent in order to speed up / parallelize their own computation. These thoughts about commutative diagrams are how I prefer to design the software for Neuralink.\nBut using only a single \u0026ldquo;F\u0026rdquo;, \u0026ldquo;G\u0026rdquo; pair for Neuralink seems lopsided and asymmetric. For example, I wanted to train a \u0026ldquo;predictor\u0026rdquo; neural network to predict a \u0026ldquo;base\u0026rdquo; neural network\u0026rsquo;s activations. I would do this by training the \u0026ldquo;base\u0026rdquo; neural network, and letting gradients propagate back into the \u0026ldquo;predictor\u0026rdquo; network. I would prefer if it was that the AI and the human in the loop both had something to gain from the conversation, and learned to call each other as API\u0026rsquo;s when they desired.\nAn Alternative Kind of Feedback Recently, I have been internally debating on whether to label my recent thoughts as a particular flavor of cyborgism, using Conjecture\u0026rsquo;s terminology. I have come to feel that there is an important distinction I want to make reflecting the bidirectional nature of conversation.\nI will sketch a different way to align an AGI than RLHF or conditional pre-training to give a sense of my aesthetic. Raising a child via a yes/no reward signal would be very low bandwidth, not to mention unkind. Instead I could imagine starting with a toddler GPT-J, and training it via minimizing prediction error on conversation with a select set of parents. As it gets older, it can ask to read parts of the internet or chat with particular people from a wider set of less vetted parents. When it reaches school age, it can be trained via conversations with similarly competent language models. This process can potentially be sped up by learning particular F and G\u0026rsquo;s to speak directly into each other\u0026rsquo;s latent spaces. At this point it is competent enough to be useful to a broad group of people, who can pay to talk to it. It can use some of that money to pay to talk to particular people and language models, at which point it is a functioning member of society.\nThe most obvious way that this can go wrong is if the initial high-bandwith parenting via conversation is too expensive. So we could fine-tune GPT on high quality parenting scripts. This GPT-Nanny already seems like a better (higher bandwith) version of feedback than reward modeling of human annotators in RLHF. The role of alignment reaserchers would to push back the pareto-frontier of cost versus annotation quality.\nI would like to call this style of approach conversationism, where we especially highlight the creation of a bidirectional relationship and cultural integration of AI. I know the reader is thinking it is naive think that the best way to build a nice AI is to treat it nicely, but I am having trouble imagining any other way. I have more thoughts on the philosophy of this approach, but that will have to wait for another post.\nAn aside: Parents (and all agents really) can prepend their messages with a secret hash so that the language model knows who it is talking to. It would be unfortunate if someone could easily pretend to be your mom while calling you on the phone.\nSome Related Work The most obvious related work is Erik Jenner\u0026rsquo;s here and here, linked again for convenience.\nHe uses the abstraction diagram from the machine\u0026rsquo;s state into S, which I am thinking of a general human interpretation language. He also is trying to discover TS jointly with F, if I understand correctly. I labelled the bottom states S to denote a common language between humans (SH for many different values of H). I am thinking more about the prediction ontology map, because I am especially interested in interpretability that works in both directions. I would like to know how to compose individual agents into a super-agent via conversation and communication.\nThe abstraction ontology map is like viewing one agent\u0026rsquo;s state as an abstraction of another agent\u0026rsquo;s state, and various choices of abstraction are discussed in Towards a Unified Theory of State Abstraction for MDPs. This looks similar to bisimulation of labelled transition systems, as well as another that looks like Probabilistic Bisimulation. Other related works include Causal Distillation for Language Models, Approximate Causal Abstractions, and Causal Abstractions of Neural Networks.\nSome Light Category Theory The prediction diagrams form a category:\nAn object is a state space and a transition function ⟨S,T:S→S⟩ The morphisms between objects ⟨S,T:S→S⟩ and ⟨U,V:U→U⟩ are pairs ⟨F:S→U,G:U→S⟩ such that T=F;V;G The morphisms are indeed transitive and associative. There are identity morphisms for each object. The objects are dynamical systems, and I am thinking of an agents as a special kind of dynamical system. The morphisms are choices of read-eval-print loops in the sense of my repl post. It is interesting that this representation allows a single agent to have multiple different ways of interacting with the world. A morphism from A to B semantically means that agent B predicts agent A, or that agent A uses agent B.\nAlso, if we allow the distinct domains and co-domain in the transition function, then the abstraction diagram category is called the arrow category. And the prediction diagram category is called the twisted arrow category.\nA two-sided conversation would correspond to a pair of morphism back and forth between two objects in the twisted arrow category. But having such a pair of morphisms is a very strong condition. So going forward we may want to formalize two agents as communicating in some shared part of their ontologies. I have some thoughts in this direction thanks to @davidad.\nThanks A very incomplete list of thanks:\n@Victor Lecomte \u0026ndash; for pushing me away from the single F story, and back to the F and G story @sudo -i \u0026ndash; for proof-reading and bouncing these ideas back and forth with me @davidad \u0026ndash; for telling me names of the categories I have been poking at, and for providing a direction for future theoretical (and hopefully empirical) work @Victor Warlop \u0026ndash; mentioned to me the idea of thinking of current AI\u0026rsquo;s as children @Erik Jenner \u0026ndash; for suggesting that particular category (in the math sense) of the diagram doesn\u0026rsquo;t matter @MalcolmOcean \u0026ndash; sketched a view of opening up the AI to interaction with the world via REPLit, and commiserated with me about using an impoverished yes/no training signal @Samuel Chen \u0026ndash; helped me brainstorm names for the aesthetic I want to cultivate in the alignment community. Pointed out that the limit of high bandwidth communication will create a symbiorganism. @Vivek Hebbar \u0026ndash; for conversations about whether F and G should be inverses, and suggestions about some probabilistic state space content @Aryan Bhattarai \u0026ndash; Gave the idea of linear F and G. ","date":1677519113,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677519113,"objectID":"6530bfa0cce26c6231272629130d7253","permalink":"http://scottviteri.github.io/post/conversationism/","publishdate":"2023-02-27T10:31:53-07:00","relpermalink":"/post/conversationism/","section":"post","summary":"Crossposted at https://www.lesswrong.com/posts/HpHyERTmsmhDiRHtY/conversationism.\nIntro There seems to be a stark contrast between my alignment research on ontology maps and on cyborgism via Neuralink. I claim that the path has consisted of a series of forced moves, and that my approach to the second follows from my conclusions about the first.","tags":[],"title":"Conversationism","type":"post"},{"authors":[],"categories":[],"content":"Crossposted at https://www.lesswrong.com/posts/FnfAnsAH6dva3kCHS/research-direction-be-the-agi-you-want-to-see-in-the-world.\nBe the AGI you want to see in the world.\nEpistemic status: highly speculative, authors are not neuroscientists.\nSummary It may be possible to enhance human intelligence via a brain-computer interface (BCI). We could put electrodes into a human brain, connect those electrodes to an artificial neural network, and train that network to predict and write neural activations. This may present a technique for gradual uploading of human minds to computers that doesn’t require technology sufficient to create AGI in the first place. The purpose of this article is to elicit feedback on this idea and, if it seems promising, encourage more work in this area. Introduction: Goal and Idea Goal We suspect that it may be possible to develop an enhancement system for human brains which satisfies the following desiderata:\nCompetitiveness: Our enhancement system must be powerful enough to help humans escape the acute AGI risk period. This desideratum may be satisfied if enhanced humans are able to find a solution to the prosaic AI alignment problem. Timeliness: Our enhancement system must be fully developed before the advent of AGI. Value preservation: Our enhancement system must not severely distort the core values of the enhanced human. Idea Human brains are sharply constrained in size. Indeed, human birth is difficult because our brains (and thus skulls) are unusually large compared to other primates.\nIf evolution progressed such that human brains evolved until it hit a size constraint, then it seems likely that further increasing human brain size would yield more than modest gains to human intelligence.\nOne possible approach to increasing human brain size is by predicting real-time activations in the neocortex. The guiding intuition is that this is already how the brain extends itself, like how the cerebellum currently predicts activations in the telencephalon. Namely, Neuralink hardware currently supports three thousand EEG probes which can read from the brain, and Neuralink has the intention to add write access as well. Write access in this context means that the Neuralink probes could pipe brain activation data into a neural network which is trained to predict future brain activations, and the write probes could send those predictions as signals back into the brain. Scott believes that other hypotheses such as spike-timing dependent plasticity and predictive coding further support the hypothesis that the brain will learn to harness predictive signals from the EEG probes.\nHuman brains are optimized for low energy expenditure rather than speed, and it can take tens of milliseconds for a single serial communication between neurons. Artificial neural networks could run faster. For instance, if a language model the size of Chinchilla could predict neural activations, then current neural network hardware could speed serial computations in the brain by two orders of magnitude (0.7ms to 70ms, of course depending on the distance between the neurons).\nHowever, 3e3 probes is essentially nothing in the context of a brain with 8e10 neurons. To keep the orders of magnitude consistent, this is like expecting to read and predict 10 or 100 activations out of a neural network with 100M. Our core hypothesis is that this is not the correct neural network analogy: since both the neural prosthesis and your brain are simultaneously learning, it is more like jointly training the two networks. In this setting, the loss could flow into the helper network through the EEG probes, and the main network could learn to supply inputs to the helper network such that it is especially helpful. For instance, if the Neuralink network has access to a compass, the internet, or 4D-shape rotation software, the brain could potentially learn to take advantage of the new hardware.\nSo suppose we could wire into a computational substrate in this way. Then not only could we get access to new senses, but also we might discover an increase in raw processing ability. One might experience this as a speed up of serial computation in the brain, resulting in substantial improvements in intelligence.\nThis architecture seems highly scalable. Indeed, any advancement in the general class of predictive computers could be leveraged to improve our brain state predictor. If humans could stay roughly competitive with AI for longer, then humanity would have a better shot at survival into the long term future. An additional win condition here would involve enough modeling of the brain\u0026rsquo;s dynamics that we could essentially use this as an incremental uploading procedure.\nDetails on competitiveness Method Since we are bandwidth constrained, it may make sense to start out with a helper network that has been pre-trained on other brain data and/or the internet. But there is some reason to believe that predicting brain activations might be feasible even without pre-training, or the brain rerouting to make use of the helper network.\nFirst, let\u0026rsquo;s Fermi estimate how much data we might need to predict neural activations. Maybe predicting sufficiently dense probes in the visual cortex is analogous to predicting next pixels on YouTube, and predicting sufficiently dense probes in Broca\u0026rsquo;s area is analogous to predicting the next word. Let\u0026rsquo;s naively assume that we need as much data as was used to train GPT3 Davinci, which used 570GB (around 5.7e11 bytes or 5e12 bits) of text data after filtering. Suppose we read from 3e3 neurons. Each reading is in the form of a boolean array of length 3e3, denoting whether that neuron is currently undergoing an activation potential. Let\u0026rsquo;s take the readings 10 times per second to reduce correlation, giving us 3e4 bits/seconds. So we need 5e12 bits / 3e4 bits/sec ~ 1.7e8 seconds = 17 years. So this would need an order of magnitude improvement in the (very preliminary) Neuralink probe count or an order of magnitude improvement in data efficient of neural networks to be feasible.\nSome ways in which this could be false:\nPredicting insufficiently dense probes is akin to predicting a single random pixel on YouTube \u0026ndash; this could be much harder than the original task, since we don\u0026rsquo;t have direct access to global information in the image Concepts in the brain are not fixed to neurons, but rather might move around faster than continual helper network learning can keep up with (again we are really not neuroscientists) One reason to believe that 3,000 faster neurons could meaningfully help your cognition is that the current state-of-the-art artificial neural networks have hidden dimension only ~10,000 neurons wide (d_model), and can do very impressive things (e.g. strongly superhuman Go play, roughly human-level language abilities, memorizing large amounts of knowledge on the internet).\nEven in the case of an unlearnable signal, it is possible to get partial credit. GPT3 probably has some human-like conceptual structures since it was trained on so much human-generated, information-bearing signal. We could imagine just adding more human-generated data \u0026ndash; EEG, iEEG, fMRI, YouTube, Spotify, speech and movement data, eye tracking data, etc. Reinforcement learning from human feedback also jams more human information into the model. For relevant information for this idea, see Gwern\u0026rsquo;s post https://www.reddit.com/r/reinforcementlearning/comments/9pwy2f/wbe_and_drl_a_middle_way_of_imitation_learning/.\nSome particularly relevant examples:\nLow-dimensional Embodied Semantics for Music and Language Interpretable Semantic Vectors from a Joint Model of Brain- and Text-Based Meaning Exploring Semantic Representation in Brain Activity Using Word Embeddings These at least provide evidence for the possibility of using fMRI data, which provides spatially coarse (typically 3-4 mm) read access, but not write access. Even if we used a different kind of write access in conjunction with fMRI reading, the authors suspect that predicting fMRI data would not yield sufficiently detailed modeling of human cognition. Also, some of these papers talk about tracking the contents of attention (usually via eye movement and surface electrodes). Anecdotally, humans seem to fruitfully exploit low bandwidth channels by tracking each others\u0026rsquo; attention.\nIn the scenario where we do not have enough direct write access, we could harness auxillary pathways such as projecting information onto a Google glass equivalent or via a vest with targeted vibrations, potentially using a joint brain-image embedding space trained similarly to CLIP. Here is an example of gaining hearing abilities through a vibrating wristband. The key challenge here is that the brain may be slow to learn to use this new signal. For example, it generally takes an adult 4 months to learn braille, and predicted brain data would be a much more complicated signal. It took 1 month for vibrating wristband users to distinguish between three sounds with 70% average accuracy.\nA key aspect of competitiveness is that humans cannot be expected to think as quickly as future AI\u0026rsquo;s if any part of the human is running on a biological neuron substrate. The authors imagine that adding more predictive capacity and bandwidth will lead to more and more of the human being implemented on the computer. At this point, perhaps the ML predictive model will contain enough of you that transitioning to fully digital would be more like a software port between between ruby and python than a full rewrite. The authors hope that this can be done in such a way to provide continuity of self. Much more research is needed here to flesh out what such a transition might look like. Continuity of self seems more tenuous without direct write access to the brain, even with indirect neural predictions such as through the visual system.\nCurrent state of the technology As mentioned earlier, Neuralink currently has the capability to read from three thousand neurons. The status of write access is unclear \u0026ndash; the paper says:\nModulating neural activity will be an important part of next-generation clinical brain-machine interfaces [39], for example, to provide a sense of touch or proprioception to neuroprosthetic movement control [40,41]. Therefore, we designed the Neuralink ASIC to be capable of electrical stimulation on every channel, although we have not demonstrated these capabilities here. Existing solutions can both read and write with 128 electrodes.\nZhao et al recently harnessed advances in electrode probe flexibility to stably record 1000 neurons in a regular grid over 1 cubic millimeter.\nMany other brain measurement technologies exist besides electrodes, but electrodes are especially nice because they offer high spatial (10s of micrometers) and temporal (ms) resolution.\nSome EEG signals are predicted in Modern Machine Learning as a Benchmark for Fitting Neural Responses but it is from 2018 and uses LSTM\u0026rsquo;s, so it would be interesting to see how accurately we can do prediction on https://github.com/KordingLab/spykesML.\nSelf-Supervised Learning of Brain Dynamics from Broad Neuroimaging Data predicts fMRI signals via pre-training on OpenNeuro.org and Human Connectome Project datasets. This model achieves an F1 score of 0.9 on the downstream task of predicting mental states such as story-telling vs math, fear vs neural, and left vs right finger.\nDetails on alignment In the extreme, this is a post about brain uploading, values and all. To avoid the failure mode of uploading your least favorite person, it may be possible to upload large numbers of people if this technology is sufficiently scalable. While Neuralink has automated the surgical process to allow greater scaling, many people are likely to gawk at the notion of invasive surgery. Fortunately, there are a whole spectrum of possible merging techniques possible, from Neuralink to fine-tuning a language model on your own speech and writing.\nBut the farther we are from Neuralink on this spectrum, the more suspect the alignment properties. Even within the Neuralink story, alignment might depend on details of the training procedure. If the prediction is trained from scratch, then it may be analogous to simply having more brain. But if, for sample efficiency reasons, the neural activations are first embedded into a pre-trained language model\u0026rsquo;s latent space, a person\u0026rsquo;s internal models may end up distorted. We might be able to mitigate this concern by using contrastive learning to encode the Neuralink user\u0026rsquo;s brain activations into a joint latent space with internet text. We would welcome theory work to understand how these and other options affect alignment properties.\nFuture Work Run main and helper neural networks and see how many connection points are needed to improve performance a. Try this with and without training the helper and main networks jointly b. See how the difficulty of prediction changes as a function of the distance between the inputs and outputs of the helper network Find online repositories of EEG data and experiment to check the difficulty of the prediction task a. Check the usefulness of pre-training in this regime Dishbrain is the name of an experiment where a plate of biological neurons learned to play pong in five minutes by controlling the paddle height with spiking patterns. We would like to know if neural predictions would allow the neurons to learn the game more quickly. The authors would like to thank @Adam Shai and @Steven Byrnes for feedback on earlier drafts, as well as Joscha Bach, @Garrett Baker, @NicholasKees, @Erik Jenner, Rylan Schaeffer, and Victor Lecomte for fruitful conversations.\n","date":1675536842,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675536842,"objectID":"bd06d7d53fd65ff80626b544d72166c5","permalink":"http://scottviteri.github.io/post/be-the-agi/","publishdate":"2023-02-04T11:54:02-07:00","relpermalink":"/post/be-the-agi/","section":"post","summary":"Crossposted at https://www.lesswrong.com/posts/FnfAnsAH6dva3kCHS/research-direction-be-the-agi-you-want-to-see-in-the-world.\nBe the AGI you want to see in the world.\nEpistemic status: highly speculative, authors are not neuroscientists.\nSummary It may be possible to enhance human intelligence via a brain-computer interface (BCI).","tags":[],"title":"Be the Agi","type":"post"},{"authors":["Haniel Barbosa","Andrew Reynolds","Gereon Kremer","Hanna Lachnitt","Aina Niemetz","Andres Nötzli","Alex Ozdemir","Mathias Preiner","Arjun Viswanathan","Scott Viteri","Yoni Zohar","Cesare Tinelli \u0026 Clark Barrett"],"categories":[],"content":"* indicates equal contribution, † indicates co-corresponding authors.\n","date":1659341289,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659341289,"objectID":"ab660aea8a2345807fec6b08d872f8bd","permalink":"http://scottviteri.github.io/publication/smt/","publishdate":"2022-08-01T00:08:09-08:00","relpermalink":"/publication/smt/","section":"publication","summary":"Proof production for SMT solvers is paramount to ensure their correctness independently from implementations, which are often prohibitively difficult to verify. Historically, however, SMT proof production has struggled with performance and coverage issues, resulting in the disabling of many crucial solving techniques and in coarse-grained (and thus hard to check) proofs. We present a flexible proof-production architecture designed to handle the complexity of versatile, industrial-strength SMT solvers and show how we leverage it to produce detailed proofs, including for components previously unsupported by any solver. The architecture allows proofs to be produced modularly, lazily, and with numerous safeguards for correctness. This architecture has been implemented in the state-of-the-art SMT solver cvc5. We evaluate its proofs for SMT-LIB benchmarks and show that the new architecture produces better coverage than previous approaches, has acceptable performance overhead, and supports detailed proofs for most solving components.","tags":[],"title":"Flexible Proof Production in an Industrial-Strength SMT Solver","type":"publication"},{"authors":[],"categories":null,"content":"Abstract: How can we create intelligent systems that retain and expand into that which we find valuable in the universe? During this talk I will present my own thoughts based on ontology mapping, and I will communicate why I believe that mathematicians who think about systemic interactions are in an especially good place to answer this important question. I will start with a framework in which read-eval-print loops (repls) form a basis for reasoning about agents and computation in general. Then I will build ontology maps as an alignment framework on top of repls, and discuss its implications. Lastly I will invite discussion on what we might want out of the future, and talk about my thoughts on the central role of communication and its relation to both repls and ontology maps.\n","date":1656378000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656378000,"objectID":"dfa4750d9b3f73c7d94d1be94833cf57","permalink":"http://scottviteri.github.io/talk/topos_talk/","publishdate":"2022-06-27T12:00:00-13:00","relpermalink":"/talk/topos_talk/","section":"talk","summary":"Abstract: How can we create intelligent systems that retain and expand into that which we find valuable in the universe? During this talk I will present my own thoughts based on ontology mapping, and I will communicate why I believe that mathematicians who think about systemic interactions are in an especially good place to answer this important question.","tags":["tutorial"],"title":"Computation, Communication, and Ontology Maps","type":"talk"},{"authors":[],"categories":[],"content":"Crossposted at https://www.lesswrong.com/posts/C5PZNi5fueH2RC6aF/repl-s-and-elk.\nIn my previous post I talked about read-eval-print loops as providing a type signature for agents. I will now explain how you can quickly transition from this framework to an ELK solution. Notation is imported from that post.\nImagine we have two agents, a human and a strong AI, denoted H and M respectively. They both interact with the environment in lockstep, according to the following diagram.\nWe have the human\u0026rsquo;s utility function UH:SH→Q, which is defined on the human\u0026rsquo;s model of reality. We would like to lift UH to a version UM:SM→Q that the machine can use to influence the world in way that is agreeable to the human, which we can do by learning a mapping F : SM→SH and deriving UM=F∘UH.\nBut we haven\u0026rsquo;t yet said what properties we want the ontology map F to have. I want to call two concepts sh and sm equal if they act the same with respect to transformation: ∀f. f(sh) = f(sm) → sh = sm. The issue is that since the concepts have different types we cannot feed them as arguments to the same function. So instead let\u0026rsquo;s say that ∀s:S, EvalH(sh, ReadH(s)) = EvalM(sm, ReadM(s)) → sh = sm. But now we are back to the same problem where we are trying to compare concepts in two different ontologies. But this does give us a kind of inductive step where we can transfer evidence of equality between concept pairs (sh, sm) and (sh\u0026rsquo;, sm\u0026rsquo;). I also believe that this kind of a coherence argument is the best we can do, since we are not allowed to peer into the semantic content of particular machine or human states when constructing the ontology map.\nConsider the following two graphs.\nMy intuition is that even if I don\u0026rsquo;t know the labels of the above graphs, I can still infer that the bottom nodes correspond to each other. And the arrows that I get in the context of ELK are the agents\u0026rsquo; Eval transitions, leading to the following commutative diagram specification for F.\nWe can learn an ontology map F : SM→SH by minimizing the difference between two paths from a state sm, one in which the machine\u0026rsquo;s prediction function is used and one in which the human\u0026rsquo;s prediction function is used. Concretely, I propose minimizing Dist(sh1,sh2) + λ |U(sh1)-U(sh2)| where sh1 = F(EvalM(sm,om)) and sh2 = EvalH(F(sm),oh), Dist is a distance metric in SH, and observations om and oh are generated by the same underlying state S.\nIf you are interested in getting more detail and why I believe this circumvents existing counterexamples, please check out the full proposal.\n","date":1645038167,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676574167,"objectID":"cd7534ac4aaef66367060f9d9ef72cdd","permalink":"http://scottviteri.github.io/post/repls-and-elk/","publishdate":"2022-02-16T12:02:47-07:00","relpermalink":"/post/repls-and-elk/","section":"post","summary":"Crossposted at https://www.lesswrong.com/posts/C5PZNi5fueH2RC6aF/repl-s-and-elk.\nIn my previous post I talked about read-eval-print loops as providing a type signature for agents. I will now explain how you can quickly transition from this framework to an ELK solution.","tags":[],"title":"Repls and Elk","type":"post"},{"authors":[],"categories":[],"content":"Cross-posted at https://www.lesswrong.com/posts/kN2cFPaLQhExEzgeZ/repl-s-a-type-signature-for-agents.\nRead-eval-print loops are used to interact with programming languages as well as in the shell to interact with your computer. I claim that they are a reasonable starting point for reasoning about agents. This is a framing that all but wrote my ELK proposal for me, and has been paying rent in other research domains as well.\nLet S be the set of world states, and assume a environment transition function T : S × A → S, where Aₓ is a set of actions. We define an agent X to be a 3-tuple of functions Readₓ, Evalₓ, and Printₓ, where:\nReadₓ is a surjective \u0026ldquo;abstraction\u0026rdquo; function of type S → Oₓ, where Oₓ is a type for X\u0026rsquo;s abstracted observations. Evalₓ is a prediction function of type Sₓ × Oₓ → Sₓ, where Sₓ represents the type of X\u0026rsquo;s internal knowledge of the environment. Printₓ is a function from Sₓ to Aₓ, where Aₓ denotes X\u0026rsquo;s actions on the environment. If there are multiple agents, we assume that an action Aₓ contains one action from each agent. The environment and agent start in initial states S⁰ and Sₓ⁰, respectively. At each time-step, the agent observes the current state of the environment with its sensors, updates its worldview using its prediction function, produces an action, and then the universe uses that action to produce a next universe state. This process is depicted in the figure below.\nNotice that in the above diagram, the transition functions of the agent and the environment look similar. In fact they are dual, and we can show this by considering the agent\u0026rsquo;s Readₓ to be the environment\u0026rsquo;s Print, and the agent\u0026rsquo;s observation type Oₓ to be the environment\u0026rsquo;s action type A.\nEnv := { Print : S → Aₓ, Eval : S × Aₓ → S } Agentₓ := { Printₓ : Sₓ → Aₓ, Evalₓ : Sₓ × A → Sₓ }\nBoth the agent and environment start in a given initial state, pick an action based on that state, feed that action to each other\u0026rsquo;s transition functions, and transition to the next state.\nThis interaction can be drawn as a game tree where the agent and the environment are selecting which of each other\u0026rsquo;s next branches to follow.\nThe agent and environment run in lockstep, each round simultaneously printing and thereby choosing their partner\u0026rsquo;s next branch. If you want to think of the environment as giving a reward, that reward can be a function of your whole action history, which put you in a particular branch of the physics game tree.\nThere is much more to say about this setup. I may add future posts on how this leads to an ELK proposal, how it helps bring to light common pitfalls in others\u0026rsquo; ELK proposals that I have seen so far, a similar setup in the embedded agency setting, details about nested compositional REPL\u0026rsquo;s, the connection to polynomial functors and coinductive datatypes, and maybe even a diagrammatic REPL programming language. Please let me know which if any you are interested in and I can post accordingly.\nHuge thanks to David Spivak for helping me with these ideas, as well as Gabriel Poesia and John Wentworth.\n","date":1644952003,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644952003,"objectID":"19468bf411737c898a98a92c1478027c","permalink":"http://scottviteri.github.io/post/repls/","publishdate":"2022-02-15T12:06:43-07:00","relpermalink":"/post/repls/","section":"post","summary":"Cross-posted at https://www.lesswrong.com/posts/kN2cFPaLQhExEzgeZ/repl-s-a-type-signature-for-agents.\nRead-eval-print loops are used to interact with programming languages as well as in the shell to interact with your computer. I claim that they are a reasonable starting point for reasoning about agents.","tags":[],"title":"Repls","type":"post"},{"authors":["Scott Viteri","Simon DeDeo"],"categories":[],"content":"* indicates equal contribution, † indicates co-corresponding authors.\n","date":1615871289,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615871289,"objectID":"3acca4949164a883d8d169e28049b8f1","permalink":"http://scottviteri.github.io/publication/ept/","publishdate":"2022-08-22T00:08:09-08:00","relpermalink":"/publication/ept/","section":"publication","summary":"Mathematical proofs are both paradigms of certainty and some of the most explicitly-justified arguments that we have in the cultural record. Their very explicitness, however, leads to a paradox, because the probability of error grows exponentially as the argument expands. When a mathematician encounters a proof, how does she come to believe it? Here we show that, under a cognitively-plausible belief formation mechanism combining deductive and abductive reasoning, belief in mathematical arguments can undergo what we call an epistemic phase transition: a dramatic and rapidly-propagating jump from uncertainty to near-complete confidence at reasonable levels of claim-to-claim error rates. To show this, we analyze an unusual dataset of forty-eight machine-aided proofs from the formalized reasoning system Coq, including major theorems ranging from ancient to 21st Century mathematics, along with five hand-constructed cases including Euclid, Apollonius, Hernstein's Topics in Algebra, and Andrew Wiles's proof of Fermat's Last Theorem. Our results bear both on recent work in the history and philosophy of mathematics on how we understand proofs, and on a question, basic to cognitive science, of how we justify complex beliefs.","tags":[],"title":"Epistemic Phase Transitions in Mathematical Proofs","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"443ba1582dc4811a9c20089ef25fb885","permalink":"http://scottviteri.github.io/teaching/index_/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/teaching/index_/","section":"Teaching","summary":"","tags":null,"title":"Teaching","type":"Teaching"}]